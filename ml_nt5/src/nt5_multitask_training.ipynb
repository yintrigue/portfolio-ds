{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYi0HlUHzyZZ"
   },
   "source": [
    "# NT5 Multitask Training\n",
    "Authors:   \n",
    "Tim (Ying Ting) Chen, chentim@berkeley.edu  \n",
    "Lester (Peng-Jian) Yang, lesterpjy@berkeley.edu  \n",
    "Sonya Chen, sonyachen@berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFQb1V1jz8N6"
   },
   "source": [
    "## Setup\n",
    "The section includes all the global parameters and codes to set up the environment for the NT5 training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_i7eoklU6KI"
   },
   "outputs": [],
   "source": [
    "if 'colab' in str(get_ipython()):\n",
    "    import google.colab as colab\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    colab.auth.authenticate_user()\n",
    "    colab.drive.mount('/content/gdrive') # mount google drive\n",
    "\n",
    "    \n",
    "    !pip install tensorflow-text\n",
    "    !pip install transformers\n",
    "    !pip install datasets\n",
    "    !pip install tqdm\n",
    "    !pip install gsutil\n",
    "    !pip install --upgrade wandb\n",
    "    !pip install --pre allennlp-models\n",
    "\n",
    "    clear_output() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41229,
     "status": "ok",
     "timestamp": 1605901862815,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifbYqdqoVX3NoViBOtgGwuQf_zCM-cnWaavU1TSA=s64",
      "userId": "08804338425967190040"
     },
     "user_tz": 480
    },
    "id": "7IQfnAOUTleu",
    "outputId": "16843e4d-c9e5-4343-d8a6-9e67ba922cdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.3.0\n"
     ]
    }
   ],
   "source": [
    "# ml libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_datasets as tfds\n",
    "import transformers\n",
    "import datasets # https://huggingface.co/docs/datasets/\n",
    "from allennlp.common.testing import (\n",
    "                                AllenNlpTestCase,\n",
    "                                global_distributed_metric,\n",
    "                                run_distributed_test)\n",
    "from allennlp_models.rc.metrics import DropEmAndF1\n",
    "\n",
    "# data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from google.cloud import storage as gcs\n",
    "\n",
    "# other libraries\n",
    "import os\n",
    "import json\n",
    "import functools\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "from __future__ import print_function\n",
    "\n",
    "# plot libs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f'TensorFlow {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8Cue-K52LC-"
   },
   "outputs": [],
   "source": [
    "PROCESSOR = 'TPU'\n",
    "EVALUATE_DEV = True\n",
    "EVALUATE_TEST = False\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# SEMSITIVE PRIVATE INFO\n",
    "\n",
    "# path to repo on gdrive\n",
    "# gdrive is used to save/load csv (e.g. performance evaluation reports) and model files \n",
    "GDRIVE_REPO_PATH=''\n",
    "\n",
    "\n",
    "# gcp storage paths\n",
    "DROP_PATH_GCP = ''\n",
    "\n",
    "# token for wandb, training results tracker\n",
    "WAND_LOGIN_TOKEN = ''\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# T5 NONSTOP TRAINING CONFIGS\n",
    "\n",
    "# \"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json\"\n",
    "# \"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\"\n",
    "# \"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-config.json\"\n",
    "# \"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-config.json\"\n",
    "# \"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-config.json\"\n",
    "T5_MODEL = 't5-small'\n",
    "EPOCHS = 3\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# T5 CONTINUOUS TRAINING CONFIGS\n",
    "# NOTE: Do NOT confuse \"continuous training\" with \"stage training.\" \"Continuous \n",
    "# training\" in this notebook refers to the scenarion where the full traning schedule \n",
    "# (e.g. 60 epochs) for the SAME dataset is broken in to multiple sub trainng schedules \n",
    "# (e.g. two 30-epochs training). \"Stage training\" refers to a new training schedule\n",
    "# thatfor a new dataset, although the training can be on top of a model that has been\n",
    "# trained on other datasets. For example, each stage of the following training schedule\n",
    "# is a stage: NUMERICAL DATASET -> TEXTUAL DATASET -> SQUAD -> DROP. \"Continous\n",
    "# training\" can take place when the 60-epoch training schedule on NUMERICAL DATASET is \n",
    "# broken into two 30-epoch training schedules.\n",
    "\n",
    "# doit: set to true ONLY IF continue training for the SAME dataset\n",
    "# epoch_total defines the total number of epochs for the entire training schedule\n",
    "# epoch_start defines the starting # of epoch for the current training\n",
    "# epoch_end defines the ending # of epoch for the current training\n",
    "EPOCHS_CONTINUE = {'doit': False, \n",
    "                   'epoch_total': 60,\n",
    "                   'epoch_start': 16,\n",
    "                   'epoch_end': 30}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DATASET CONFIGS\n",
    "\n",
    "# DROP DATASET  \n",
    "DROP_TRAIN_TFREC = os.path.join(DROP_PATH_GCP, 'DROP/drop_dataset_train_v4_77400.tfrecord')\n",
    "DROP_DEV_TFREC = os.path.join(DROP_PATH_GCP, 'DROP/drop_dataset_dev_v4_9536.tfrecord')\n",
    "DROP_TRAIN_EXAMPLE_COUNT = 77400\n",
    "DROP_DEV_EXAMPLE_COUNT = 9536\n",
    "DROP_DEV_JSON = os.path.join(DROP_PATH_GCP, 'DROP/drop_dataset_dev.json')\n",
    "DROP_TEST_JSON = os.path.join(DROP_PATH_GCP, 'DROP/drop_dataset_test_questions.json')\n",
    "\n",
    "# DROP CATEGORY DATASET\n",
    "DROP_CATEGORY_TRAIN_TFREC = os.path.join(DROP_PATH_GCP, 'DROP/drop_dataset_train_v4_cat_77400.tfrecord')\n",
    "DROP_CATEGORY_DEV_TFREC = os.path.join(DROP_PATH_GCP, 'DROP/drop_dataset_dev_v4_cat_9536.tfrecord')\n",
    "DROP_CATEGORY_TRAIN_EXAMPLE_COUNT = 77400\n",
    "DROP_CATEGORY_DEV_EXAMPLE_COUNT = 9536\n",
    "\n",
    "# NUMERICAL DATASET\n",
    "SYN_NUM_TRAIN_TFREC = os.path.join(DROP_PATH_GCP, 'NUMERIC/numeric_train_data_parsed_v2_990000.tfrecord')\n",
    "SYN_NUM_DEV_TFREC = os.path.join(DROP_PATH_GCP, 'NUMERIC/numeric_dev_data_parsed_v2_9996.tfrecord')\n",
    "SYN_NUM_TRAIN_EXAMPLE_COUNT = 990000\n",
    "SYN_NUM_DEV_EXAMPLE_COUNT = 9996\n",
    "\n",
    "SQUAD_TRAIN_TFREC = os.path.join(DROP_PATH_GCP, 'SQUAD/squad1.1_train_data_parsed_v3_87599.tfrecord')\n",
    "SQUAD_DEV_TFREC = os.path.join(DROP_PATH_GCP, 'SQUAD/squad1.1_dev_data_parsed_v3_10570.tfrecord')\n",
    "SQUAD_TRAIN_EXAMPLE_COUNT = 87599\n",
    "SQUAD_DEV_EXAMPLE_COUNT = 10570\n",
    "\n",
    "# TEXTUAL DATASET\n",
    "SYN_TXT_TRAIN_TFREC = os.path.join(DROP_PATH_GCP, 'TEXTUAL/textual_train_data_parsed_v2_2523192.tfrecord')\n",
    "SYN_TXT_DEV_TFREC = os.path.join(DROP_PATH_GCP, 'TEXTUAL/textual_dev_data_parsed_v2_10000.tfrecord')\n",
    "SYN_TXT_TRAIN_EXAMPLE_COUNT = 2523192\n",
    "SYN_TXT_DEV_EXAMPLE_COUNT = 10000\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# TRAINING CONFIGS\n",
    "\n",
    "# BASIC SETUP\n",
    "BATCH_SIZE = 32\n",
    "ENCODER_MAX_LEN = 512 # must be consistent with configs used to build TFRecords\n",
    "DECODER_MAX_LEN = 54 # must be consistent with configs used to build TFRecords\n",
    "TOKENIZER = transformers.AutoTokenizer.from_pretrained(T5_MODEL)\n",
    "special_tokens_dict = {'additional_special_tokens': ['0','1','2','3', '4', '5', '6', '7', '8', '9', '<ss>', '<sv>']}\n",
    "num_added_toks = TOKENIZER.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# DATASET MIXTURE CONFIGS\n",
    "IS_TRAIN_DS_MIXTURE = True\n",
    "MIXTURE_TEMP = 1\n",
    "MIXTURE_MAX = None\n",
    "MIXTURE_SCALE = 1\n",
    "\n",
    "# TRAINING STEPS\n",
    "if IS_TRAIN_DS_MIXTURE:\n",
    "    # mixture\n",
    "    VALID_TOTAL_EXAMPLE_COUNT = SYN_TXT_DEV_EXAMPLE_COUNT\n",
    "    VALID_STEPS = math.ceil(VALID_TOTAL_EXAMPLE_COUNT / BATCH_SIZE)\n",
    "    TRAIN_STEPS = None # train steps will be updated after ds is loaded with the predefined tempature\n",
    "else:\n",
    "    # single ds\n",
    "    TRAIN_TOTAL_EXAMPLE_COUNT = DROP_TRAIN_EXAMPLE_COUNT\n",
    "    VALID_TOTAL_EXAMPLE_COUNT = DROP_DEV_EXAMPLE_COUNT\n",
    "    TRAIN_STEPS = math.ceil(TRAIN_TOTAL_EXAMPLE_COUNT / BATCH_SIZE)\n",
    "    VALID_STEPS = math.ceil(VALID_TOTAL_EXAMPLE_COUNT / BATCH_SIZE)    \n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# LARNING RATE CONFIGS\n",
    "\n",
    "LR_DO_WARMUP_DECAY = False\n",
    "LR_CONSTANT = 0.001 # constant lr will be used if LR_DO_WARMUP_DECAY is set to False\n",
    "LR_WARMUP_START = 1e-8 # lr for the 1st epoch\n",
    "LR_WARMUP_END = 1e-4\n",
    "LR_MIN = LR_WARMUP_START\n",
    "LR_DECAY_RATE_LINEAR = 0.001\n",
    "if EPOCHS >= 10:\n",
    "    # allocate 10% of training steps to warmup\n",
    "    WARMP_UP_EPOCHS = EPOCHS / 10 \n",
    "else:\n",
    "    # just for testing\n",
    "    WARMP_UP_EPOCHS = 1\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# MODEL SAVING CONFIGS\n",
    "\n",
    "SAVE_TRAINED_MODEL = False\n",
    "\n",
    "SAVE_MODEL_WEIGHTS_ONLY = True\n",
    "SAVE_MODEL_CHECKPOINTS_LOCAL = False\n",
    "SAVE_MODEL_CHECKPOINTS_WANDB = False\n",
    "SAVE_BEST_MODEL_ONLY = True\n",
    "TENSORBOARD_LOG_PATH = f'{GDRIVE_REPO_PATH}/logs' # must be GCP bucket if the model is running on TPU\n",
    "MODEL_SAVE_PATH = f'{GDRIVE_REPO_PATH}/models'\n",
    "SHOW_TENSORBOARD = False\n",
    "SHOW_WANDB = False\n",
    "WANDB_PROJECT_NAME = 't5-numerical-reasoning'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# TRAINING RESULTS TRACKER CONFIGS\n",
    "\n",
    "if SHOW_WANDB:\n",
    "    !wandb login $WAND_LOGIN_TOKEN  \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52840,
     "status": "ok",
     "timestamp": 1605901874446,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifbYqdqoVX3NoViBOtgGwuQf_zCM-cnWaavU1TSA=s64",
      "userId": "08804338425967190040"
     },
     "user_tz": 480
    },
    "id": "tKp4tsQP2zjp",
    "outputId": "9b0fd015-27eb-4c0e-8f93-3b532067011c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to TPU...\n",
      "Running on TPU:  grpc://10.77.75.210:8470\n",
      "Initializing TPU...\n",
      "WARNING:tensorflow:TPU system grpc://10.77.75.210:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.77.75.210:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.77.75.210:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.77.75.210:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU initialized!\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.tensorflow.org/guide/tpu\n",
    "if PROCESSOR == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        # 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU: ', tpu.get_master())\n",
    "    except ValueError:\n",
    "        print(\"Error: Unable to connect to TPU...\")\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"Initializing TPU...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.TPUStrategy(tpu) # tf 2.3.x\n",
    "            print(\"TPU initialized!\")\n",
    "        except:\n",
    "            print(\"Error: Failed to initialize TPU...\")\n",
    "    else:\n",
    "        PROCESSOR = \"GPU\"\n",
    "\n",
    "if PROCESSOR != \"TPU\":\n",
    "    print(\"Using default strategy for CPU/GPU...\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "if PROCESSOR == \"GPU\":\n",
    "    # ref: https://tinyurl.com/yxddsxxq\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # memory growth must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpYS2xVMzUob"
   },
   "source": [
    "## Dataset Prep\n",
    "The section provides functions and codes to prepare the datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgemIZLhptrl"
   },
   "outputs": [],
   "source": [
    "def prep_tfrec_ds(\n",
    "            tfrec_path: str, \n",
    "            batch_size: int = BATCH_SIZE,\n",
    "            repeat: bool = True,\n",
    "            shuffle: bool = True,\n",
    "            processor: str = PROCESSOR,\n",
    "            encoder_max_len: int = ENCODER_MAX_LEN,\n",
    "            decoder_max_len: int = DECODER_MAX_LEN,\n",
    "            mode: str = 'train') -> tf.data.TFRecordDataset:\n",
    "\n",
    "    if mode == 'train':\n",
    "        features_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([ENCODER_MAX_LEN], tf.int64),\n",
    "            'attention_mask': tf.io.FixedLenFeature([ENCODER_MAX_LEN], tf.int64), \n",
    "            'labels': tf.io.FixedLenFeature([DECODER_MAX_LEN], tf.int64),\n",
    "            'decoder_attention_mask': tf.io.FixedLenFeature([DECODER_MAX_LEN], tf.int64)\n",
    "        }\n",
    "    else:\n",
    "        features_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([ENCODER_MAX_LEN], tf.int64),\n",
    "            'attention_mask': tf.io.FixedLenFeature([ENCODER_MAX_LEN], tf.int64), \n",
    "            'labels': tf.io.FixedLenFeature([DECODER_MAX_LEN], tf.int64),\n",
    "            'decoder_attention_mask': tf.io.FixedLenFeature([DECODER_MAX_LEN], tf.int64),\n",
    "            'answer_type': tf.io.FixedLenFeature([1], tf.string),\n",
    "            'query_id': tf.io.FixedLenFeature([1], tf.string),\n",
    "            'validated_answers': tf.io.FixedLenFeature([1], tf.string)\n",
    "        }\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(\n",
    "                        tfrec_path, \n",
    "                        num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.map(lambda batch_seralized: tf.io.parse_example(\n",
    "                                                batch_seralized, \n",
    "                                                features_description),\n",
    "                                                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(buffer_size=1024) # 1024 to optimize TPU performance\n",
    "    ds = ds.repeat(-1) if repeat else ds.repeat(1)\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    # From tf doc (https://tinyurl.com/yavczqkr):\n",
    "    # Most dataset input pipelines should end with a call to prefetch. This allows \n",
    "    # later elements to be prepared while the current element is being processed. This \n",
    "    # often improves latency and throughput, at the cost of using additional memory to \n",
    "    # store prefetched elements.\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # https://tinyurl.com/yao4obsb\n",
    "    # A single Cloud TPU device consists of four chips, each of which has two TPU cores. \n",
    "    # Therefore, for efficient utilization of Cloud TPU, a program should make use of \n",
    "    # each of the eight cores.\n",
    "    #\n",
    "    # https://tinyurl.com/y99kjyh5\n",
    "    # Model processing performance\n",
    "    # For optimum memory usage, use the largest batch size that will fit in memory. \n",
    "    # Each TPU core uses a 128 x 128 memory cell matrix for processing. In general, \n",
    "    # your batch sized should be evenly divisible by 128 to most effectively use the TPU memory.\n",
    "    #\n",
    "    # https://tinyurl.com/yawn2acn\n",
    "    # Batch Size Too Small\n",
    "    # The batch size of any model should always be at least 64 (8 per TPU core) \n",
    "    # because TPU always pads the tensors to this size. The ideal batch size when \n",
    "    # training on the TPU is 1024 (128 per TPU core), since this eliminates inefficiencies \n",
    "    # related to memory transfer and padding.\n",
    "    #\n",
    "    # https://tinyurl.com/y9nojpa2\n",
    "    # Minimal requirement: A multiple of 8!\n",
    "    if processor == 'TPU':\n",
    "        if batch_size < 64:\n",
    "            # better\n",
    "            print('Warning: Batch size {} is smaller than 64...'.format(batch_size))\n",
    "        if batch_size % 8 > 0:\n",
    "            # min requirement\n",
    "            print('Warning: Batch size {} is not a multiple of 8...'.format(batch_size))\n",
    "        \n",
    "    return ds\n",
    "\n",
    "def prep_multitask_tfrec_ds(\n",
    "                    tfrec_paths: list, # path is now a list object\n",
    "                    tfrec_example_counts: list = [],\n",
    "                    batch_size: int = BATCH_SIZE,\n",
    "                    repeat: bool = True,\n",
    "                    shuffle: bool = True,\n",
    "                    processor: str = PROCESSOR,\n",
    "                    encoder_max_len: int = ENCODER_MAX_LEN,\n",
    "                    decoder_max_len: int = DECODER_MAX_LEN):\n",
    "\n",
    "    datasets = []\n",
    "    dataset_lengths = []\n",
    "    if len(tfrec_example_counts) == len(tfrec_paths):\n",
    "        dataset_lengths = tfrec_example_counts\n",
    "    else:\n",
    "        dataset_lengths = []\n",
    "\n",
    "    # iterate datasets to:\n",
    "    # 1. grab length from file name, xyz_1234.tfrecord -> 1234\n",
    "    # 2. read in to list\n",
    "    for i, path in enumerate(tfrec_paths):\n",
    "        file_name = os.path.basename(path)\n",
    "\n",
    "        # compute & store number of examples in tfrec\n",
    "        if len(tfrec_example_counts) != len(tfrec_paths):\n",
    "            length = int(re.search(r'_(\\d+)\\.', file_name).group(1))\n",
    "            dataset_lengths.append(length)\n",
    "\n",
    "        # prep each individual ds\n",
    "        ds = tf.data.TFRecordDataset(\n",
    "                            path, \n",
    "                            num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
    "        if shuffle: \n",
    "            ds = ds.shuffle(buffer_size=1024)\n",
    "        datasets.append(ds.repeat())\n",
    "        \n",
    "    # calculate sampling weights\n",
    "    print(\"Dataset lengths:\", dataset_lengths)\n",
    "    dataset_weights = temperature_to_weights(dataset_lengths)\n",
    "    print(\"Ratio:\", dataset_weights)\n",
    "    # sample from datasets for a mixture dataset\n",
    "    ds = tf.data.experimental.sample_from_datasets(\n",
    "                                                datasets, \n",
    "                                                weights=dataset_weights)\n",
    "\n",
    "    # prep consolidated ds\n",
    "    features_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([ENCODER_MAX_LEN], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([ENCODER_MAX_LEN], tf.int64), \n",
    "        'labels': tf.io.FixedLenFeature([DECODER_MAX_LEN], tf.int64),\n",
    "        'decoder_attention_mask': tf.io.FixedLenFeature([DECODER_MAX_LEN], tf.int64),\n",
    "    }\n",
    "    ds = ds.map(lambda batch_seralized: tf.io.parse_example(\n",
    "                                                batch_seralized, \n",
    "                                                features_description),\n",
    "                                                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    if processor == 'TPU':\n",
    "        if batch_size < 64:\n",
    "            # better\n",
    "            print('Warning: Batch size {} is smaller than 64...'.format(batch_size))\n",
    "        if batch_size % 8 > 0:\n",
    "            # min requirement\n",
    "            print('Warning: Batch size {} is not a multiple of 8...'.format(batch_size))\n",
    "    \n",
    "    min_size_index = np.argmin(dataset_lengths)\n",
    "    min_size = dataset_lengths[min_size_index]\n",
    "    min_size_sampling_weight = dataset_weights[min_size_index]\n",
    "    scale = float(min_size) / min_size_sampling_weight\n",
    "    max_ds_size = [scale * i for i in dataset_weights]\n",
    "    print(\"Max sampled sizes:\", max_ds_size)\n",
    "    \n",
    "    return ds, max_ds_size\n",
    "\n",
    "def temperature_to_weights(\n",
    "            dataset_lengths: list, \n",
    "            temperature: float = MIXTURE_TEMP, \n",
    "            maximum: bool = MIXTURE_MAX, \n",
    "            scale: float = MIXTURE_SCALE) -> list:\n",
    "    '''Calculate renormalized mixing rates\n",
    "       this is the rates at which the datasets are sampled\n",
    "    '''\n",
    "    mixing_rates = []\n",
    "    for length in dataset_lengths:\n",
    "        rate = length * scale\n",
    "        if maximum:\n",
    "            rate = min(rate, maximum)\n",
    "        if temperature != 1.0:\n",
    "            rate = rate ** (1.0/temperature)\n",
    "        mixing_rates.append(rate)\n",
    "    mixing_rates = [float(rate)/sum(mixing_rates) for rate in mixing_rates]\n",
    "    return mixing_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9OPNVSz9oo6"
   },
   "outputs": [],
   "source": [
    "if IS_TRAIN_DS_MIXTURE:\n",
    "    # load/prep multitasking ds\n",
    "    train_ds, max_ds_size = prep_multitask_tfrec_ds([DROP_TRAIN_TFREC, DROP_CATEGORY_TRAIN_TFREC])\n",
    "    val_ds, max_ds_valid_size = prep_multitask_tfrec_ds([DROP_DEV_TFREC, DROP_CATEGORY_DEV_TFREC])\n",
    "\n",
    "    # update the global constants\n",
    "    TRAIN_STEPS = math.ceil(int(sum(max_ds_size)) / BATCH_SIZE)\n",
    "    VALID_STEPS = math.ceil(int(sum(max_ds_valid_size)) / BATCH_SIZE)\n",
    "\n",
    "else:\n",
    "    # load/prep a single ds\n",
    "    train_ds, max_ds_size = prep_multitask_tfrec_ds(\n",
    "                                tfrec_paths=[DROP_TRAIN_TFREC],\n",
    "                                tfrec_example_counts=[DROP_TRAIN_EXAMPLE_COUNT])\n",
    "    val_ds = prep_tfrec_ds(DROP_DEV_TFREC)\n",
    "\n",
    "print(f'Train Steps: {TRAIN_STEPS}, Valid Steps: {VALID_STEPS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9bL5iBjRDyS"
   },
   "outputs": [],
   "source": [
    "# set to true to print example counts for training and dev sets\n",
    "PRINT_EXAMPLE_COUNT = False\n",
    "\n",
    "if PRINT_EXAMPLE_COUNT:\n",
    "    def count_examples_in_tfrec(tfrec_path: str) -> int:\n",
    "        return sum(1 for _ in tf.data.TFRecordDataset(tfrec_path))\n",
    "\n",
    "    print(f'# of examples in {DROP_TRAIN_TFREC}: {count_examples_in_tfrec(DROP_TRAIN_TFREC)}')\n",
    "    print(f'DROP_TRAIN_EXAMPLE_COUNT = {DROP_TRAIN_EXAMPLE_COUNT}')\n",
    "    print('')\n",
    "    print(f'# of examples in {DROP_DEV_TFREC}: {count_examples_in_tfrec(DROP_DEV_TFREC)}')\n",
    "    print(f'DROP_DEV_EXAMPLE_COUNT = {DROP_DEV_EXAMPLE_COUNT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t6C6HmUzLOW"
   },
   "source": [
    "## Training\n",
    "In this section, we build the pipeline for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpS-qUAIzkLh"
   },
   "outputs": [],
   "source": [
    "class DROPBaseT5(transformers.TFT5ForConditionalGeneration):\n",
    "    def __init__(self, *args, log_dir: str = None, cache_dir: str = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_tracker= tf.keras.metrics.Mean(name='loss') \n",
    "\n",
    "    # Notes from doc:\n",
    "    # When you need to customize what fit() does, you should override the training \n",
    "    # step function of the Model class. This is the function that is called by fit() \n",
    "    # for every batch of data. You will then be able to call fit() as usual -- and \n",
    "    # it will be running your own learning algorithm.\n",
    "    @tf.function\n",
    "    def train_step(self, data: datasets.Dataset) -> dict:\n",
    "        # required for eager execution when customzing the the training loops\n",
    "        with tf.GradientTape() as tape:\n",
    "            # forward pass; equivalent of pushing the input x through the entire \n",
    "            # neural network for a prediction\n",
    "            # Note:\n",
    "            # \"self\" is a reference to TFT5ForConditionalGeneration, which is a \n",
    "            # subclass of keras.Model, which returns a reference to a function.\n",
    "            # https://tinyurl.com/yx9qvmyf\n",
    "            # https://keras.io/guides/customizing_what_happens_in_fit/\n",
    "            # https://huggingface.co/transformers/model_doc/t5.html#tft5forconditionalgeneration\n",
    "            # outputs = self(data, training=True) \n",
    "            print(data[\"input_ids\"])\n",
    "            outputs = super().call(\n",
    "                        inputs=data[\"input_ids\"],\n",
    "                        labels=data[\"labels\"],\n",
    "                        attention_mask=data[\"attention_mask\"],\n",
    "                        decoder_attention_mask=data[\"decoder_attention_mask\"],\n",
    "                        training=True) \n",
    "\n",
    "            # record the gradient\n",
    "            # output of hugging face is a tuple: (outputs.loss, outputs.logits)\n",
    "            # https://huggingface.co/transformers/main_classes/output.html \n",
    "            loss = tf.reduce_mean(outputs[0])\n",
    "            grads = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # optimizer as defined through Keras' API when compiling the model\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        \n",
    "        # track metrics\n",
    "        y = tf.reshape(data[\"labels\"], [-1, 1])\n",
    "        y_pred_logits = outputs[1]\n",
    "        lr = self.optimizer._decayed_lr(tf.float32)\n",
    "        self.loss_tracker.update_state(loss)        \n",
    "        self.compiled_metrics.update_state(data[\"labels\"], y_pred_logits)\n",
    "        metrics = {m.name: m.result() for m in self.metrics}\n",
    "        metrics.update({'lr': lr})\n",
    "        metrics.update({'loss': loss})\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data: datasets.Dataset) -> dict:\n",
    "        outputs = self(data, training=True) \n",
    "        y_pred_logits = outputs[1]\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # compute loss\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        # update metrics\n",
    "        y = tf.reshape(data[\"labels\"], [-1, 1])\n",
    "        self.compiled_metrics.update_state(y, y_pred_logits)\n",
    "        metrics = {m.name: m.result() for m in self.metrics}\n",
    "        metrics.update({'loss': loss})\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    @tf.function\n",
    "    def reset_f1(self) -> dict:\n",
    "        print('Reset F1...')\n",
    "\n",
    "class LRScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Performing custom learning rate scheduling by extending Keras' Callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            warmup_batches      : int = TRAIN_STEPS * WARMP_UP_EPOCHS, \n",
    "            lr_warmup_start     : float = LR_WARMUP_START, \n",
    "            lr_warmup_end       : float = LR_WARMUP_END, \n",
    "            lr_min              : int = LR_MIN,\n",
    "            decay_rate          : float = LR_DECAY_RATE_LINEAR,\n",
    "            verbose             : int = 1):\n",
    "        super(LRScheduler, self).__init__()\n",
    "\n",
    "        # warm-up\n",
    "        self.__warmup_batches = warmup_batches\n",
    "        self.__lr_warmup_start = lr_warmup_start\n",
    "        self.__lr_warmup_end = lr_warmup_end\n",
    "        self.__lr_warmup_increment = (self.__lr_warmup_end - self.__lr_warmup_start) / self.__warmup_batches\n",
    "        self.__is_warmup = True\n",
    "\n",
    "        # decay\n",
    "        self.__decay_rate = decay_rate\n",
    "        self.__lr_min = lr_min\n",
    "\n",
    "        # other properties\n",
    "        self.__verbose = verbose\n",
    "        self.__batch_count = 0\n",
    "        self.__lr_warmup_arr = [] # by training step\n",
    "        self.__lr_epoch_arr = [] # by training epoch\n",
    "        \n",
    "        self.__lr = 0\n",
    "        self.__epoch = 0\n",
    "        self.__warmup_epoch_last = 0 # epoch number where warmup ends\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None) -> None:\n",
    "        if self.__is_warmup:\n",
    "            if self.__batch_count <= self.__warmup_batches: \n",
    "                self.__lr = self.__batch_count * self.__lr_warmup_increment + self.__lr_warmup_start\n",
    "                K.set_value(self.model.optimizer.lr, self.__lr)\n",
    "                self.__lr_warmup_arr.append(self.__lr)\n",
    "                self.__batch_count  += 1\n",
    "\n",
    "                if self.__verbose > 0:\n",
    "                    print(f'\\nWarm-up LR {self.__lr} at batch {self.__batch_count}...')                \n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None) -> None:\n",
    "        if self.__is_warmup:\n",
    "            # end of warmup\n",
    "            if self.__batch_count > self.__warmup_batches: \n",
    "                self.__warmup_epoch_last = self.__epoch\n",
    "                self.__is_warmup = False\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None) -> None:\n",
    "        self.__epoch = epoch\n",
    "        if self.__is_warmup:\n",
    "            # hard code the lr calculation because on_batch_end for the first\n",
    "            # batch of the epoch happens right after on_epoch_begin\n",
    "            lr = self.__batch_count * self.__lr_warmup_increment + self.__lr_warmup_start    \n",
    "            self.__lr_epoch_arr.append(lr)\n",
    "            return\n",
    "        \n",
    "        self.__lr = K.get_value(self.model.optimizer.lr)\n",
    "        if self.__lr <= self.__lr_min:\n",
    "            K.set_value(self.model.optimizer.lr, self.__lr)\n",
    "            self.__lr_epoch_arr.append(K.get_value(self.model.optimizer.lr))\n",
    "            return\n",
    "\n",
    "        lr_new = self.__lr * 1 / (1 + self.__decay_rate * (epoch - self.__warmup_epoch_last))\n",
    "        K.set_value(self.model.optimizer.lr, lr_new)\n",
    "        self.__lr = K.get_value(self.model.optimizer.lr) # pull lr from optimizer to confirm\n",
    "        self.__lr_epoch_arr.append(self.__lr)\n",
    "\n",
    "    def simulate_lr(\n",
    "            self, \n",
    "            steps_simulation: int = 1000,\n",
    "            steps_per_epoch: int = TRAIN_STEPS,\n",
    "            plot: bool = True) -> list:\n",
    "        # hard code a model for simulation\n",
    "        # note: self.model is accessible through tf in production\n",
    "        self.model = DROPBaseT5.from_pretrained(T5_MODEL)\n",
    "        self.model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        # simulate training\n",
    "        for global_step in range(steps_simulation):\n",
    "            batch_n = global_step % steps_per_epoch\n",
    "            if batch_n == 0:\n",
    "                self.on_epoch_begin(global_step // steps_per_epoch)\n",
    "            self.on_batch_begin(batch_n)\n",
    "            self.on_batch_end(batch_n)\n",
    "\n",
    "        if plot:\n",
    "            # plot warmup lr\n",
    "            x = range(1, len(self.__lr_warmup_arr) + 1)\n",
    "            print('')\n",
    "            print(f'Steps per epoch: {steps_per_epoch}')\n",
    "            fig, ax = plt.subplots(figsize=(15, 6))\n",
    "            ax.set_title(f'LR Warmup Schedule ({self.__warmup_batches:.0f} warm-up batches)', fontsize=14, pad=11)\n",
    "            ax.set_ylabel('Learning Rate')\n",
    "            ax.set_xlabel('Batch Step')\n",
    "            ax.plot(x, self.__lr_warmup_arr)\n",
    "            plt.show()\n",
    "\n",
    "            # plot epoch lr\n",
    "            x = range(1, len(self.__lr_epoch_arr) + 1)\n",
    "            print('')\n",
    "            fig, ax = plt.subplots(figsize=(15, 6))\n",
    "            ax.set_title(f'LR Epoch Schedule', fontsize=14, pad=11)\n",
    "            ax.set_ylabel('Learning Rate')\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.plot(x, self.__lr_epoch_arr)\n",
    "            print(f'Ending LR: {self.__lr}')\n",
    "            plt.show()\n",
    "\n",
    "        return self.__lr_epoch_arr\n",
    "\n",
    "class LRSimpleScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"The scheduler takes in a list of learning rates, and feed each learning\n",
    "    rate to the model at the beginning of an epoch. If the number of epoch is larger\n",
    "    than the length of the supplied learning rate list, the last item in the list will\n",
    "    be repeatedly fed to the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rates: list) -> None:\n",
    "        self.__lr_schedule = learning_rates\n",
    "        self.__lr_actual = []\n",
    "            \n",
    "    def on_epoch_begin(self, epoch, logs=None) -> None:\n",
    "        if epoch < len(self.__lr_schedule):\n",
    "            K.set_value(self.model.optimizer.lr, self.__lr_schedule[epoch])\n",
    "        else:\n",
    "            i_last = len(self.__lr_schedule) - 1\n",
    "            K.set_value(self.model.optimizer.lr, self.__lr_schedule[i_last])\n",
    "        \n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.__lr_actual.append(lr)\n",
    "    \n",
    "    def simulate_lr(\n",
    "            self, \n",
    "            steps_simulation: int,\n",
    "            steps_per_epoch: int = TRAIN_STEPS) -> list:\n",
    "        self.model = DROPBaseT5.from_pretrained(T5_MODEL)\n",
    "        self.model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        # simulate training\n",
    "        for global_step in range(steps_simulation):\n",
    "            batch_n = global_step % steps_per_epoch\n",
    "            if batch_n == 0:\n",
    "                self.on_epoch_begin(global_step // steps_per_epoch)\n",
    "            self.on_batch_begin(batch_n)\n",
    "            self.on_batch_end(batch_n)\n",
    "\n",
    "        # plot epoch lr\n",
    "        x = range(1, len(self.__lr_actual) + 1)\n",
    "        print('')\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "        ax.set_title(f'LR Epoch Schedule', fontsize=14, pad=11)\n",
    "        ax.set_ylabel('Learning Rate')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.plot(x, self.__lr_actual)\n",
    "        plt.show()\n",
    "\n",
    "        return self.__lr_actual\n",
    "\n",
    "def get_continous_training_lr_list(\n",
    "                            warmup_batches: int,\n",
    "                            lr_warmup_start: int = LR_WARMUP_START,\n",
    "                            lr_warmup_end: int = LR_WARMUP_END,\n",
    "                            lr_min: float = LR_MIN,\n",
    "                            decay_rate: float = LR_DECAY_RATE_LINEAR,\n",
    "                            epochs_continue: dict = EPOCHS_CONTINUE,\n",
    "                            training_steps: int = TRAIN_STEPS) -> list:\n",
    "    # first, simulate the entire training schedule as if it is nonstop\n",
    "    total_train_step = epochs_continue['epoch_total'] * TRAIN_STEPS\n",
    "    lrs = LRScheduler(\n",
    "                warmup_batches      = warmup_batches, \n",
    "                lr_warmup_start     = lr_warmup_start, \n",
    "                lr_warmup_end       = lr_warmup_end, \n",
    "                lr_min              = lr_min,\n",
    "                decay_rate          = decay_rate,\n",
    "                verbose             = 0)\n",
    "    lr_epochs_simulated = lrs.simulate_lr(total_train_step, TRAIN_STEPS, False)\n",
    "\n",
    "    # second, extract the learning rates from the simulated epochs \n",
    "    return lr_epochs_simulated[\n",
    "                        epochs_continue['epoch_start']-1:\n",
    "                        epochs_continue['epoch_end']:\n",
    "                        1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 994,
     "referenced_widgets": [
      "d535a1b227994531a08d2d9a02baaadb",
      "f235368a8b5e4be18e2e3faae0d067b9",
      "b9ad7efc007e4c37ab165edf4aab02b3",
      "3f13a79969344276a5d410798953bbfc",
      "a0a49d26c2f74878ac28af98755855f2",
      "e853c26adccd44fa8d4e3b653ed7c3f3",
      "b2dd9e4677214910880dc880cc6d531d",
      "91a7c7223ace4886949e14703f241c61"
     ]
    },
    "executionInfo": {
     "elapsed": 69664,
     "status": "ok",
     "timestamp": 1605901892334,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifbYqdqoVX3NoViBOtgGwuQf_zCM-cnWaavU1TSA=s64",
      "userId": "08804338425967190040"
     },
     "user_tz": 480
    },
    "id": "nQ-jDYCOvxiy",
    "outputId": "91a32791-df27-4ee2-88e5-9034786b7445"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d535a1b227994531a08d2d9a02baaadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242303832.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing DROPBaseT5.\n",
      "\n",
      "Some layers of DROPBaseT5 were not initialized from the model checkpoint at t5-small and are newly initialized: ['loss']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps per epoch: 303\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ending LR: 2.450869942549616e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simulate and plot the learning rates for training\n",
    "if LR_DO_WARMUP_DECAY:\n",
    "    if EPOCHS_CONTINUE['doit']:\n",
    "        print('Continous training mode...')\n",
    "        lr_epochs_continue = get_continous_training_lr_list(TRAIN_STEPS * WARMP_UP_EPOCHS)\n",
    "        lrs_simple = LRSimpleScheduler(lr_epochs_continue)\n",
    "        total_train_step = (EPOCHS_CONTINUE['epoch_end'] - EPOCHS_CONTINUE['epoch_start']) * TRAIN_STEPS\n",
    "        lr_simple_epochs_simulated = lrs_simple.simulate_lr(total_train_step, TRAIN_STEPS)\n",
    "    else:\n",
    "        print('Nonstop training mode...')\n",
    "        total_train_step = EPOCHS * TRAIN_STEPS\n",
    "        lrs = LRScheduler(\n",
    "                    warmup_batches      = TRAIN_STEPS * WARMP_UP_EPOCHS, \n",
    "                    lr_warmup_start     = LR_WARMUP_START, \n",
    "                    lr_warmup_end       = LR_WARMUP_END, \n",
    "                    lr_min              = LR_MIN,\n",
    "                    decay_rate          = LR_DECAY_RATE_LINEAR,\n",
    "                    verbose             = 0)\n",
    "        \n",
    "        lrs.simulate_lr(total_train_step, TRAIN_STEPS)\n",
    "else:\n",
    "    print('Warning: No learning rate decay is scheduled...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 2293,
     "status": "ok",
     "timestamp": 1605903015352,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifbYqdqoVX3NoViBOtgGwuQf_zCM-cnWaavU1TSA=s64",
      "userId": "08804338425967190040"
     },
     "user_tz": 480
    },
    "id": "YzlkmTkEMwxp",
    "outputId": "80559d8a-0dc8-4f38-f3a1-f854aab67418"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/yintrigue/t5-numerical-reasoning/runs/1wy8ms1c?jupyter=true\" style=\"border:none;width:100%;height:420px\">\n",
       "                </iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.Run at 0x7f2397e356d8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WB_NAME_PREFIX = 'nt5-test'\n",
    "\n",
    "if SHOW_TENSORBOARD:\n",
    "    if PROCESSOR is 'TPU':\n",
    "        print('Warning: TPU requires Tensorboard logs to be saved to GCS...')\n",
    "    %load_ext tensorboard \n",
    "    %tensorboard --logdir '$LOG_PATH'\n",
    "    \n",
    "if SHOW_WANDB:\n",
    "    time = datetime.utcnow().strftime(f'-%m-%d-%H%M%S')\n",
    "    name = WB_NAME_PREFIX + '-' + T5_MODEL + time\n",
    "    wandb.init(project=WANDB_PROJECT_NAME, name=name)\n",
    "    clear_output()\n",
    "    %%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7699638,
     "status": "ok",
     "timestamp": 1605910784851,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifbYqdqoVX3NoViBOtgGwuQf_zCM-cnWaavU1TSA=s64",
      "userId": "08804338425967190040"
     },
     "user_tz": 480
    },
    "id": "_0Afk9YVtDKQ",
    "outputId": "e6b80823-f26d-4c02-e029-d2ce33dc333a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing DROPBaseT5.\n",
      "\n",
      "Some layers of DROPBaseT5 were not initialized from the model checkpoint at t5-small and are newly initialized: ['loss']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0461 - lr: 8.3150e-06\n",
      "Epoch 00001: val_loss improved from inf to 0.07167, saving model to /content/gdrive/My Drive/t5_math/branch_tim/_saves/t5-small_11-20-201154/model_best_chkpt_e_01_vl_0.07167161.h5\n",
      "303/303 [==============================] - 146s 481ms/step - loss: 0.0461 - lr: 8.3423e-06 - val_loss: 0.0717\n",
      "Epoch 2/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0095 - lr: 2.4980e-05\n",
      "Epoch 00002: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 128s 423ms/step - loss: 0.0095 - lr: 2.5007e-05 - val_loss: 0.0840\n",
      "Epoch 3/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0086 - lr: 4.1645e-05\n",
      "Epoch 00003: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 129s 425ms/step - loss: 0.0086 - lr: 4.1672e-05 - val_loss: 0.0757\n",
      "Epoch 4/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0103 - lr: 5.8310e-05\n",
      "Epoch 00004: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 128s 423ms/step - loss: 0.0103 - lr: 5.8337e-05 - val_loss: 0.1045\n",
      "Epoch 5/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0235 - lr: 7.4975e-05\n",
      "Epoch 00005: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 128s 422ms/step - loss: 0.0235 - lr: 7.5002e-05 - val_loss: 0.0941\n",
      "Epoch 6/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0136 - lr: 9.1640e-05\n",
      "Epoch 00006: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 129s 425ms/step - loss: 0.0136 - lr: 9.1667e-05 - val_loss: 0.0767\n",
      "Epoch 7/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 00007: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 125s 414ms/step - loss: 0.0117 - lr: 1.0000e-04 - val_loss: 0.1125\n",
      "Epoch 8/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0184 - lr: 9.9900e-05\n",
      "Epoch 00008: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0184 - lr: 9.9900e-05 - val_loss: 0.0739\n",
      "Epoch 9/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0133 - lr: 9.9701e-05\n",
      "Epoch 00009: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0133 - lr: 9.9701e-05 - val_loss: 0.0856\n",
      "Epoch 10/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0175 - lr: 9.9402e-05\n",
      "Epoch 00010: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 125s 414ms/step - loss: 0.0175 - lr: 9.9402e-05 - val_loss: 0.1324\n",
      "Epoch 11/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0100 - lr: 9.9006e-05\n",
      "Epoch 00011: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 126s 417ms/step - loss: 0.0100 - lr: 9.9006e-05 - val_loss: 0.1318\n",
      "Epoch 12/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0191 - lr: 9.8514e-05\n",
      "Epoch 00012: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 126s 417ms/step - loss: 0.0191 - lr: 9.8514e-05 - val_loss: 0.0957\n",
      "Epoch 13/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0229 - lr: 9.7926e-05\n",
      "Epoch 00013: val_loss did not improve from 0.07167\n",
      "303/303 [==============================] - 126s 417ms/step - loss: 0.0229 - lr: 9.7926e-05 - val_loss: 0.1125\n",
      "Epoch 14/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0186 - lr: 9.7246e-05\n",
      "Epoch 00014: val_loss improved from 0.07167 to 0.04934, saving model to /content/gdrive/My Drive/t5_math/branch_tim/_saves/t5-small_11-20-201154/model_best_chkpt_e_14_vl_0.04933894.h5\n",
      "303/303 [==============================] - 128s 424ms/step - loss: 0.0186 - lr: 9.7246e-05 - val_loss: 0.0493\n",
      "Epoch 15/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0123 - lr: 9.6474e-05\n",
      "Epoch 00015: val_loss did not improve from 0.04934\n",
      "303/303 [==============================] - 126s 415ms/step - loss: 0.0123 - lr: 9.6474e-05 - val_loss: 0.0827\n",
      "Epoch 16/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0113 - lr: 9.5613e-05\n",
      "Epoch 00016: val_loss did not improve from 0.04934\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0113 - lr: 9.5613e-05 - val_loss: 0.1746\n",
      "Epoch 17/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0146 - lr: 9.4667e-05\n",
      "Epoch 00017: val_loss did not improve from 0.04934\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0146 - lr: 9.4667e-05 - val_loss: 0.0858\n",
      "Epoch 18/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0137 - lr: 9.3637e-05\n",
      "Epoch 00018: val_loss did not improve from 0.04934\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0137 - lr: 9.3637e-05 - val_loss: 0.0789\n",
      "Epoch 19/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0143 - lr: 9.2526e-05\n",
      "Epoch 00019: val_loss did not improve from 0.04934\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0143 - lr: 9.2526e-05 - val_loss: 0.1258\n",
      "Epoch 20/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0080 - lr: 9.1339e-05\n",
      "Epoch 00020: val_loss did not improve from 0.04934\n",
      "303/303 [==============================] - 126s 415ms/step - loss: 0.0080 - lr: 9.1339e-05 - val_loss: 0.1040\n",
      "Epoch 21/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0113 - lr: 9.0078e-05\n",
      "Epoch 00021: val_loss improved from 0.04934 to 0.03735, saving model to /content/gdrive/My Drive/t5_math/branch_tim/_saves/t5-small_11-20-201154/model_best_chkpt_e_21_vl_0.03734756.h5\n",
      "303/303 [==============================] - 129s 426ms/step - loss: 0.0113 - lr: 9.0078e-05 - val_loss: 0.0373\n",
      "Epoch 22/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0074 - lr: 8.8747e-05\n",
      "Epoch 00022: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0074 - lr: 8.8747e-05 - val_loss: 0.1033\n",
      "Epoch 23/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0213 - lr: 8.7349e-05\n",
      "Epoch 00023: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0213 - lr: 8.7349e-05 - val_loss: 0.1256\n",
      "Epoch 24/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0121 - lr: 8.5889e-05\n",
      "Epoch 00024: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0121 - lr: 8.5889e-05 - val_loss: 0.0724\n",
      "Epoch 25/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0088 - lr: 8.4370e-05\n",
      "Epoch 00025: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0088 - lr: 8.4370e-05 - val_loss: 0.1431\n",
      "Epoch 26/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0114 - lr: 8.2797e-05\n",
      "Epoch 00026: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0114 - lr: 8.2797e-05 - val_loss: 0.0939\n",
      "Epoch 27/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0125 - lr: 8.1174e-05\n",
      "Epoch 00027: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0125 - lr: 8.1174e-05 - val_loss: 0.1653\n",
      "Epoch 28/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0105 - lr: 7.9504e-05\n",
      "Epoch 00028: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0105 - lr: 7.9504e-05 - val_loss: 0.1109\n",
      "Epoch 29/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0085 - lr: 7.7793e-05\n",
      "Epoch 00029: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0085 - lr: 7.7793e-05 - val_loss: 0.0936\n",
      "Epoch 30/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0075 - lr: 7.6044e-05\n",
      "Epoch 00030: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0075 - lr: 7.6044e-05 - val_loss: 0.1139\n",
      "Epoch 31/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0180 - lr: 7.4261e-05\n",
      "Epoch 00031: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0180 - lr: 7.4261e-05 - val_loss: 0.0395\n",
      "Epoch 32/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0240 - lr: 7.2450e-05\n",
      "Epoch 00032: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0240 - lr: 7.2450e-05 - val_loss: 0.0729\n",
      "Epoch 33/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0149 - lr: 7.0614e-05\n",
      "Epoch 00033: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0149 - lr: 7.0614e-05 - val_loss: 0.0969\n",
      "Epoch 34/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0092 - lr: 6.8758e-05\n",
      "Epoch 00034: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0092 - lr: 6.8758e-05 - val_loss: 0.1515\n",
      "Epoch 35/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0063 - lr: 6.6885e-05\n",
      "Epoch 00035: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0063 - lr: 6.6885e-05 - val_loss: 0.1494\n",
      "Epoch 36/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0095 - lr: 6.5000e-05\n",
      "Epoch 00036: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0095 - lr: 6.5000e-05 - val_loss: 0.1462\n",
      "Epoch 37/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0142 - lr: 6.3107e-05\n",
      "Epoch 00037: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0142 - lr: 6.3107e-05 - val_loss: 0.1195\n",
      "Epoch 38/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0112 - lr: 6.1209e-05\n",
      "Epoch 00038: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0112 - lr: 6.1209e-05 - val_loss: 0.0792\n",
      "Epoch 39/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0097 - lr: 5.9311e-05\n",
      "Epoch 00039: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 126s 414ms/step - loss: 0.0097 - lr: 5.9311e-05 - val_loss: 0.1130\n",
      "Epoch 40/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0168 - lr: 5.7417e-05\n",
      "Epoch 00040: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0168 - lr: 5.7417e-05 - val_loss: 0.1602\n",
      "Epoch 41/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0170 - lr: 5.5529e-05\n",
      "Epoch 00041: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0170 - lr: 5.5529e-05 - val_loss: 0.0918\n",
      "Epoch 42/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0163 - lr: 5.3651e-05\n",
      "Epoch 00042: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0163 - lr: 5.3651e-05 - val_loss: 0.0805\n",
      "Epoch 43/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0133 - lr: 5.1786e-05\n",
      "Epoch 00043: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0133 - lr: 5.1786e-05 - val_loss: 0.1062\n",
      "Epoch 44/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0085 - lr: 4.9939e-05\n",
      "Epoch 00044: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0085 - lr: 4.9939e-05 - val_loss: 0.1955\n",
      "Epoch 45/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0080 - lr: 4.8111e-05\n",
      "Epoch 00045: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0080 - lr: 4.8111e-05 - val_loss: 0.1101\n",
      "Epoch 46/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0086 - lr: 4.6305e-05\n",
      "Epoch 00046: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 414ms/step - loss: 0.0086 - lr: 4.6305e-05 - val_loss: 0.0493\n",
      "Epoch 47/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0080 - lr: 4.4524e-05\n",
      "Epoch 00047: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 126s 415ms/step - loss: 0.0080 - lr: 4.4524e-05 - val_loss: 0.0876\n",
      "Epoch 48/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0134 - lr: 4.2770e-05\n",
      "Epoch 00048: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 126s 416ms/step - loss: 0.0134 - lr: 4.2770e-05 - val_loss: 0.0909\n",
      "Epoch 49/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0174 - lr: 4.1046e-05\n",
      "Epoch 00049: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0174 - lr: 4.1046e-05 - val_loss: 0.1319\n",
      "Epoch 50/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0068 - lr: 3.9354e-05\n",
      "Epoch 00050: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 127s 418ms/step - loss: 0.0068 - lr: 3.9354e-05 - val_loss: 0.1532\n",
      "Epoch 51/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0119 - lr: 3.7695e-05\n",
      "Epoch 00051: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 126s 415ms/step - loss: 0.0119 - lr: 3.7695e-05 - val_loss: 0.1323\n",
      "Epoch 52/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0147 - lr: 3.6072e-05\n",
      "Epoch 00052: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 126s 414ms/step - loss: 0.0147 - lr: 3.6072e-05 - val_loss: 0.1300\n",
      "Epoch 53/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0146 - lr: 3.4486e-05\n",
      "Epoch 00053: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0146 - lr: 3.4486e-05 - val_loss: 0.1465\n",
      "Epoch 54/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0097 - lr: 3.2938e-05\n",
      "Epoch 00054: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0097 - lr: 3.2938e-05 - val_loss: 0.0943\n",
      "Epoch 55/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0090 - lr: 3.1429e-05\n",
      "Epoch 00055: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0090 - lr: 3.1429e-05 - val_loss: 0.1041\n",
      "Epoch 56/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0071 - lr: 2.9961e-05\n",
      "Epoch 00056: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0071 - lr: 2.9961e-05 - val_loss: 0.1080\n",
      "Epoch 57/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0153 - lr: 2.8534e-05\n",
      "Epoch 00057: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0153 - lr: 2.8534e-05 - val_loss: 0.0953\n",
      "Epoch 58/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0135 - lr: 2.7150e-05\n",
      "Epoch 00058: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0135 - lr: 2.7150e-05 - val_loss: 0.1256\n",
      "Epoch 59/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0080 - lr: 2.5808e-05\n",
      "Epoch 00059: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 412ms/step - loss: 0.0080 - lr: 2.5808e-05 - val_loss: 0.0869\n",
      "Epoch 60/60\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.0087 - lr: 2.4509e-05\n",
      "Epoch 00060: val_loss did not improve from 0.03735\n",
      "303/303 [==============================] - 125s 413ms/step - loss: 0.0087 - lr: 2.4509e-05 - val_loss: 0.1439\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Training can be performed on top of anotheran existing trained model. This\n",
    "# is required for both \"continuous training\" and \"stage training.\" Refer to the \n",
    "# global config section for more details on \"continuous training\" and \"stage training.\"\n",
    "\n",
    "# model loading configs\n",
    "TRAIN_LOAD_MODEL = False\n",
    "TRAIN_MODEL_PATH = f'{GDRIVE_REPO_PATH}/models/model_chkpt_e_42_vl_0.04173539.h5'\n",
    "TRAIN_MODEL = 't5-small'\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# BUILD MODEL\n",
    "\n",
    "keras.backend.clear_session() \n",
    "with strategy.scope(): \n",
    "    if TRAIN_LOAD_MODEL:    \n",
    "            model = DROPBaseT5.from_pretrained(TRAIN_MODEL)\n",
    "            model.compile()  \n",
    "            model.load_weights(TRAIN_MODEL_PATH)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=LR_CONSTANT)\n",
    "        model = DROPBaseT5.from_pretrained(T5_MODEL)\n",
    "        model.compile(optimizer=optimizer)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# BUILD CALLBACKS\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "# LR schedule\n",
    "if LR_DO_WARMUP_DECAY:\n",
    "    if EPOCHS_CONTINUE['doit']:\n",
    "        # first, simulate the entire training schedule as if it is nonstop\n",
    "        lr_epochs_continue = get_continous_training_lr_list(TRAIN_STEPS * WARMP_UP_EPOCHS)\n",
    "        lr_epochs_simulated = lrs.simulate_lr(total_train_step, TRAIN_STEPS, False)\n",
    "\n",
    "        # second, extract the learning rates from the simulated epochs \n",
    "        lr_epochs_continue = lr_epochs_simulated[\n",
    "                                            EPOCHS_CONTINUE['epoch_start']-1:\n",
    "                                            EPOCHS_CONTINUE['epoch_end']:\n",
    "                                            1]\n",
    "\n",
    "        # lastly, build the LR schedule for the current training\n",
    "        lrs = LRSimpleScheduler(lr_epochs_continue)\n",
    "    else:\n",
    "        lrs = LRScheduler(\n",
    "                    warmup_batches   = TRAIN_STEPS * WARMP_UP_EPOCHS, \n",
    "                    lr_warmup_start  = LR_WARMUP_START, \n",
    "                    lr_warmup_end    = LR_WARMUP_END, \n",
    "                    lr_min           = LR_MIN,\n",
    "                    decay_rate       = LR_DECAY_RATE_LINEAR,\n",
    "                    verbose          = 0)\n",
    "    callbacks.append(lrs)\n",
    "\n",
    "# callback to save best model \n",
    "time = datetime.utcnow().strftime(f'%m-%d-%H%M%S')\n",
    "model_save_dir = f'{MODEL_SAVE_PATH}/{T5_MODEL}_{time}'\n",
    "if SAVE_MODEL_CHECKPOINTS_LOCAL:\n",
    "    os.mkdir(model_save_dir)\n",
    "    checkpoint_path = f'{model_save_dir}/model_best_chkpt_' + 'e_{epoch:02d}_vl_{val_loss:.8f}.h5'\n",
    "\n",
    "    if SAVE_MODEL_WEIGHTS_ONLY:\n",
    "        options = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
    "    else:\n",
    "        options = tf.saved_model.SaveOptions(experimental_io_device=\"/job:localhost\")\n",
    "    callback_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "                                            filepath=checkpoint_path,\n",
    "                                            save_weights_only=SAVE_MODEL_WEIGHTS_ONLY,\n",
    "                                            monitor='val_loss',\n",
    "                                            mode='min',\n",
    "                                            save_best_only=SAVE_BEST_MODEL_ONLY,\n",
    "                                            options=options,\n",
    "                                            verbose=1)\n",
    "    callbacks.append(callback_checkpoint)\n",
    "\n",
    "# callback to render tensorboard\n",
    "if SHOW_TENSORBOARD:\n",
    "    # TENSORBOARD_LOG_PATH must a GCP storage bucket if the model is running on TPU\n",
    "    callback_tensorboard = keras.callbacks.TensorBoard(\n",
    "                                                log_dir=TENSORBOARD_LOG_PATH, \n",
    "                                                histogram_freq=1)\n",
    "    callbacks.append(callback_tensorboard)\n",
    "\n",
    "# callback for wandb tracker\n",
    "if SHOW_WANDB:\n",
    "    callback_wandb = wandb.keras.WandbCallback(\n",
    "                                        monitor='val_loss',\n",
    "                                        mode='min',\n",
    "                                        save_model=SAVE_MODEL_CHECKPOINTS_WANDB,\n",
    "                                        save_weights_only=SAVE_MODEL_WEIGHTS_ONLY)\n",
    "    callbacks.append(callback_wandb)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# TRAINING\n",
    "\n",
    "# NOTE: Do NOT specify batch_size!\n",
    "# DOC: Integer or None. Number of samples per gradient update. If unspecified, \n",
    "# batch_size will default to 32. Do not specify the batch_size if your data is \n",
    "# in the form of datasets, generators, or keras.utils.Sequence instances (since \n",
    "# they generate batches).\n",
    "\n",
    "# TRAIN_STEPS = math.ceil(DROP_TRAIN_EXAMPLE_COUNT / BATCH_SIZE)\n",
    "# VALID_STEPS = math.ceil(DROP_DEV_EXAMPLE_COUNT / BATCH_SIZE)\n",
    "train_steps = math.ceil(int(sum(max_ds_size)) / BATCH_SIZE)#DROP_TRAIN_EXAMPLE_COUNT / BATCH_SIZE)\n",
    "valid_steps = math.ceil(DROP_DEV_EXAMPLE_COUNT / BATCH_SIZE)\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "model.fit(\n",
    "    x=train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    verbose=1)\n",
    "\n",
    "if SAVE_TRAINED_MODEL:\n",
    "    # save trained model\n",
    "    # doc: https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model\n",
    "    # not working due to an odd bug...\n",
    "    # model.save(f'{model_save_dir}/model_full_trained', save_format='tf') \n",
    "    \n",
    "    model.save_weights(f'{model_save_dir}/model_train_end.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-8fXxHVF3c8"
   },
   "source": [
    "## Performance Evaluation\n",
    "The section includes classes and codes used to evaluate the performance of the model according to the [DROP paper](https://allennlp.org/drop). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELCG2elfHd99"
   },
   "outputs": [],
   "source": [
    "class DropPerformanceMeasure:\n",
    "    def __init__(self, tokenizer: transformers.PreTrainedTokenizer) -> None:\n",
    "        self.__metric = DropEmAndF1()\n",
    "        self.__parsed_examples = []\n",
    "        self.__tokenizer = tokenizer\n",
    "        self.__preds = {}\n",
    "        self.__time = datetime.utcnow().strftime(f'%m%d%H%M%S')\n",
    "        self.__csv_save_path= ''\n",
    "\n",
    "    def load_parse_drop_json_for_eval(\n",
    "                    self,\n",
    "                    json_url: str = DROP_DEV_JSON, \n",
    "                    example_count: int = DROP_DEV_EXAMPLE_COUNT,\n",
    "                    test_mode: bool = False) -> int:\n",
    "        \"\"\" Parse Drop json and reconstruct the DROP dataset. \n",
    "        Returns:\n",
    "            (int) Total number of examples parsed.\n",
    "        \"\"\"\n",
    "        with open(json_url) as f:\n",
    "            drop_dict = json.load(f)\n",
    "\n",
    "            # examples = []\n",
    "            input_ids                  = []\n",
    "            attention_masks            = []\n",
    "            answer_types               = []\n",
    "            answer_dicts               = []\n",
    "            validated_answers_dicts    = []\n",
    "            query_ids                  = []\n",
    "            print(f'Parsing \"{json_url}\"...')\n",
    "            with tqdm(total=example_count) as pbar:\n",
    "                for key, json_example in drop_dict.items():\n",
    "                    context = json_example['passage']\n",
    "                    for qa_dict in json_example['qa_pairs']:\n",
    "                        question = qa_dict['question']\n",
    "                        answer = qa_dict['answer']\n",
    "                        _, answer_type = self.__parse_answer(qa_dict['answer'])\n",
    "                        query_id = qa_dict['query_id']\n",
    "                        if 'validated_answers' in qa_dict:\n",
    "                            validated_answers = qa_dict['validated_answers']\n",
    "                        else:\n",
    "                            validated_answers = None\n",
    "                        if answer is None:\n",
    "                            print(f'Skip {query_id} due to None answer...')\n",
    "                            continue\n",
    "\n",
    "                        input_text = f\"answer_me: {question} context: {context}\"\n",
    "                        encoded_query = self.__tokenizer(\n",
    "                                                    input_text, \n",
    "                                                    return_tensors='tf', \n",
    "                                                    padding='max_length', \n",
    "                                                    truncation=True, \n",
    "                                                    max_length=ENCODER_MAX_LEN)\n",
    "\n",
    "                        input_ids.append(encoded_query[\"input_ids\"][0])                  \n",
    "                        attention_masks.append(encoded_query[\"attention_mask\"][0])            \n",
    "                        answer_types.append(answer_type)               \n",
    "                        answer_dicts.append(answer)               \n",
    "                        validated_answers_dicts.append(validated_answers)    \n",
    "                        query_ids.append(query_id)                  \n",
    "\n",
    "                        pbar.update(1)\n",
    "\n",
    "                        if test_mode:\n",
    "                            # parse only one passge & its questions\n",
    "                            break\n",
    "        \n",
    "        self.__parsed_examples = {\n",
    "                'input_ids'                 : input_ids,\n",
    "                'attention_mask'            : attention_masks,\n",
    "                'answer_type'               : answer_types,\n",
    "                'answer_dict'               : answer_dicts,\n",
    "                'validated_answers_dict'    : validated_answers_dicts,\n",
    "                'query_id'                  : query_ids\n",
    "        }\n",
    "\n",
    "        return len(input_ids)\n",
    "\n",
    "    def __parse_answer(\n",
    "                    self, \n",
    "                    answer_dict: dict, \n",
    "                    spans_sep_token: str = '<ss>') -> str:\n",
    "        \"\"\"Parse the answer json node to string. Spans answers will be seperated \n",
    "        using spans_sep_token.\n",
    "        \"\"\"\n",
    "        number = answer_dict['number'].strip()\n",
    "        if number:\n",
    "            return number, 'n'\n",
    "        \n",
    "        spans = answer_dict['spans']\n",
    "        spans_str = spans_sep_token.join([span.strip() for span in spans if span.strip()])\n",
    "        if spans_str:\n",
    "            if len(spans) > 1:\n",
    "                return spans_str, 's'\n",
    "            else:\n",
    "                return spans_str, 'ss'\n",
    "        \n",
    "        date = answer_dict['date']\n",
    "        if len(date) != 3:\n",
    "            return None, None\n",
    "        date = ' '.join([d.strip() for d in [date['day'], date['month'], date['year']] if d.strip()])\n",
    "        if date:\n",
    "            return date, 'd'\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    def __parse_validated_answers(\n",
    "                        self, \n",
    "                        validated_answers: list, \n",
    "                        sep_token: str = '<sv>') -> str:\n",
    "        \"\"\"Parse validated_answers json node to string using sep_token.\n",
    "        \"\"\"\n",
    "        if validated_answers is None:\n",
    "            return ''\n",
    "\n",
    "        answers = []\n",
    "        for answer_dict in validated_answers:\n",
    "            ans, _ = self.__parse_answer(answer_dict)\n",
    "            answers.append(ans)\n",
    "        return sep_token.join([a.strip() for a in answers if a.strip()])\n",
    "\n",
    "    def __to_tf_dataset(\n",
    "            self,\n",
    "            parsed_examples: dict, \n",
    "            max_len: int = 512,\n",
    "            batch_size: int = BATCH_SIZE,\n",
    "            test_reduce_data: bool = False) -> tf.data.Dataset:\n",
    "        # convert to hugging face ds\n",
    "        ds = datasets.Dataset.from_dict(parsed_examples)\n",
    "        if test_reduce_data:\n",
    "            test_ds = datasets.Dataset.from_dict(test_ds[:10])\n",
    "            test_quids = test_quids[:10]\n",
    "        \n",
    "        # convert to tf ds\n",
    "        columns = [\n",
    "                'input_ids', \n",
    "                'attention_mask']\n",
    "        ds.set_format(type='tensorflow', columns=columns)\n",
    "        ds_tf = {x: ds[x].to_tensor(default_value=0, shape=[None, max_len]) \n",
    "                        for x in [\n",
    "                            'input_ids', \n",
    "                            'attention_mask']}        \n",
    "        ds_tf = tf.data.Dataset.from_tensor_slices(ds_tf)\n",
    "        ds_tf = ds_tf.repeat(1)\n",
    "        ds_tf = ds_tf.batch(batch_size)\n",
    "        ds_tf = ds_tf.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return ds_tf\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            model: transformers.TFT5ForConditionalGeneration, \n",
    "            ds_example_count: int = 0,\n",
    "            decoder_max_len: int = DECODER_MAX_LEN,\n",
    "            batch_size: int = BATCH_SIZE,\n",
    "            dry_run: bool = False) -> dict:\n",
    "        \"\"\"Predict and decode using the trained T5 model.\n",
    "        Refs:\n",
    "            https://tinyurl.com/yxlr4jmb\n",
    "            https://tinyurl.com/y53l7vfm\n",
    "        Returns:\n",
    "            (tuple) (preds, labels, types)\n",
    "        \"\"\"\n",
    "        if len(self.__parsed_examples) == 0:\n",
    "            return None\n",
    "        \n",
    "        def fix_output_spacing(s: str) -> str:\n",
    "            \"\"\"Fixing the odd bug that numerical numbers are losing a whitespace in \n",
    "            front after adding digits to special tokens.\n",
    "            \"\"\"\n",
    "            match = re.compile(r'([a-z]|,|-)(\\d)')\n",
    "            s = re.sub(match, r'\\1 \\2', s)\n",
    "            match = re.compile(r'(\\d|[a-z])( )?(-)( )?(\\d|[a-z])')\n",
    "            s = re.sub(match, r'\\1\\3\\5', s)\n",
    "            return s\n",
    "\n",
    "        ds = self.__to_tf_dataset(self.__parsed_examples, batch_size=batch_size)        \n",
    "        with tqdm(total=ds_example_count) as pbar:\n",
    "            inputs, preds = [], []\n",
    "            for batch in ds:\n",
    "                outs_pred = model.generate(\n",
    "                                input_ids=batch['input_ids'], \n",
    "                                attention_mask=batch['attention_mask'],\n",
    "                                max_length=decoder_max_len)\n",
    "                \n",
    "                # detokenize and fix the output whitespace bug in front of numbers\n",
    "                outs_pred = [fix_output_spacing(self.__tokenizer.decode(ids))\n",
    "                             for ids in outs_pred]\n",
    "                preds.extend(outs_pred)\n",
    "\n",
    "                # inputs\n",
    "                ins = batch['input_ids']\n",
    "                ins = [self.__tokenizer.decode(input) for input in ins]\n",
    "                inputs.extend(ins)\n",
    "\n",
    "                # update progress bar\n",
    "                pbar.update(len(batch['input_ids']))\n",
    "                if dry_run:\n",
    "                    # process only one batch if dry run\n",
    "                    break\n",
    "                    \n",
    "        self.__preds['inputs'] = inputs\n",
    "        self.__preds['predictions'] = preds\n",
    "        self.__preds['answer_dict'] = self.__parsed_examples['answer_dict']\n",
    "        self.__preds['answer_type'] = self.__parsed_examples['answer_type']\n",
    "        self.__preds['query_id'] = self.__parsed_examples['query_id']\n",
    "        self.__preds['validated_answers_dict'] = self.__parsed_examples['validated_answers_dict']\n",
    "\n",
    "        return self.__preds\n",
    "\n",
    "    def evaluate(\n",
    "            self, \n",
    "            query_id: list,\n",
    "            answer_dict: list,\n",
    "            validated_answers_dict: list,\n",
    "            answer_type: list,\n",
    "            predictions: list,\n",
    "            prediction_spans_sep_token: str = '<ss>',\n",
    "            print_reports: bool = True) -> tuple:\n",
    "        \"\"\"Evaluate a batch.\n",
    "        Refs:\n",
    "            https://tinyurl.com/yyuuvc5b\n",
    "            https://tinyurl.com/y6rmrp85\n",
    "        \"\"\"\n",
    "        metric_overall = DropEmAndF1()\n",
    "        errors = []\n",
    "        for qid, ans, ans_validated, ans_type, pred in zip(\n",
    "                                                query_id,\n",
    "                                                answer_dict, \n",
    "                                                validated_answers_dict,\n",
    "                                                answer_type,\n",
    "                                                predictions):\n",
    "            # build predction for evaluation\n",
    "            if ans_type == 's': # spans answer\n",
    "                prediction_parsed = pred.split(prediction_spans_sep_token)\n",
    "            else:\n",
    "                prediction_parsed = pred\n",
    "\n",
    "            # build truths for evaluation\n",
    "            ground_truths = [ans]\n",
    "            if ans_validated is not None:\n",
    "                ground_truths.extend(ans_validated)\n",
    "            \n",
    "            # track performance per example\n",
    "            metric_example = DropEmAndF1()\n",
    "            metric_example(prediction=prediction_parsed, ground_truths=ground_truths)\n",
    "            example_exact_match, exampe_f1_score = metric_example.get_metric()\n",
    "            \n",
    "            ans_parsed, _ = self.__parse_answer(ans)\n",
    "            ans_val = self.__parse_validated_answers(ans_validated)\n",
    "            \n",
    "            # add to error report\n",
    "            if exampe_f1_score < 1:\n",
    "                example_error = {\n",
    "                        'query_id'          : qid,\n",
    "                        'prediction'        : pred,\n",
    "                        'answer'            : ans_parsed,\n",
    "                        'validated_answers' : ans_val,\n",
    "                        'f1'                : exampe_f1_score,\n",
    "                        'exact_match'       : example_exact_match\n",
    "                }\n",
    "                errors.append(example_error)\n",
    "\n",
    "            # update metrics\n",
    "            metric_overall(prediction=prediction_parsed, ground_truths=ground_truths)\n",
    "\n",
    "        exact_match, f1_score = metric_overall.get_metric()\n",
    "        return exact_match, f1_score, errors\n",
    "    \n",
    "    def evaluate_by_type(\n",
    "                    self, \n",
    "                    prediction_spans_sep_token: str = '<ss>',\n",
    "                    print_reports: bool = True,\n",
    "                    error_print_count: int = 100,\n",
    "                    save_dir: str = None) -> tuple:\n",
    "        \"\"\"Evaluate the performance by question type.\n",
    "        \"\"\"\n",
    "        predictions     = np.array(self.__preds['predictions'])\n",
    "        types           = np.array(self.__preds['answer_type'])\n",
    "        qids            = np.array(self.__preds['query_id'])\n",
    "        inputs          = np.array(self.__preds['inputs'])\n",
    "        answer_dict     = np.array(self.__preds['answer_dict'])\n",
    "        vals            = np.array(self.__preds['validated_answers_dict'])\n",
    "        \n",
    "        types   = types.reshape(types.shape[0])\n",
    "        qids    = qids.reshape(qids.shape[0])\n",
    "        inputs  = inputs.reshape(inputs.shape[0])\n",
    "\n",
    "        types_n     = types=='n'\n",
    "        types_d     = types=='d'\n",
    "        types_ss    = types=='ss'\n",
    "        types_s     = types=='s'\n",
    "\n",
    "        types_n     = types_n.reshape(types_n.shape[0])\n",
    "        types_d     = types_d.reshape(types_d.shape[0])\n",
    "        types_ss    = types_ss.reshape(types_ss.shape[0])\n",
    "        types_s     = types_s.reshape(types_s.shape[0])\n",
    "\n",
    "        number_eval = self.evaluate(\n",
    "                        qids[types_n].tolist(),\n",
    "                        answer_dict[types_n].tolist(),\n",
    "                        vals[types_n].tolist(),\n",
    "                        types[types_n].tolist(),\n",
    "                        predictions[types_n].tolist(),\n",
    "                        prediction_spans_sep_token=prediction_spans_sep_token)\n",
    "\n",
    "        date_eval = self.evaluate(\n",
    "                        qids[types_d].tolist(),\n",
    "                        answer_dict[types_d].tolist(),\n",
    "                        vals[types_d].tolist(),\n",
    "                        types[types_d].tolist(),\n",
    "                        predictions[types_d].tolist(),\n",
    "                        prediction_spans_sep_token=prediction_spans_sep_token)\n",
    "        span_eval = self.evaluate(\n",
    "                        qids[types_ss].tolist(),\n",
    "                        answer_dict[types_ss].tolist(),\n",
    "                        vals[types_ss].tolist(),\n",
    "                        types[types_ss].tolist(),\n",
    "                        predictions[types_ss].tolist(),\n",
    "                        prediction_spans_sep_token=prediction_spans_sep_token)\n",
    "        spans_eval = self.evaluate(\n",
    "                        qids[types_s].tolist(),\n",
    "                        answer_dict[types_s].tolist(),\n",
    "                        vals[types_s].tolist(),\n",
    "                        types[types_s].tolist(),\n",
    "                        predictions[types_s].tolist(),\n",
    "                        prediction_spans_sep_token=prediction_spans_sep_token)\n",
    "        \n",
    "        evals_cat = {\n",
    "            'number'    : {'exact_match': number_eval[0], 'f1': number_eval[1]},\n",
    "            'date'      : {'exact_match': date_eval[0], 'f1': date_eval[1]},\n",
    "            'span'      : {'exact_match': span_eval[0], 'f1': span_eval[1]},\n",
    "            'spans'     : {'exact_match': spans_eval[0], 'f1': spans_eval[1]}\n",
    "        }\n",
    "        errors_cat = {\n",
    "            'number'    : number_eval[2],\n",
    "            'date'      : date_eval[2],\n",
    "            'sapn'      : span_eval[2],\n",
    "            'spans'     : spans_eval[2]\n",
    "        }\n",
    "\n",
    "        # generate reports\n",
    "        print('\\n\\n')\n",
    "        print('CATEGORY PERFORMANCE')\n",
    "        df_cat = pd.DataFrame(evals_cat)\n",
    "        display(df_cat)\n",
    "\n",
    "        print('\\n\\n')\n",
    "        print('ERROR ANALYSIS')\n",
    "        for item in errors_cat.items():\n",
    "            q_type = item[0]\n",
    "            \n",
    "            # print\n",
    "            print(f'Question Type: {q_type.capitalize()}')\n",
    "            df = pd.DataFrame(item[1])\n",
    "            if df.shape[0] > error_print_count:\n",
    "                df_sample = df.sample(n=error_print_count)\n",
    "            else:\n",
    "                df_sample = df\n",
    "            display(df_sample)\n",
    "            print('\\n')\n",
    "\n",
    "            # save\n",
    "            if save_dir is not None:\n",
    "                path = f'{self.__csv_save_path}/{save_dir}/performace_reports_{self.__time}'\n",
    "            else:\n",
    "                path = f'{self.__csv_save_path}/performace_reports_{self.__time}'\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            df_cat.to_csv(f'{path}/performance_category.csv')\n",
    "            df_sample.to_csv(f'{path}/ea_{q_type}_sample_{df_sample.shape[0]}.csv')\n",
    "            df.to_csv(f'{path}/ea_{q_type}_full_{df.shape[0]}.csv')\n",
    "        \n",
    "        print(f\"\\nError reports have been saved to {path}!\")\n",
    "\n",
    "        return evals_cat, errors_cat\n",
    "\n",
    "    def __get_errors(\n",
    "                self,\n",
    "                predictions: np.ndarray,\n",
    "                truths: np.ndarray,\n",
    "                qids: np.ndarray,\n",
    "                inputs: np.ndarray) -> dict:\n",
    "        filter_ = predictions != truths\n",
    "        return {\n",
    "            'query_ids': qids[filter_],\n",
    "            'predictions': predictions[filter_], \n",
    "            'truths': truths[filter_],\n",
    "            'inputs': inputs[filter_]}\n",
    "\n",
    "    def evaluate_all(\n",
    "                self, \n",
    "                save_dir: str = None,\n",
    "                prediction_spans_sep_token: str = '<ss>') -> None:\n",
    "        exact_match, f1_score, _ = self.evaluate(\n",
    "                                        self.__preds['query_id'], \n",
    "                                        self.__preds['answer_dict'], \n",
    "                                        self.__preds['validated_answers_dict'],\n",
    "                                        self.__preds['answer_type'],\n",
    "                                        self.__preds['predictions'],\n",
    "                                        prediction_spans_sep_token=prediction_spans_sep_token)\n",
    "        \"\"\"Evaluate the performance across the entire dataset.\n",
    "        \"\"\"\n",
    "        # generate eval report\n",
    "        print('')\n",
    "        print('OVERALL PERFORMANCE')\n",
    "        evals_overall = {'exact_match': exact_match, 'f1': f1_score}\n",
    "        df_overall = pd.DataFrame(evals_overall, index=[0])\n",
    "        display(df_overall.style.hide_index())\n",
    "\n",
    "        # save report\n",
    "        if save_dir is not None:\n",
    "            path = f'{self.__csv_save_path}/{save_dir}/performace_reports_{self.__time}'\n",
    "            \n",
    "        else:\n",
    "            path = f'{self.__csv_save_path}/performace_reports_{self.__time}'\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        df_overall.to_csv(f'{path}/performance_overall.csv')\n",
    "\n",
    "    def get_eval_reports(\n",
    "                    self, \n",
    "                    csv_save_path: str,\n",
    "                    prediction_spans_sep_token: str = '<ss>',\n",
    "                    print_reports: bool = True,\n",
    "                    grouping_dir: str = None) -> None:\n",
    "        \"\"\"Function to produce the performance evaluation report.\n",
    "        Args:\n",
    "            csv_save_path: Path to the folder where the reports will be saved to.\n",
    "            prediction_spans_sep_token: Token used to separate the different answers for spans.\n",
    "            print_reports: Set true to print sample reports in the notebook.\n",
    "            grouping_dir: Optional; folder to be created under csv_save_path for saving the reports.\n",
    "        \"\"\"\n",
    "        self.__csv_save_path = csv_save_path\n",
    "        self.evaluate_all(\n",
    "                    save_dir=grouping_dir, \n",
    "                    prediction_spans_sep_token=prediction_spans_sep_token)\n",
    "        self.evaluate_by_type(\n",
    "                    save_dir=grouping_dir,\n",
    "                    prediction_spans_sep_token=prediction_spans_sep_token)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.__metric = DropEmAndF1()\n",
    "        self.__parsed_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a96Mnybas_WF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL PERFORMANCE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_57dc0558_2ade_11eb_a4f0_0242ac1c0002\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >exact_match</th>        <th class=\"col_heading level0 col1\" >f1</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_57dc0558_2ade_11eb_a4f0_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.655621</td>\n",
       "                        <td id=\"T_57dc0558_2ade_11eb_a4f0_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.690578</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc55dcf2198>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CATEGORY PERFORMANCE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "      <th>span</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>exact_match</th>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.452229</td>\n",
       "      <td>0.652262</td>\n",
       "      <td>0.340388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.693814</td>\n",
       "      <td>0.525032</td>\n",
       "      <td>0.707434</td>\n",
       "      <td>0.614974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               number      date      span     spans\n",
       "exact_match  0.693333  0.452229  0.652262  0.340388\n",
       "f1           0.693814  0.525032  0.707434  0.614974"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "ERROR ANALYSIS\n",
      "Question Type: Number\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>answer</th>\n",
       "      <th>validated_answers</th>\n",
       "      <th>f1</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>6a9fd578-e061-4d82-9473-4e521d0abe11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0&lt;sv&gt;0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>5ba24f17-a279-43a8-bd43-09740171bf6f</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>14&lt;sv&gt;14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>d1ba4817-b6be-4cbe-b1cf-006df277fdb6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3&lt;sv&gt;3&lt;sv&gt;3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>30e30004-e6d0-4fa4-919d-6e5d3633221c</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>8c0cbfc6-298b-4347-b15c-2b8fcae61d69</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5&lt;sv&gt;5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>aea447e4-7e3e-470e-a815-85be1f5be656</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3&lt;sv&gt;3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>0d640bb1-77c1-4176-83a7-f0a1ea657a23</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>9680fb92-96bc-4a7f-9600-9f4998e4869f</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>20&lt;sv&gt;20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>31116808-3351-443e-8852-5b94a0f17dca</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4&lt;sv&gt;4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>b8c51c5c-b760-4ce1-80cd-63055787ebac</td>\n",
       "      <td>1040016</td>\n",
       "      <td>950624</td>\n",
       "      <td>950624&lt;sv&gt;950624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  query_id prediction  ...   f1 exact_match\n",
       "83    6a9fd578-e061-4d82-9473-4e521d0abe11          1  ...  0.0         0.0\n",
       "726   5ba24f17-a279-43a8-bd43-09740171bf6f          4  ...  0.0         0.0\n",
       "847   d1ba4817-b6be-4cbe-b1cf-006df277fdb6          5  ...  0.0         0.0\n",
       "1146  30e30004-e6d0-4fa4-919d-6e5d3633221c         18  ...  0.0         0.0\n",
       "568   8c0cbfc6-298b-4347-b15c-2b8fcae61d69          1  ...  0.0         0.0\n",
       "...                                    ...        ...  ...  ...         ...\n",
       "1097  aea447e4-7e3e-470e-a815-85be1f5be656         14  ...  0.0         0.0\n",
       "1193  0d640bb1-77c1-4176-83a7-f0a1ea657a23          3  ...  0.0         0.0\n",
       "1637  9680fb92-96bc-4a7f-9600-9f4998e4869f         10  ...  0.0         0.0\n",
       "806   31116808-3351-443e-8852-5b94a0f17dca          5  ...  0.0         0.0\n",
       "773   b8c51c5c-b760-4ce1-80cd-63055787ebac    1040016  ...  0.0         0.0\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question Type: Date\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>answer</th>\n",
       "      <th>validated_answers</th>\n",
       "      <th>f1</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4ef57f34-8c7c-4500-bc1f-2d2512e35523</td>\n",
       "      <td>1944</td>\n",
       "      <td>1943</td>\n",
       "      <td>1943&lt;sv&gt;1943&lt;sv&gt;1943&lt;sv&gt;1943</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0903294b-08fc-4061-937a-44890a55ca78</td>\n",
       "      <td>1944</td>\n",
       "      <td>1943</td>\n",
       "      <td>1943&lt;sv&gt;1943&lt;sv&gt;1943</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7c72066b-f148-4554-8b81-4d74fbe61101</td>\n",
       "      <td>1944</td>\n",
       "      <td>1943</td>\n",
       "      <td>1942&lt;sv&gt;1943</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45c95f84-92e4-4d83-b426-894687613da0</td>\n",
       "      <td>1683</td>\n",
       "      <td>1698</td>\n",
       "      <td>1698&lt;sv&gt;1698</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a76d9d78-2e31-49d9-9562-2abc3f758943</td>\n",
       "      <td>2002</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000&lt;sv&gt;2000&lt;sv&gt;2000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>35473529-30cf-4aaa-9aa2-4e59bc30a970</td>\n",
       "      <td>20 November 1996</td>\n",
       "      <td>27 November 1996</td>\n",
       "      <td>27 November 1996&lt;sv&gt;27</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>494a271f-d9cb-4861-8b65-4c9411684b5b</td>\n",
       "      <td>1996</td>\n",
       "      <td>1997</td>\n",
       "      <td>1977&lt;sv&gt;1997</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>8b405cec-665e-4f11-b3b4-698e6ea3b69c</td>\n",
       "      <td>1995</td>\n",
       "      <td>1980</td>\n",
       "      <td>1980&lt;sv&gt;1980</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>923c1673-9a62-4bef-9c45-6048d8f5ad09</td>\n",
       "      <td>1900</td>\n",
       "      <td>1950</td>\n",
       "      <td>1950&lt;sv&gt;1950</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>fd9f4aa9-62e0-4896-a03a-ac452679fac7</td>\n",
       "      <td>2014</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012&lt;sv&gt;2012</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                query_id        prediction  ...    f1 exact_match\n",
       "0   4ef57f34-8c7c-4500-bc1f-2d2512e35523              1944  ...  0.00         0.0\n",
       "1   0903294b-08fc-4061-937a-44890a55ca78              1944  ...  0.00         0.0\n",
       "2   7c72066b-f148-4554-8b81-4d74fbe61101              1944  ...  0.00         0.0\n",
       "3   45c95f84-92e4-4d83-b426-894687613da0              1683  ...  0.00         0.0\n",
       "4   a76d9d78-2e31-49d9-9562-2abc3f758943              2002  ...  0.00         0.0\n",
       "..                                   ...               ...  ...   ...         ...\n",
       "81  35473529-30cf-4aaa-9aa2-4e59bc30a970  20 November 1996  ...  0.67         0.0\n",
       "82  494a271f-d9cb-4861-8b65-4c9411684b5b              1996  ...  0.00         0.0\n",
       "83  8b405cec-665e-4f11-b3b4-698e6ea3b69c              1995  ...  0.00         0.0\n",
       "84  923c1673-9a62-4bef-9c45-6048d8f5ad09              1900  ...  0.00         0.0\n",
       "85  fd9f4aa9-62e0-4896-a03a-ac452679fac7              2014  ...  0.00         0.0\n",
       "\n",
       "[86 rows x 6 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question Type: Sapn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>answer</th>\n",
       "      <th>validated_answers</th>\n",
       "      <th>f1</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>fad839fe-6bdf-49c0-b104-6392d17a12b5</td>\n",
       "      <td>taking intransigent negotiating stances</td>\n",
       "      <td>Russian Revolution of 1905</td>\n",
       "      <td>Russian Revolution&lt;sv&gt;Russian Revolution of 1905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>fca7116f-43b4-4fb3-95b9-350f39057dfa</td>\n",
       "      <td>Chiefs</td>\n",
       "      <td>Bears</td>\n",
       "      <td>Bears&lt;sv&gt;Bears</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>c438c440-274e-4cf2-aafb-520bfb23f40b</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Acalan</td>\n",
       "      <td>native populations&lt;sv&gt;Acalan&lt;sv&gt;Acalan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>01521a1d-a116-4b99-90c0-913d2f7e86be</td>\n",
       "      <td>Bengals</td>\n",
       "      <td>Bears</td>\n",
       "      <td>the Bears&lt;sv&gt;Bears</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>2f3a94a3-0b3a-4a3f-a1bd-a061a57cc1ae</td>\n",
       "      <td>Howes</td>\n",
       "      <td>Howe</td>\n",
       "      <td>Howe&lt;sv&gt;Howe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>c962f3f0-1cec-4bf0-81a6-f23db872347e</td>\n",
       "      <td>the Bengals</td>\n",
       "      <td>Jaguars</td>\n",
       "      <td>Jaguars&lt;sv&gt;Jaguars</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>fd42f71a-a133-480f-b1f2-9ab60e3cbf10</td>\n",
       "      <td>Plaxico Burress</td>\n",
       "      <td>Lawrence Tynes</td>\n",
       "      <td>Lawrence Tynes&lt;sv&gt;Lawrence Tynes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>d070b0bb-cf79-4d15-af15-ea62c9fe6812</td>\n",
       "      <td>2</td>\n",
       "      <td>bulbar polio</td>\n",
       "      <td>bulbar polio&lt;sv&gt;bulbar polio</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>4af89ed2-3a8e-4c71-8f04-eb7aa0db7ba1</td>\n",
       "      <td>Outlander novels</td>\n",
       "      <td>Waverley novels</td>\n",
       "      <td>Waverley novels&lt;sv&gt;Waverley novels</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>9ddd9bd3-d0cc-4bc8-8caa-6ff5c30e4f98</td>\n",
       "      <td>Philip Rivers</td>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>Brees&lt;sv&gt;Brees</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 query_id  ... exact_match\n",
       "361  fad839fe-6bdf-49c0-b104-6392d17a12b5  ...         0.0\n",
       "392  fca7116f-43b4-4fb3-95b9-350f39057dfa  ...         0.0\n",
       "312  c438c440-274e-4cf2-aafb-520bfb23f40b  ...         0.0\n",
       "322  01521a1d-a116-4b99-90c0-913d2f7e86be  ...         0.0\n",
       "689  2f3a94a3-0b3a-4a3f-a1bd-a061a57cc1ae  ...         0.0\n",
       "..                                    ...  ...         ...\n",
       "52   c962f3f0-1cec-4bf0-81a6-f23db872347e  ...         0.0\n",
       "450  fd42f71a-a133-480f-b1f2-9ab60e3cbf10  ...         0.0\n",
       "996  d070b0bb-cf79-4d15-af15-ea62c9fe6812  ...         0.0\n",
       "232  4af89ed2-3a8e-4c71-8f04-eb7aa0db7ba1  ...         0.0\n",
       "826  9ddd9bd3-d0cc-4bc8-8caa-6ff5c30e4f98  ...         0.0\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question Type: Spans\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>answer</th>\n",
       "      <th>validated_answers</th>\n",
       "      <th>f1</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6fc56ebd-64f9-48ef-a336-174e465fe48e</td>\n",
       "      <td>murder rate &lt;ss&gt; burglary rate</td>\n",
       "      <td>murder&lt;ss&gt;burglary</td>\n",
       "      <td>murder rate&lt;ss&gt;burglary&lt;sv&gt;murder&lt;ss&gt;burglary</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>bdd34ef1-19e4-4baf-b763-0a346e870471</td>\n",
       "      <td>under age 18 &lt;ss&gt; 25 to 44</td>\n",
       "      <td>under age 18&lt;ss&gt;45 to 64</td>\n",
       "      <td>under age 18&lt;ss&gt;age 45 to 64&lt;sv&gt;under age 18&lt;s...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a0d6b6e2-edbe-47e9-a8bf-41d434edd643</td>\n",
       "      <td>Italian &lt;ss&gt; Guyanese</td>\n",
       "      <td>Guyanese&lt;ss&gt;Irish</td>\n",
       "      <td>Guyanese&lt;ss&gt;Irish&lt;sv&gt;Guyanese, Irish</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2c4c5003-e35a-45fa-bd57-2bec7a019c6c</td>\n",
       "      <td>4-yard &lt;ss&gt; 14-yard &lt;ss&gt; Derek Anderson &lt;ss&gt; C...</td>\n",
       "      <td>4-yard&lt;ss&gt;5-yard</td>\n",
       "      <td>5-yard&lt;sv&gt;4-yard&lt;ss&gt;5-yard</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3b1d2481-ea0f-410d-a818-8c234af697b2</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>Cantonese&lt;ss&gt;French&lt;ss&gt;Vietnamese</td>\n",
       "      <td>Cantonese&lt;ss&gt;French&lt;ss&gt;Vietnamese&lt;sv&gt;Cantonese...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>ae089c94-9de5-4ae0-9b2e-c71c9c6d597c</td>\n",
       "      <td>Lichfield &lt;ss&gt; Lichfield</td>\n",
       "      <td>London&lt;ss&gt;Lichfield</td>\n",
       "      <td>St Georges&lt;ss&gt;Hanover Square&lt;ss&gt;London&lt;ss&gt;Staf...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>07f25a86-71a4-4cf7-89f2-f3efc200e3c2</td>\n",
       "      <td>two incorporated municipalities &lt;ss&gt; Green Party</td>\n",
       "      <td>Democratic&lt;ss&gt;Republican&lt;ss&gt;American Independe...</td>\n",
       "      <td>Democratic&lt;ss&gt;Republican&lt;ss&gt;American Independe...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44a430a0-b710-4e85-a3c2-6ea8e6f03a4d</td>\n",
       "      <td>under the age of 18 &lt;ss&gt; 18 to 24 &lt;ss&gt; 25 to 4...</td>\n",
       "      <td>under the age of 18&lt;ss&gt;25 to 44&lt;ss&gt;45 to 64</td>\n",
       "      <td>under the age of 18&lt;ss&gt;25 to 44&lt;ss&gt;45 to 64&lt;sv...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>b0c8be4d-b753-48cc-9bd2-8e6b32dad7d6</td>\n",
       "      <td>Brian Westbrook &lt;ss&gt; L.J. Smith</td>\n",
       "      <td>4-yard touchdown pass to running&lt;ss&gt;back Brian...</td>\n",
       "      <td>Brian Westbrook&lt;ss&gt;James Thrash&lt;sv&gt;James Thras...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>9f89bba6-c095-4c2b-bcf7-9e6f05c38158</td>\n",
       "      <td>2010 &lt;ss&gt; 4 th quarter</td>\n",
       "      <td>2008&lt;ss&gt;1998</td>\n",
       "      <td>2010&lt;ss&gt;1998&lt;sv&gt;2008&lt;ss&gt;1998</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 query_id  ... exact_match\n",
       "65   6fc56ebd-64f9-48ef-a336-174e465fe48e  ...         0.0\n",
       "247  bdd34ef1-19e4-4baf-b763-0a346e870471  ...         0.0\n",
       "19   a0d6b6e2-edbe-47e9-a8bf-41d434edd643  ...         0.0\n",
       "174  2c4c5003-e35a-45fa-bd57-2bec7a019c6c  ...         0.0\n",
       "233  3b1d2481-ea0f-410d-a818-8c234af697b2  ...         0.0\n",
       "..                                    ...  ...         ...\n",
       "364  ae089c94-9de5-4ae0-9b2e-c71c9c6d597c  ...         0.0\n",
       "256  07f25a86-71a4-4cf7-89f2-f3efc200e3c2  ...         0.0\n",
       "14   44a430a0-b710-4e85-a3c2-6ea8e6f03a4d  ...         0.0\n",
       "122  b0c8be4d-b753-48cc-9bd2-8e6b32dad7d6  ...         0.0\n",
       "373  9f89bba6-c095-4c2b-bcf7-9e6f05c38158  ...         0.0\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Error reports have been saved to /error_reports/performace_reports_1120010759!\n"
     ]
    }
   ],
   "source": [
    "if EVALUATE_DEV:\n",
    "    PRED_LOAD_MODEL = True\n",
    "    PRED_MODEL = 't5-small'\n",
    "    PRED_MODEL_PATH = f'{GDRIVE_REPO_PATH}/models/model_chkpt_e_42_vl_0.04173539.h5'\n",
    "    EVAL_REPORT_SAVE_PATH = f'{GDRIVE_REPO_PATH}/error_reports/'\n",
    "\n",
    "    if PRED_LOAD_MODEL:\n",
    "        keras.backend.clear_session() \n",
    "        with strategy.scope(): \n",
    "            model_pred = DROPBaseT5.from_pretrained(PRED_MODEL)\n",
    "            model_pred.compile()\n",
    "            model_pred.load_weights(PRED_MODEL_PATH)\n",
    "    else:\n",
    "        model_pred = model\n",
    "\n",
    "    dev_eval = DropPerformanceMeasure(TOKENIZER)\n",
    "    count = dev_eval.load_parse_drop_json_for_eval(test_mode=False)\n",
    "    print(f'Total number of examples to be evaluated: {count}')\n",
    "    dev_eval.predict(\n",
    "                model=model_pred, \n",
    "                # batch_size=10, # only required if dry_run = True\n",
    "                ds_example_count=DROP_DEV_EXAMPLE_COUNT,\n",
    "                dry_run=False)\n",
    "    dev_eval.get_eval_reports(\n",
    "                csv_save_path=EVAL_REPORT_SAVE_PATH,\n",
    "                prediction_spans_sep_token='<ss>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epXeQUZxHi_X"
   },
   "source": [
    "## DROP Leaderboard Submission\n",
    "In this section, we build the csv submission for [DROP's leaderboard](https://leaderboard.allenai.org/drop/submissions/public) using the trained model and DROP's test set json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnATAwcsA2w9"
   },
   "outputs": [],
   "source": [
    "def parse_drop_json(json_url: str, mode: str = 'test') -> dict:\n",
    "    \"\"\" Parse Drop json and reconstruct the DROP dataset.\n",
    "     Args:\n",
    "        mode (str): 'train', 'dev', or 'test'\n",
    "    \"\"\"\n",
    "    with open(json_url) as f:\n",
    "        drop_dict = json.load(f)\n",
    "\n",
    "    contexts, questions, question_ids, answers = [], [], [], []\n",
    "    # iterate over passages\n",
    "    for passage_name in drop_dict.keys():\n",
    "        passage = drop_dict[passage_name]['passage']\n",
    "        \n",
    "        # iterate over question/answer sets\n",
    "        for qas in drop_dict[passage_name]['qa_pairs']:\n",
    "            question, question_id = qas['question'], qas['query_id']\n",
    "            \n",
    "            if mode != 'test':\n",
    "                answer_list = []\n",
    "                answer_list.append(\" \".join([date for date in qas['answer']['date'].values() if date.strip()]))\n",
    "                answer_list.append(qas['answer']['number'])\n",
    "                answer_list.append(\", \".join(qas['answer']['spans']))\n",
    "                answer = [ans for ans in answer_list if ans.strip()]\n",
    "                \n",
    "                # skip record if no answer in dataset\n",
    "                if len(answer) != 0:\n",
    "                    answer = answer[0]\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                answers.append(answer)\n",
    "            \n",
    "            contexts.append(passage)\n",
    "            questions.append(question)\n",
    "            question_ids.append(question_id)\n",
    "    \n",
    "    if mode == 'test':\n",
    "        return {\n",
    "            'context': contexts,\n",
    "            'question': questions,\n",
    "            'query_id': question_ids}\n",
    "    else:\n",
    "        return {\n",
    "            'context': contexts,\n",
    "            'question': questions,\n",
    "            'answers': answers,\n",
    "            'query_id': question_ids}\n",
    "\n",
    "def encode_drop_example(\n",
    "                example: dict,\n",
    "                is_test_set: bool = True,\n",
    "                encoder_max_len: int = ENCODER_MAX_LEN, \n",
    "                decoder_max_len: int = DECODER_MAX_LEN,\n",
    "                tokenizer: transformers.PreTrainedTokenizer = TOKENIZER) -> dict:\n",
    "    \"\"\"Tokenize data.\n",
    "    Args:\n",
    "        example (dict): Raw dict parsed from DROP json:\n",
    "                            example['context']\n",
    "                            example['question']\n",
    "                            example['answers']\n",
    "    Returns: \n",
    "        (dict) Dictionary with values tokenized:\n",
    "                            return['input_ids']\n",
    "                            return['attention_mask']\n",
    "                            return['labels']\n",
    "                            return['decoder_attention_mask']\n",
    "    \"\"\"            \n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "\n",
    "    question_plus = f\"answer_me: {str(question)}\"\n",
    "    question_plus += f\"context: {str(context)}\"\n",
    "    \n",
    "    encoder_inputs = tokenizer(\n",
    "                            question_plus, \n",
    "                            truncation=True, \n",
    "                            return_tensors='tf', \n",
    "                            max_length=encoder_max_len,\n",
    "                            padding='max_length')\n",
    "    \n",
    "    if not is_test_set:\n",
    "        answer = example['answers']\n",
    "        answer_plus = f\"{str(answer)}\"\n",
    "        decoder_inputs = tokenizer(\n",
    "                                answer_plus, \n",
    "                                truncation=True, \n",
    "                                return_tensors='tf', \n",
    "                                max_length=decoder_max_len,\n",
    "                                padding='max_length')\n",
    "    \n",
    "    # [0] to convert to rank 1 array (e.g. (1, 250) => (250,))\n",
    "    return {\n",
    "        'input_ids': encoder_inputs['input_ids'][0], \n",
    "        'attention_mask': encoder_inputs['attention_mask'][0]}\n",
    "\n",
    "def to_tf_dataset(\n",
    "        dataset: datasets.Dataset, \n",
    "        is_test_set: bool = True,\n",
    "        tf_generator: bool = False,\n",
    "        max_len: int = 512) -> tf.data.Dataset:\n",
    "    \"\"\" Keep only requried columns from raw data. Covert everything to \n",
    "    tf.data.Dataset for tf modeling.\n",
    "    \"\"\"\n",
    "    if not is_test_set:\n",
    "        columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']\n",
    "        dataset.set_format(type='tensorflow', columns=columns)\n",
    "\n",
    "        if tf_generator:\n",
    "            return_types = {\n",
    "                    'input_ids': tf.int32, \n",
    "                    'attention_mask': tf.int32, \n",
    "                    'labels': tf.int32, \n",
    "                    'decoder_attention_mask': tf.int32}\n",
    "            return_shapes = {\n",
    "                    'input_ids': tf.TensorShape([None]), \n",
    "                    'attention_mask': tf.TensorShape([None]), \n",
    "                    'labels': tf.TensorShape([None]), \n",
    "                    'decoder_attention_mask': tf.TensorShape([None])}\n",
    "\n",
    "            # HuggingFace's bug on CoLab's TPU with from_generator...\n",
    "            # https://tinyurl.com/y6gu7ezd\n",
    "            # https://tinyurl.com/yywrk2sq\n",
    "            return tf.data.Dataset.from_generator(\n",
    "                                        generator=lambda: dataset, \n",
    "                                        output_types=return_types, \n",
    "                                        output_shapes=return_shapes)\n",
    "        else:\n",
    "            ds_tf = {x: dataset[x].to_tensor(default_value=0, shape=[None, max_len]) \n",
    "                    for x in [\n",
    "                        'input_ids', \n",
    "                        'attention_mask', \n",
    "                        'labels',\n",
    "                        'decoder_attention_mask']}\n",
    "            return tf.data.Dataset.from_tensor_slices(ds_tf)\n",
    "    else:\n",
    "        columns = ['input_ids', 'attention_mask']\n",
    "        dataset.set_format(type='tensorflow', columns=columns)\n",
    "\n",
    "        if tf_generator:\n",
    "            return_types = {\n",
    "                    'input_ids': tf.int32, \n",
    "                    'attention_mask': tf.int32}\n",
    "            return_shapes = {\n",
    "                    'input_ids': tf.TensorShape([None]), \n",
    "                    'attention_mask': tf.TensorShape([None])}\n",
    "\n",
    "            return tf.data.Dataset.from_generator(\n",
    "                                        generator=lambda: dataset, \n",
    "                                        output_types=return_types, \n",
    "                                        output_shapes=return_shapes)\n",
    "        else:\n",
    "            ds_tf = {x: dataset[x].to_tensor(default_value=0, shape=[None, max_len]) \n",
    "                    for x in [\n",
    "                        'input_ids', \n",
    "                        'attention_mask']}\n",
    "            return tf.data.Dataset.from_tensor_slices(ds_tf)\n",
    "\n",
    "def prep_tf_dataset(\n",
    "            ds: tf.data.Dataset,\n",
    "            batch_size: int = BATCH_SIZE, \n",
    "            buffer_size: int = 1024, \n",
    "            shuffle: bool = False,\n",
    "            repeat: bool = False,\n",
    "            tf_generator: bool = False,\n",
    "            cache_path: str = None) -> tf.data.Dataset:\n",
    "    \"\"\"Prep tf dataset for feeding to the tf model.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        ds = dataset.shuffle(buffer_size)\n",
    "    ds = ds.repeat(-1) if repeat else ds.repeat(1)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    if cache_path is not None:\n",
    "        ds = dataset.cache(cache_path)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def predict_test(\n",
    "        model: transformers.TFT5ForConditionalGeneration, \n",
    "        ds: datasets.Dataset, \n",
    "        ds_example_count: int = 0,\n",
    "        decode: bool = True,\n",
    "        tokenizer: transformers.T5Tokenizer = TOKENIZER,\n",
    "        decoder_max_len: int = DECODER_MAX_LEN,\n",
    "        batch_size: int = BATCH_SIZE) -> tuple:\n",
    "    \"\"\"Predict and decode using the trained T5 model.\n",
    "    Refs:\n",
    "        https://tinyurl.com/yxlr4jmb\n",
    "        https://tinyurl.com/y53l7vfm\n",
    "    Returns:\n",
    "        (tuple) preds\n",
    "    \"\"\"\n",
    "    ds = ds.map(encode_drop_example)\n",
    "    ds = to_tf_dataset(ds)\n",
    "    ds = prep_tf_dataset(\n",
    "                    ds=ds,\n",
    "                    repeat=False, \n",
    "                    shuffle=False, \n",
    "                    batch_size=batch_size)\n",
    "    with tqdm(total=ds_example_count) as pbar:\n",
    "        preds, qids = [], []\n",
    "        for batch in ds:\n",
    "            # predictions\n",
    "            outs_pred = model.generate(\n",
    "                            input_ids=batch['input_ids'], \n",
    "                            attention_mask=batch['attention_mask'],\n",
    "                            max_length=decoder_max_len,\n",
    "                            early_stopping=True)\n",
    "            outs_pred = [TOKENIZER.decode(ids) for ids in outs_pred]\n",
    "            preds.extend(outs_pred)\n",
    "\n",
    "            # update progress bar\n",
    "            pbar.update(len(batch['input_ids']))\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpEg6iw4lIDl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Submission format required:\n",
    "https://tinyurl.com/yy9xjrjt\n",
    "{\n",
    "    \"2319bcf1-9f2d-4f12-ab91-433e084dbad6\": [\n",
    "        \"Ayutthaya\"\n",
    "    ],\n",
    "    \"0f1e2d6a-b20a-436b-bec9-428b43b110c8\": \"15\",\n",
    "    \"892bf21d-6579-45f3-b4c7-28ac6e4d9d72\": [\n",
    "        \"Maha Sithu\",\n",
    "        \"Maha Thiha Thura\",\n",
    "        \"Ne Myo Sithu\",\n",
    "        \"Balamindin\"\n",
    "    ],\n",
    "    \"c917db96-7062-4644-bd5a-658a8482945a\": \"1\",\n",
    "    \"f6fa8277-5d5b-4cbd-99f9-044d54c86a69\": \"5\",\n",
    "    \"7c2b7bbd-0316-485c-8e50-c4edbe611e72\": [\n",
    "        \"Ryan Fitzpatrick\"\n",
    "    ],\n",
    "    \"bea06a8a-7d2c-4620-b3c2-eb358569192d\": \"1\",\n",
    "    \"127d29b6-32cc-42e0-8e55-8d39e970080f\": \"2\",\n",
    "    \"c41fc1f1-3c1f-4dc8-9730-c1a8751e22e8\": \"5\",\n",
    "    \"6b6faba7-1806-4635-820f-18d58a0cc014\": \"20\",\n",
    "    \"11b8fbc2-8d9c-473b-a54e-e56c6b78d748\": \"4\",\n",
    "    \"6b22f49b-270c-44c2-a638-0e7ddc8d5a05\": \"22\",\n",
    "    \"6cff732c-8fba-4aaa-82bc-ea06018a4d0a\": \"1\",\n",
    "    \"6791e6c2-6f53-42b9-9b20-0cf35c11847e\": \"3\",\n",
    "    \"eae7c735-c9bd-4317-853f-a01dd65e079e\": \"51\",\n",
    "    \"7d213e14-3975-4883-9c4f-68d8d448e77f\": \"40\",\n",
    "    \"06a58c6e-f14a-40ca-b6e4-79a9dfd6d702\": \"23\",\n",
    "    \"bcc4f2dc-9485-465c-9a5d-0b3837694230\": \"39\",\n",
    "    \"ab236b01-a8d0-4e77-a88c-3c088771f93f\": [\n",
    "        \"Terrell Suggs\"\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "if EVALUATE_TEST:\n",
    "    TEST_LOAD_MODEL = True\n",
    "    TEST_REDUCE_DATA = True\n",
    "\n",
    "    TEST_MODEL_PATH = f'{GDRIVE_REPO_PATH}/models/model_chkpt_e_42_vl_0.04173539.h5'\n",
    "    TEST_DATA_PATH = DROP_TEST_JSON\n",
    "\n",
    "    TEST_MODEL = 't5-small'\n",
    "    time = datetime.utcnow().strftime(f'%m%d%H%M%S')\n",
    "    os.makedirs(f'{GDRIVE_REPO_PATH}/submission_drop_{time}', exist_ok=True)\n",
    "    DROP_SUBMISSION_EXPORT_JSON_PATH = f'{GDRIVE_REPO_PATH}/submission_drop_{time}/drop_submission_{time}.json'\n",
    "    TEST_PRED_PRINT_COUNT = 10\n",
    "\n",
    "    # build model\n",
    "    if TEST_LOAD_MODEL:\n",
    "        keras.backend.clear_session() \n",
    "        with strategy.scope(): \n",
    "            model_test = DROPBaseT5.from_pretrained(TEST_MODEL)\n",
    "            model_test.compile()  \n",
    "            model_test.load_weights(TEST_MODEL_PATH)\n",
    "    else:\n",
    "        model_test = model\n",
    "\n",
    "    # load data\n",
    "    test_dict = parse_drop_json(DROP_TEST_JSON)\n",
    "    test_quids = test_dict['query_id']\n",
    "    test_count = len(test_quids)\n",
    "\n",
    "    test_ds = datasets.Dataset.from_dict(test_dict)\n",
    "    if TEST_REDUCE_DATA:\n",
    "        test_ds = datasets.Dataset.from_dict(test_ds[:10])\n",
    "        test_quids = test_quids[:10]\n",
    "        test_count = 10\n",
    "\n",
    "    # predict\n",
    "    test_preds = predict_test(\n",
    "                    model=model_test, \n",
    "                    ds=test_ds, \n",
    "                    ds_example_count=test_count)\n",
    "\n",
    "    # export predictions to json\n",
    "    export = {}\n",
    "    for qid, pred in zip(test_quids, test_preds):\n",
    "        if pred.isnumeric():\n",
    "            export[qid] = pred    \n",
    "        elif ',' in pred:\n",
    "            export[qid] = [w.strip() for w in pred.split(',')]\n",
    "        else:\n",
    "            export[qid] = [pred]\n",
    "\n",
    "    # save json\n",
    "    with open(DROP_SUBMISSION_EXPORT_JSON_PATH, 'w') as fp:\n",
    "        json.dump(export, fp)\n",
    "\n",
    "    # print predictions\n",
    "    print_df = pd.DataFrame({'query_id': test_quids, 'prediction': test_preds})\n",
    "    display(print_df.iloc[:TEST_PRED_PRINT_COUNT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaIW1Y25zEUb"
   },
   "source": [
    "## Prediction Playground\n",
    "This section includes a simple script to make quick predictions using trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10413,
     "status": "ok",
     "timestamp": 1605422839246,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifbYqdqoVX3NoViBOtgGwuQf_zCM-cnWaavU1TSA=s64",
      "userId": "08804338425967190040"
     },
     "user_tz": 480
    },
    "id": "HOyURNacxT7G",
    "outputId": "cc0270c3-745d-476f-ab08-ff5a35ecc2a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing DROPBaseT5.\n",
      "\n",
      "Some weights of DROPBaseT5 were not initialized from the model checkpoint at t5-small and are newly initialized: ['loss']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Answer:  Johnny Knox<extra_id_-1> Greg Olsen\n"
     ]
    }
   ],
   "source": [
    "PLAY_LOAD_MODEL = True\n",
    "PLAY_MODEL_PATH = f'{GDRIVE_REPO_PATH}/models/model_chkpt_e_42_vl_0.04173539.h5'\n",
    "PLAY_MODEL = 't5-small'\n",
    "\n",
    "# build model\n",
    "if PLAY_LOAD_MODEL:\n",
    "    model_play = DROPBaseT5.from_pretrained(PLAY_MODEL)\n",
    "    model_play.compile()  \n",
    "    model_play.load_weights(PLAY_MODEL_PATH)\n",
    "else:\n",
    "    model_play = model\n",
    "\n",
    "# # sample data 1\n",
    "# # answer: 3\n",
    "# context = \"To start the season, the Lions traveled south to Tampa, Florida to take on the Tampa Bay Buccaneers. The Lions scored first in the first quarter with a 23-yard field goal by Jason Hanson. The Buccaneers tied it up with a 38-yard field goal by Connor Barth, then took the lead when Aqib Talib intercepted a pass from Matthew Stafford and ran it in 28 yards. The Lions responded with a 28-yard field goal. In the second quarter, Detroit took the lead with a 36-yard touchdown catch by Calvin Johnson, and later added more points when Tony Scheffler caught an 11-yard TD pass. Tampa Bay responded with a 31-yard field goal just before halftime. The second half was relatively quiet, with each team only scoring one touchdown. First, Detroit's Calvin Johnson caught a 1-yard pass in the third quarter. The game's final points came when Mike Williams of Tampa Bay caught a 5-yard pass. The Lions won their regular season opener for the first time since 2007\"\n",
    "# question = \"How many points did the buccaneers need to tie in the first?\" \n",
    "\n",
    "# # sample data 2\n",
    "# # answer: mobile games, retail games\n",
    "# context = \"According to the market research firm SuperData, as of May 2015, the global games market was worth USD 74.2 billion. By region, North America accounted for $23.6 billion, Asia for $23.1 billion, Europe for $22.1 billion and South America for $4.5 billion. By market segment, mobile games were worth $22.3 billion, retail games 19.7 billion, free-to-play Massively multiplayer online game 8.7 billion, social games $7.9 billion, PC Downloadable content 7.5 billion, and other categories $3 billion or less each.\"\n",
    "# question = \"Which market segments were worth more than $15 billion each?\"\n",
    "\n",
    "# sample data 3\n",
    "# answer: Johnny Knox, Roddy White\n",
    "context = \"Coming off their impressive road win over the 49ers, the Falcons went home for a Week 6 Sunday night duel with the Chicago Bears.  After a scoreless first quarter, Atlanta would trail early in the second quarter as Bears quarterback Jay Cutler found wide receiver Johnny Knox on a 23-yard touchdown pass.  Afterwards, the Falcons took the lead as quarterback Matt Ryan completed a 40-yard touchdown pass to wide receiver Roddy White and a 10-yard touchdown pass to tight end Tony Gonzalez. After a scoreless third quarter, Chicago would tie the game in the fourth quarter with Cutler hooking up with tight end Greg Olsen on a 2-yard touchdown.  Atlanta would regain the lead as running back Michael Turner got a 5-yard touchdown run.  Afterwards, the defense would fend off a last-second Bears drive to lock up the victory.\"\n",
    "question = \"Which players scored touchdowns longer than 20 yards?\" \n",
    "\n",
    "# # sample data 4\n",
    "# # answer: \"day\": \"12\", \"month\": \"January\", \"year\": \"1905\"\n",
    "# context = \"On 12 January, the Tsar appointed Dmitri Feodorovich Trepov as governor in St Petersburg and dismissed the Minister of the Interior, Pyotr Sviatopolk-Mirskii, on 18 February\\u00a0\\u00a01905. He appointed a government commission \\\"to enquire without delay into the causes of discontent among the workers in the city of St Petersburg and its suburbs\\\" in view of the strike movement. The commission was headed by Senator NV\\u00a0Shidlovsky, a member of the State Council, and included officials, chiefs of government factories, and private factory owners. It was also meant to have included workers' delegates elected according to a two-stage system. Elections of the workers delegates were, however, blocked by the socialists who wanted to divert the workers from the elections to the armed struggle. On 5 March\\u00a0\\u00a01905, the Commission was dissolved without having started work. Following the assassination of his uncle, the Grand Duke Sergei Aleksandrovich, on 17 February\\u00a0\\u00a01905, the Tsar made new concessions. On 18 February\\u00a0\\u00a01905 he published the Bulygin Rescript, which promised the formation of a consultative assembly, religious tolerance, freedom of speech  and a reduction in the peasants' redemption payments. On 24 and 25 May\\u00a0\\u00a01905, about 300 Zemstvo and municipal representatives held three meetings in Moscow, which passed a resolution, asking for popular representation at the national level. On 6 June\\u00a0\\u00a01905, Nicholas II had received a Zemstvo deputation. Responding to speeches by Prince Sergei Trubetskoi and Mr Fyodrov, the Tsar confirmed his promise to convene an assembly of people's representatives.\"\n",
    "# question = \"The Tsar appointed Dmitri Feodorovich Trepov as governor in St. Petersburg on what date?\" \n",
    "\n",
    "# encode context & question\n",
    "input_text = f\"answer_me: {question} context: {context}\"\n",
    "encoded_query = TOKENIZER(\n",
    "                    input_text, \n",
    "                    return_tensors='tf', \n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    max_length=ENCODER_MAX_LEN)\n",
    "\n",
    "# generate answer\n",
    "# doc: https://tinyurl.com/yy24a3yj\n",
    "generated_answer = model_play.generate(\n",
    "                                input_ids=encoded_query[\"input_ids\"], \n",
    "                                attention_mask=encoded_query[\"attention_mask\"], \n",
    "                                max_length=DECODER_MAX_LEN, \n",
    "                                top_p=0.95, \n",
    "                                top_k=50, \n",
    "                                repetition_penalty=2)\n",
    "decoded_answer = TOKENIZER.decode(generated_answer.numpy()[0])\n",
    "print(\"T5 Answer: \", decoded_answer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nt5_multitask_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3f13a79969344276a5d410798953bbfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91a7c7223ace4886949e14703f241c61",
      "placeholder": "​",
      "style": "IPY_MODEL_b2dd9e4677214910880dc880cc6d531d",
      "value": " 242M/242M [00:06&lt;00:00, 36.8MB/s]"
     }
    },
    "91a7c7223ace4886949e14703f241c61": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0a49d26c2f74878ac28af98755855f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b2dd9e4677214910880dc880cc6d531d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9ad7efc007e4c37ab165edf4aab02b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e853c26adccd44fa8d4e3b653ed7c3f3",
      "max": 242303832,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0a49d26c2f74878ac28af98755855f2",
      "value": 242303832
     }
    },
    "d535a1b227994531a08d2d9a02baaadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b9ad7efc007e4c37ab165edf4aab02b3",
       "IPY_MODEL_3f13a79969344276a5d410798953bbfc"
      ],
      "layout": "IPY_MODEL_f235368a8b5e4be18e2e3faae0d067b9"
     }
    },
    "e853c26adccd44fa8d4e3b653ed7c3f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f235368a8b5e4be18e2e3faae0d067b9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

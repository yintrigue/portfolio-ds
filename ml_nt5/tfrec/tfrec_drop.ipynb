{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp9-AReX2bt_"
   },
   "source": [
    "# T5 TFRecord Preprocessing\n",
    "\n",
    "This notebook preprocesses the DROP dataset in JSON format to a TFRecords dataset that can be fed into the model after batching for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nfp8x5hOmrPI"
   },
   "outputs": [],
   "source": [
    "if 'colab' in str(get_ipython()):\n",
    "    import google.colab as colab\n",
    "    colab.auth.authenticate_user()\n",
    "    colab.drive.mount('/content/gdrive') # mount google drive\n",
    "\n",
    "    # install libraries not native to colab\n",
    "    !pip install tensorflow-text\n",
    "    !pip install transformers==3.3.1\n",
    "    !pip install datasets==1.1.2\n",
    "    !pip install tqdm\n",
    "\n",
    "# remove pip install outputs\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7649,
     "status": "ok",
     "timestamp": 1603953774465,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCHzDArJyPPAHsYKioqVZeP1reZIsBL9RZDKwCOy8=s64",
      "userId": "17136390159455304905"
     },
     "user_tz": 420
    },
    "id": "Ls1fXWF3sLs1",
    "outputId": "6da152f6-0a57-4abf-fefe-7b06ea7c11e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.2.0\n"
     ]
    }
   ],
   "source": [
    "# ml libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import transformers\n",
    "import datasets # https://huggingface.co/docs/datasets/\n",
    "\n",
    "# data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# other libraries\n",
    "import os\n",
    "import json\n",
    "import functools\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Generator\n",
    "\n",
    "print(f'TensorFlow {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY88Y3FzXUWN"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 563,
     "status": "ok",
     "timestamp": 1603953774466,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCHzDArJyPPAHsYKioqVZeP1reZIsBL9RZDKwCOy8=s64",
      "userId": "17136390159455304905"
     },
     "user_tz": 420
    },
    "id": "l1NeuclbwOB0",
    "outputId": "603c57ff-7a3f-427c-9918-cbba06d7cd45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tokens added: 2\n"
     ]
    }
   ],
   "source": [
    "#@title Constants\n",
    "T5_MODEL                = 't5-small'  #@param { type: \"string\" }\n",
    "ENCODER_MAX_LEN         = 512 #@param { type: \"integer\" }\n",
    "DECODER_MAX_LEN         = 54 #@param { type: \"integer\" } \n",
    "TOKENIZER               = transformers.AutoTokenizer.from_pretrained(T5_MODEL)\n",
    "special_tokens_dict     = {'additional_special_tokens': ['0','1','2','3', '4', '5', '6', '7', '8', '9', '<ss>', '<sv>']}\n",
    "num_added_toks          = TOKENIZER.add_special_tokens(special_tokens_dict)\n",
    "print(f'New tokens added: {num_added_toks}')\n",
    "\n",
    "# tim's default paths\n",
    "LOCAL_PATH              = '' # local folder containing DROP json\n",
    "TRAIN_JSON              = 'drop_dataset_train.json' \n",
    "DEV_JSON                = 'drop_dataset_dev.json' \n",
    "DROP_JSON_DIR_LOCAL     = LOCAL_PATH + '/data/drop_dataset' #@param { type: \"string\" }\n",
    "DROP_JSON_DIR_GCP       = '' # gcp storage path for saving tfrecords #@param { type: \"string\" }\n",
    "EXPORT_DIR              = LOCAL_PATH + '/temp' #@param { type: \"string\" }\n",
    "\n",
    "TRAIN_DROP_JSON_GCP     = os.path.join(DROP_JSON_DIR_GCP, TRAIN_JSON)\n",
    "DEV_DROP_JSON_GCP       = os.path.join(DROP_JSON_DIR_GCP, DEV_JSON) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Mode Setup\n",
    "def sample_data_to_file(\n",
    "                json_file: str,\n",
    "                sample_count: int,\n",
    "                source_dir: str, \n",
    "                export_dir: str = EXPORT_DIR) -> str:\n",
    "    \n",
    "    # load source json\n",
    "    source_file_path = os.path.join(source_dir, json_file)\n",
    "    with tf.io.gfile.GFile(source_file_path) as f:\n",
    "        data_json_dict = json.load(f)  # dict of dict\n",
    "        sampled_data_json_dict = {\n",
    "            key: value\n",
    "            for i, (key, value) in enumerate(data_json_dict.items())\n",
    "            if i < sample_count\n",
    "        }\n",
    "\n",
    "    # prep export json file path\n",
    "    sample_count = len(sampled_data_json_dict)  # get the real sample count\n",
    "    sampled_output_path = os.path.join(export_dir, json_file)\n",
    "    sampled_output_path = sampled_output_path.replace('.json', f'{sample_count}.json')\n",
    "\n",
    "    # export sample json\n",
    "    with tf.io.gfile.GFile(sampled_output_path, 'w') as fout:\n",
    "        json.dump(sampled_data_json_dict, fout)\n",
    "\n",
    "    print(f'Dumped {sample_count} samples to file: {sampled_output_path}')\n",
    "    return sampled_output_path\n",
    "\n",
    "SAMPLE_DATA = True  #@param { type: \"boolean\" }\n",
    "SAMPLE_COUNT = 3 #@param { type: \"integer\" }\n",
    "if SAMPLE_DATA:\n",
    "    train_json = sample_data_to_file(\n",
    "                    json_file=TRAIN_JSON, \n",
    "                    sample_count=SAMPLE_COUNT,\n",
    "                    source_dir=DROP_JSON_DIR_LOCAL,\n",
    "                    export_dir=EXPORT_DIR)\n",
    "    dev_json = sample_data_to_file(\n",
    "                    json_file=DEV_JSON, \n",
    "                    sample_count=SAMPLE_COUNT,\n",
    "                    source_dir=DROP_JSON_DIR_LOCAL,\n",
    "                    export_dir=EXPORT_DIR)\n",
    "else:\n",
    "    train_json = os.path.join(DROP_JSON_DIR_LOCAL, TRAIN_JSON)\n",
    "    dev_json = os.path.join(DROP_JSON_DIR_LOCAL, DEV_JSON)\n",
    "    print(train_json)\n",
    "    print(dev_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOBK366_aq6P"
   },
   "source": [
    "## Encoding TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "x9rc72mrwSVd"
   },
   "outputs": [],
   "source": [
    "def encode(\n",
    "        example: dict,\n",
    "        encoder_max_len: int = ENCODER_MAX_LEN, \n",
    "        decoder_max_len: int = DECODER_MAX_LEN,\n",
    "        tokenizer: transformers.PreTrainedTokenizer = TOKENIZER) -> dict:\n",
    "    \"\"\"Tokenize data.\n",
    "    Args:\n",
    "        example (dict): Raw dict parsed from DROP json:\n",
    "                            example['context']\n",
    "                            example['question']\n",
    "                            example['answer']\n",
    "    Returns: \n",
    "        (dict) Dictionary with values tokenized:\n",
    "                            return['input_ids']\n",
    "                            return['attention_mask']\n",
    "                            return['labels']\n",
    "                            return['decoder_attention_mask']\n",
    "    \"\"\"            \n",
    "    context = example['context']\n",
    "    question = example['question'] # '1+1+(1+(1+1))=10'\n",
    "    answer = example['answer']\n",
    "  \n",
    "    question_plus = f\"answer_me: {str(question)} context: {str(context)}\"\n",
    "    answer_plus = str(answer)\n",
    "\n",
    "    encoder_inputs = tokenizer(\n",
    "                            question_plus, \n",
    "                            truncation=True, \n",
    "                            return_tensors='tf', \n",
    "                            max_length=encoder_max_len,\n",
    "                            padding='max_length')\n",
    "    decoder_inputs = tokenizer(\n",
    "                            answer_plus, \n",
    "                            truncation=True, \n",
    "                            return_tensors='tf', \n",
    "                            max_length=decoder_max_len,\n",
    "                            padding='max_length')\n",
    "    \n",
    "    return {\n",
    "        'query_id'                  : [str.encode(example['query_id'])], \n",
    "        'answer_type'               : [str.encode(example['answer_type'])], \n",
    "        'validated_answers'         : [str.encode(example['validated_answers'])],\n",
    "        'input_ids'                 : encoder_inputs['input_ids'][0], \n",
    "        'attention_mask'            : encoder_inputs['attention_mask'][0], \n",
    "        'labels'                    : decoder_inputs['input_ids'][0], \n",
    "        'decoder_attention_mask'    : decoder_inputs['attention_mask'][0]}\n",
    "\n",
    "def _bytes_feature(values):\n",
    "  \"\"\"Returns a bytes_list from a list of string / byte.\"\"\"\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=values))\n",
    "\n",
    "def _float_feature(values):\n",
    "  \"\"\"Returns a float_list from a list of float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n",
    "\n",
    "def _int64_feature(values):\n",
    "    \"\"\"Returns an int64_list from a list of bool / enum / int / uint.\"\"\"\n",
    "    try:\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n",
    "    except Exception as e:\n",
    "        print(f'failed at: {values}')\n",
    "        raise e\n",
    "\n",
    "def to_serialized_example(encoded_example):\n",
    "    feature = {\n",
    "        'query_id': _bytes_feature(encoded_example['query_id']),\n",
    "        'answer_type': _bytes_feature(encoded_example['answer_type']),\n",
    "        'validated_answers': _bytes_feature(encoded_example['validated_answers']),\n",
    "        'input_ids': _int64_feature(encoded_example['input_ids']),\n",
    "        'attention_mask': _int64_feature(encoded_example['attention_mask']),\n",
    "        'labels': _int64_feature(encoded_example['labels']),\n",
    "        'decoder_attention_mask': _int64_feature(encoded_example['decoder_attention_mask']),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def parse_validated_answers(validated_answers: list) -> str:\n",
    "    answers = []\n",
    "    for answer_dict in validated_answers:\n",
    "        ans, _ = parse_answer(answer_dict)\n",
    "        answers.append(ans)\n",
    "    return '<sv>'.join([a.strip() for a in answers if a.strip()])\n",
    "        \n",
    "def parse_answer(answer_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Example of answer_dict = { \"number\": \"\", \"date\": { \"day\": \"\", \"month\": \"\", \"year\": \"\" }, \"spans\": [ \"Chaz Schilens\" ] }\n",
    "    Returns a string or None.\n",
    "    \"\"\"\n",
    "    number = answer_dict['number'].strip()\n",
    "    if number:\n",
    "        return number, 'n'\n",
    "    \n",
    "    spans = answer_dict['spans']\n",
    "    spans_str = '<ss>'.join([span.strip() for span in spans if span.strip()])\n",
    "    if spans_str:\n",
    "        if len(spans) > 1:\n",
    "            return spans_str, 's'\n",
    "        else:\n",
    "            return spans_str, 'ss'\n",
    "    \n",
    "    date = answer_dict['date']\n",
    "    if len(date) != 3:\n",
    "        return None, None\n",
    "    date = ' '.join([d.strip() for d in [date['day'], date['month'], date['year']] if d.strip()])\n",
    "    if date:\n",
    "        return date, 'd'\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def make_data_generator(file_path: str) -> Generator:\n",
    "    \"\"\"\n",
    "    json_example: {\n",
    "        \"passage\": \"foo bar\",\n",
    "        \"qa_pairs\": [\n",
    "            {\n",
    "                \"question\": \"foo bar\",\n",
    "                \"answer\": { \"number\": \"\", \"date\": { \"day\": \"\", \"month\": \"\", \"year\": \"\" }, \"spans\": [ \"Chaz Schilens\" ] },\n",
    "                \"query_id\": \"f37e81fa-ef7b-4583-b671-762fc433faa9\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    Returns a generator function.\n",
    "    \"\"\"\n",
    "    with tf.io.gfile.GFile(file_path) as json_file:\n",
    "        data_json_dict = json.load(json_file) \n",
    "\n",
    "    def gen():\n",
    "        for key, json_example in data_json_dict.items():\n",
    "            passage = json_example['passage']\n",
    "            for qa_dict in json_example['qa_pairs']:\n",
    "                \n",
    "                question = qa_dict['question']\n",
    "                answer, answer_type = parse_answer(qa_dict['answer'])\n",
    "                query_id = qa_dict['query_id']\n",
    "                if 'validated_answers' in qa_dict:\n",
    "                    valid_ans = parse_validated_answers(qa_dict['validated_answers'])\n",
    "                else:\n",
    "                    valid_ans = ''                \n",
    "                if answer is None:\n",
    "                    continue\n",
    "\n",
    "                encoded_example = encode({\n",
    "                    'context': passage,\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                    'validated_answers': valid_ans,\n",
    "                    'answer_type': answer_type,\n",
    "                    'query_id': query_id\n",
    "                })\n",
    "                serialized_example = to_serialized_example(encoded_example)\n",
    "                \n",
    "                yield serialized_example\n",
    "    \n",
    "    return gen\n",
    "\n",
    "def get_num_examples(file_path):\n",
    "    with tf.io.gfile.GFile(file_path) as json_file:\n",
    "        data_json_dict = json.load(json_file) \n",
    "        return len(data_json_dict)\n",
    "\n",
    "def load_dataset(file_path: str) -> tf.data.Dataset:\n",
    "    return tf.data.Dataset.from_generator(\n",
    "                        make_data_generator(file_path),\n",
    "                        output_types=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUFFIX = '_test'\n",
    "def write_json_to_tf_record_file(json_path: str) -> str:\n",
    "    dataset = load_dataset(json_path)\n",
    "    tf_record_file_path = os.path.join(\n",
    "        os.path.dirname(json_path),\n",
    "        os.path.basename(json_path).replace('.json', f'{SUFFIX}.tfrecord')\n",
    "    )\n",
    "\n",
    "    print(f'Writing TF Record file to: {tf_record_file_path} ...')\n",
    "    writer = tf.data.experimental.TFRecordWriter(tf_record_file_path)\n",
    "    writer.write(dataset)\n",
    "\n",
    "    return tf_record_file_path\n",
    "\n",
    "train_tfrec_path = write_json_to_tf_record_file(train_json)\n",
    "dev_tfrec_path = write_json_to_tf_record_file(dev_json)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "drop_tfrec_conv_local.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBPhve_C3Hqj"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "This notebook preprocesses the SQuAD v1.1 dataset in JSON format to a TFRecords dataset that can be fed into the model after batching for training. Refer to `squad_parser.ipynb` for how SQuAD datasets are parsed and saved on Google storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Nfp8x5hOmrPI"
   },
   "outputs": [],
   "source": [
    "if 'colab' in str(get_ipython()):\n",
    "    import google.colab as colab\n",
    "    colab.drive.mount('/content/gdrive') # mount google drive\n",
    "\n",
    "# install libraries not native to colab\n",
    "!pip install tensorflow-text\n",
    "!pip install transformers==3.3.1\n",
    "!pip install datasets==1.1.2\n",
    "!pip install tqdm\n",
    "\n",
    "# remove pip install outputs\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9PJz-2sqMIFR"
   },
   "outputs": [],
   "source": [
    "# # For accessing data from google storage (gs://)\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "!pip install tensorflow_datasets\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ls1fXWF3sLs1",
    "outputId": "b8da14d0-7722-4e20-fcd7-309b9d873270",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.3.0\n"
     ]
    }
   ],
   "source": [
    "# ml libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "# import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_datasets as tfds\n",
    "import transformers\n",
    "import datasets # https://huggingface.co/docs/datasets/\n",
    "\n",
    "# data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# other libraries\n",
    "import os\n",
    "import json\n",
    "import functools\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "print(f'TensorFlow {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "l1NeuclbwOB0"
   },
   "outputs": [],
   "source": [
    "#@title Constants\n",
    "\n",
    "#@title define functin: write_to_gs\n",
    "# authenticate\n",
    "# For accessing data from google storage (gs://)\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "direc = '' # directory on gs containing parsed squad json\n",
    "TRAIN_DATA = \"squad1.1_train_data_parsed.json\"\n",
    "DEV_DATA = \"squad1.1_dev_data_parsed.json\"\n",
    "TRAIN_DATA_PATH = os.path.join(direc, TRAIN_DATA)\n",
    "DEV_DATA_PATH = os.path.join(direc, DEV_DATA)\n",
    "\n",
    "\n",
    "T5_MODEL = 't5-small'\n",
    "ENCODER_MAX_LEN = 512\n",
    "DECODER_MAX_LEN = 54\n",
    "TOKENIZER = transformers.AutoTokenizer.from_pretrained(T5_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2i0IQHwZKhjr"
   },
   "outputs": [],
   "source": [
    "#@title Sample data to file: so it's easy to iterate\n",
    "def sample_data_to_file(file_path, sample_count):\n",
    "    with tf.io.gfile.GFile(file_path) as json_file:\n",
    "        data_json_dict = json.load(json_file)  # dict of dict.\n",
    "\n",
    "        context = data_json_dict['context'][:sample_count]\n",
    "        questions = data_json_dict['questions'][:sample_count]\n",
    "        answers = data_json_dict['answers'][:sample_count]\n",
    "        question_ids = data_json_dict[\"question_ids\"][:sample_count]\n",
    "\n",
    "\n",
    "        sampled_data_json_dict =  {\n",
    "            \"context\": context,\n",
    "            \"questions\": questions,\n",
    "            \"answers\": answers,\n",
    "            \"question_ids\": question_ids,\n",
    "        }\n",
    "\n",
    "    # sample_count = len(sampled_data_json_dict)  # Get the real sample count.\n",
    "    sampled_output_path = os.path.join(\n",
    "        os.path.dirname(file_path),\n",
    "        os.path.basename(file_path).replace('.json', f'{sample_count}.json')\n",
    "    )\n",
    "    with tf.io.gfile.GFile(sampled_output_path, 'w') as fout:\n",
    "        json.dump(sampled_data_json_dict, fout)\n",
    "    print(f'Dumped {sample_count} samples to file: {sampled_output_path}')\n",
    "\n",
    "    return sampled_output_path\n",
    "\n",
    "\n",
    "SAMPLE_DATA_TO_FILE = False  #@param { type: \"boolean\" }\n",
    "SAMPLE_COUNT = 1  #@param { type: \"integer\" }\n",
    "if SAMPLE_DATA_TO_FILE:\n",
    "    TRAIN_DATA_PATH = sample_data_to_file(TRAIN_DATA_PATH, sample_count=SAMPLE_COUNT)\n",
    "    DEV_DATA_PATH = sample_data_to_file(DEV_DATA_PATH, sample_count=SAMPLE_COUNT)\n",
    "\n",
    "    # if want to see the saved sampled_dict\n",
    "    with tf.io.gfile.GFile(TRAIN_DATA_PATH) as json_file:\n",
    "        check_sampled_dict = json.load(json_file)\n",
    "\n",
    "    print(check_sampled_dict)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "both",
    "id": "HvtsIwtXnocn"
   },
   "outputs": [],
   "source": [
    "#@title Define functions: encode(), prep_tf_dataset()\n",
    "def encode(\n",
    "        example: dict,\n",
    "        encoder_max_len: int = ENCODER_MAX_LEN, \n",
    "        decoder_max_len: int = DECODER_MAX_LEN,\n",
    "        tokenizer: transformers.PreTrainedTokenizer = TOKENIZER) -> dict:\n",
    "    \"\"\"Tokenize data.\n",
    "    Args:\n",
    "        example (dict): Raw dict parsed from DROP json:\n",
    "                            example['context']\n",
    "                            example['question']\n",
    "                            example['answer']\n",
    "    Returns: \n",
    "        (dict) Dictionary with values tokenized:\n",
    "                            return['input_ids']\n",
    "                            return['attention_mask']\n",
    "                            return['labels']\n",
    "                            return['decoder_attention_mask']\n",
    "    \"\"\"            \n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    answer = example['answer']\n",
    "  \n",
    "    # TODO: this representation is not good - better to separately embed\n",
    "    # question and context.\n",
    "    question_plus = f\"answer_me: {str(question)} context: {str(context)}\"\n",
    "    answer_plus = str(answer)\n",
    "\n",
    "    encoder_inputs = tokenizer(\n",
    "                            question_plus, \n",
    "                            truncation=True, \n",
    "                            return_tensors='tf', \n",
    "                            max_length=encoder_max_len,\n",
    "                            # padding='longest',\n",
    "                            pad_to_max_length=True)\n",
    "    decoder_inputs = tokenizer(\n",
    "                            answer_plus, \n",
    "                            truncation=True, \n",
    "                            return_tensors='tf', \n",
    "                            max_length=decoder_max_len,\n",
    "                            # padding='longest',\n",
    "                            pad_to_max_length=True)\n",
    "    # https://tinyurl.com/y2yh56gp\n",
    "    # input_ids – Token ids to be fed to the encoder.\n",
    "    # attention_mask – Specifying which tokens are allowed to be attended by encoder.\n",
    "    # decoder_input_ids – Token ids to be fed to the decoder.\n",
    "    # decoder_attention_mask – Specifying which tokens are allowed to be attended by \n",
    "    #      decoder. Note that this is NOT the mask for the transformer decoder's \n",
    "    #      self-attention mechanism (i.e. not casual attention). The mask here is \n",
    "    #      simply telling which tokens are allowed to be attended (e.g. after padding is added).\n",
    "    # [0] convert to rank 1 array (e.g. (1, 250) => (250,))\n",
    "    return {\n",
    "        'input_ids': encoder_inputs['input_ids'][0], \n",
    "        'attention_mask': encoder_inputs['attention_mask'][0], \n",
    "        'labels': decoder_inputs['input_ids'][0], \n",
    "        'decoder_attention_mask': decoder_inputs['attention_mask'][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellView": "both",
    "id": "x9rc72mrwSVd"
   },
   "outputs": [],
   "source": [
    "#@title Define function: load_dataset()\n",
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "def _bytes_feature(values):\n",
    "  \"\"\"Returns a bytes_list from a list of string / byte.\"\"\"\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=values))\n",
    "\n",
    "def _float_feature(values):\n",
    "  \"\"\"Returns a float_list from a list of float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n",
    "\n",
    "def _int64_feature(values):\n",
    "    \"\"\"Returns an int64_list from a list of bool / enum / int / uint.\"\"\"\n",
    "    try:\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n",
    "    except Exception as e:\n",
    "        print(f'failed at: {values}')\n",
    "        raise e\n",
    "\n",
    "def to_serialized_example(encoded_example):\n",
    "    feature = {\n",
    "        'input_ids': _int64_feature(encoded_example['input_ids']),\n",
    "        'attention_mask': _int64_feature(encoded_example['attention_mask']),\n",
    "        'labels': _int64_feature(encoded_example['labels']),\n",
    "        'decoder_attention_mask': _int64_feature(encoded_example['decoder_attention_mask']),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def make_data_generator(file_path):\n",
    "    \"\"\"\n",
    "    Returns a generator function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # with open(file_path) as json_file:\n",
    "    with tf.io.gfile.GFile(file_path) as json_file:\n",
    "        data_json_dict = json.load(json_file)  # dict of dict.\n",
    "\n",
    "        passages = data_json_dict['context']\n",
    "        questions = data_json_dict['questions']\n",
    "        answers = data_json_dict['answers']\n",
    "\n",
    "    def gen():\n",
    "        for passage, question, answer in zip(passages, questions, answers):\n",
    "            encoded_example = encode({\n",
    "                'context': passage,\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "            })\n",
    "            serialized_example = to_serialized_example(encoded_example)\n",
    "            yield serialized_example\n",
    "    return gen\n",
    "\n",
    "def get_num_examples(file_path):\n",
    "    with open(file_path) as json_file:\n",
    "        data_json_dict = json.load(json_file)  # dict of dict.\n",
    "        return len(data_json_dict)\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    # num_examples = get_num_examples(file_path)\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        make_data_generator(file_path),\n",
    "        output_types=tf.string\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_tf_record_file(file_path):\n",
    "    dataset = load_dataset(file_path)\n",
    "\n",
    "    tf_record_file_path = os.path.join(\n",
    "        os.path.dirname(file_path),\n",
    "        os.path.basename(file_path).replace('.json', '.tfrecord')\n",
    "    )\n",
    "    print(f'Writing TF Record file to: {tf_record_file_path} ...')\n",
    "    writer = tf.data.experimental.TFRecordWriter(tf_record_file_path)\n",
    "    writer.write(dataset)\n",
    "\n",
    "\n",
    "WRITE_JSON_TO_TF_RECORD_FILE = True  #@param { type: \"boolean\" }\n",
    "if WRITE_JSON_TO_TF_RECORD_FILE:\n",
    "    write_json_to_tf_record_file(DEV_DATA_PATH)\n",
    "    write_json_to_tf_record_file(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owOKH0HDUqFI"
   },
   "outputs": [],
   "source": [
    "def print_tfrecord(tfrecrod_file_path, example_count=1):\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_file_path)\n",
    "    for raw_record in raw_dataset.take(example_count):\n",
    "        example_proto = tf.train.Example.FromString(raw_record.numpy())\n",
    "        print(example_proto)\n",
    "\n",
    "PRINT_TF = False #@param { type: \"boolean\"}\n",
    "tfrecord_file_path = # path to squad tfrecord on gs #@param { type: \"string\" }\n",
    "\n",
    "if PRINT_TF:\n",
    "    print_tfrecord(tfrecord_file_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "synthetic_data_tfrecord_squad.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

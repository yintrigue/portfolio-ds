# Neural Snaky  
**Model**: Neural Network, Logistics  
**Tech Stack:** PyGame, TensorFlow, Scikit-learn, Python   

## Overview
Neural Snaky is a PyGame implementation of the classic game, Snake. This is a personal weekend project that started as a simple game that I build for my 3 year old daughter. Then, I thought: Why don't I build a brain for the snake so that it can learns and plays the game by itself? The results is a Snake game with three modes:

- "**Play**" mode is nothing but a simple replicate of the classic game, Snake. The game is built using PyGame and the sprites can be easily customized for future updates. One key difference from the traditional Snake game is that the snake moves one step at a time per arrow key press. The design decision is to because my goal is to build something fun for my 3 year old daughter and help her improve hand-eye coordination.
- "**Autoplay**" gives the snake a brain to learn the game and play on its own.
- "**Replay**" mode replays the recorded game plays. Both Play and Autoplay modes can be recorded.

The game is built using OOP. Below is a simplified UML that shows select key properties and methods. `Snaky` is the tradition Snake game I build for my 3-year-old daughter while `SmartSnaky` is the Snake that is able to learns and plays itself.  

   <img src="https://www.dropbox.com/s/i2bhe7x8fhipjzl/uml.jpg?raw=1" width="600"/>

## Datasets

The training set is generated by "automatically" playing and recording the game (moves, results, visions, scores, etc.). The automatic play is achieved by making random moves (i.e. up, right, down, and left) until the snake hits an obstacle and die (i.e. a wall or the snake's own body). In addition, what the snake can see (i.e. its "vision") is recorded along each of the moves made by the snake. Depending on the chosen model as described in the Models section, the snake can see up to a total of three different objects: obstacle (i.e. a wall or a snake body), apple, and dead-end. An example of the snake's vision is:

| Obstacle | Apple | Dead-end |
| -------- | ----- | -------- |
| 1110     | 0110  | 0001     |

Each digit of the snake's vision represents what the snake can see in a certain direction. The four digits map to **up**, **right**, **down**, and **left** respectively. There are up to there objects the snake can see depending on the chosen model (as described in the Models section): obstacle, apple, and dead-end. 

- **Obstacle** indicates if a move in the chosen direction would hit an obstacle (i.e. a wall or a snake body). 
- **Apple** indicates if a move in the chosen direction would be closer to an apple. 
- **Dead-end** indicates if a move in the chosen direction would form a closed loop between the wall and the snake's body.

The **label** to be predicted depends on  the chosen model and what the snake can see. In Model 3, for example, where the snake is able to see obstacles and apples, the label is:

- 0 - Dead by hitting an obstacle 
- 1 - Live & closer to the apple
- 2 - Live & away from the apple

The snake determines its next step in the ascending order. That is: 2 is preferred over 1, and 1 is preferred over 0.

## Models & Results

Four models are built as the "brains" that enable the snake to learn and play the game on its own:

- **Model 1**: Logistic Regression + Vision on Obstacles
- **Model 2**: Neural Net + Vision on Obstacles
- **Model 3**: Neural Net + Vision on Obstacles & Apples
- **Model 4**: Neural Net + Vision on Obstacles & Apples & Dead-ends

The snake in Model 1 and 2 are unable to "see" the apple. As a result, the models simply learn the best strategy to survive: running in a loop. 

https://user-images.githubusercontent.com/57857777/117561027-e4a33580-b047-11eb-9429-cea071e856c6.mov

The snake in Model 3 is given the ability to see if its next move would get closer to the apple. As a result, the snake now is able to not just survive but also play the game for "real" and score by eating apples. However, given its current vision, the snake is unable to see beyond its very first move. As a result, the snake constantly runs into a dead-end formed by the wall, the snake's body, or a combination of the two as the snake grows long after eating multiple apples.

https://user-images.githubusercontent.com/57857777/117561040-013f6d80-b048-11eb-9c4c-50b3e966109f.mov

Now to solve the problem of the snake forming closed loop as it grows long, my immediate intuition is to add a new vision capability: the ability for the snake to see if its chosen next move would form a closed loop. However, it turns out that the geometric closed loop detection is not as straightforward as I think it would be. So the new plan for the next phase of the development (whenever I have time, of course) is to explore **deep Q reinforcement learning** which would allow me to optimize the snake's learning against the longterm rewards instead of just the survival in the very next step using the Bellman Equation:  
$$
Q(s, a)=r(s, a)+\gamma \max _{a} Q\left(s^{\prime}, a\right)
$$

## Gameplay Screenshots

<img src="./img/skin_02/screenshot_01.png" width="500"/>
<img src="./img/skin_02/screenshot_02.png" width="500"/>
<img src="./img/skin_02/screenshot_03.png" width="500"/>





## Repository 

- `demo.py` is a simple demo of the game. To run the game, simply enter `python demo.py` in the terminal.
- `./data/train.csv` is a sample of the training set generated by the random autoplay.
- `./data/autoplay.csv` is a sample record of the game played by Model 3.
- `./src` includes all source codes.
- `./models` includes the trained weights of the three models described in the Models section.  
- `./assets` includes sprites and font used for the game.
- `./img` includes images used for the readme markdown.  


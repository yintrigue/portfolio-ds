{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6t55qCf3Fsxa"
   },
   "source": [
    "# Newsgroup Classification Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9-9G8z4Fsxd"
   },
   "source": [
    "The goal of the project is to build and fine-tune multiple logistic regression models to classify posts by topics inferred from the text data from newsgroup posts on a variety of topics.\n",
    "\n",
    "The dataset includes a total of 3,387 text-based examples collected from a newsgroup with various topics. Whereas with digit classification, where each input is relatively dense (represented as a 28x28 matrix of pixels, many of which are non-zero), here each document is relatively sparse (represented as a bag-of-words). Only a few words of the total vocabulary are active in any given document. The assumption is that a label depends only on the count of words, not their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUYW83LqFsxd"
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import nltk\n",
    "\n",
    "# additional libraries just for formatting the printout\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ALNjGgCFsxg"
   },
   "source": [
    "Load the data, stripping out metadata so that only textual features will be used, and restricting documents to 4 specific topics. By default, newsgroups data is split into training and test sets, but here the test set gets further split into development and test sets.  (If you remove the categories argument from the fetch function calls, you'd get documents from all 20 topics.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecYpcoxaFsxh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "dev label shape: (676,)\n",
      "test label shape: (677,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test  = fetch_20newsgroups(subset='test',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "\n",
    "num_test = int(len(newsgroups_test.target) / 2)\n",
    "test_data, test_labels   = newsgroups_test.data[num_test:], newsgroups_test.target[num_test:]\n",
    "dev_data, dev_labels     = newsgroups_test.data[:num_test], newsgroups_test.target[:num_test]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:',      dev_labels.shape)\n",
    "print('test label shape:',     test_labels.shape)\n",
    "print('labels names:',         newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHTnOke6Fsxk"
   },
   "source": [
    "### Part 1:\n",
    "\n",
    "For each of the first 5 training examples, print the text of the message along with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8zhA06xFsxl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06 th {\n",
       "          text-align: left;\n",
       "    }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row0_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row0_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row0_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row1_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row1_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row1_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row2_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row2_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row2_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row3_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row3_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row3_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row4_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row4_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row4_col2 {\n",
       "            text-align:  left;\n",
       "        }</style><table id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >Label</th>        <th class=\"col_heading level0 col1\" >Category</th>        <th class=\"col_heading level0 col2\" >Data</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "                        <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row0_col1\" class=\"data row0 col1\" >comp.graphics</td>\n",
       "                        <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row0_col2\" class=\"data row0 col2\" >Hi,\n",
       "\n",
       "I've noticed that if you only save a model (with all your mapping planes\n",
       "positioned carefully) to a .3DS file that when you reload it after restarting\n",
       "3DS, they are given a default position and orientation.  But if you save\n",
       "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
       "know why this information is not stored in the .3DS file?  Nothing is\n",
       "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
       "I'd like to be able to read the texture rule information, does anyone have \n",
       "the format for the .PRJ file?\n",
       "\n",
       "Is the .CEL file format available from somewhere?\n",
       "\n",
       "Rych</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row1_col0\" class=\"data row1 col0\" >3</td>\n",
       "                        <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row1_col1\" class=\"data row1 col1\" >talk.religion.misc</td>\n",
       "                        <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row1_col2\" class=\"data row1 col2\" >\n",
       "\n",
       "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
       "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
       "folks with him, children and all, to satisfy his delusional mania. Jim\n",
       "Jones, circa 1993.\n",
       "\n",
       "\n",
       "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
       "for centuries.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "                        <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row2_col1\" class=\"data row2 col1\" >sci.space</td>\n",
       "                        <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row2_col2\" class=\"data row2 col2\" >\n",
       " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
       "\n",
       "MB>                                                             So the\n",
       "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
       "\n",
       "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
       "\n",
       "Couldn't we just say periapsis or apoapsis?\n",
       "\n",
       " </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "                        <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row3_col1\" class=\"data row3 col1\" >alt.atheism</td>\n",
       "                        <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row3_col2\" class=\"data row3 col2\" >I have a request for those who would like to see Charley Wingate\n",
       "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
       "appear to be quite a few of you.)  \n",
       "\n",
       "It is clear that Mr. Wingate intends to continue to post tangential or\n",
       "unrelated articles while ingoring the Challenges themselves.  Between\n",
       "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
       "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
       "\n",
       "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
       "will just go away, and he is doing his level best to change the\n",
       "subject.  Given that this seems a rather common net.theist tactic, I\n",
       "would like to suggest that we impress upon him our desire for answers,\n",
       "in the following manner:\n",
       "\n",
       "1. Ignore any future articles by Mr. Wingate that do not address the\n",
       "Challenges, until he answers them or explictly announces that he\n",
       "refuses to do so.\n",
       "\n",
       "--or--\n",
       "\n",
       "2. If you must respond to one of his articles, include within it\n",
       "something similar to the following:\n",
       "\n",
       "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
       "\n",
       "Really, I'm not looking to humiliate anyone here, I just want some\n",
       "honest answers.  You wouldn't think that honesty would be too much to\n",
       "ask from a devout Christian, would you?  \n",
       "\n",
       "Nevermind, that was a rhetorical question.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row4_col0\" class=\"data row4 col0\" >2</td>\n",
       "                        <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row4_col1\" class=\"data row4 col1\" >sci.space</td>\n",
       "                        <td id=\"T_a4d25af4_bf01_11ea_9b2a_3c15c2e74c06row4_col2\" class=\"data row4 col2\" >AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
       "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
       "\n",
       "Does anyone know more about this?  How much, to attend????\n",
       "\n",
       "Anyone want to go?</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff99a61b4f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#def P1(num_examples=5):\n",
    "    ### STUDENT START ###\n",
    "d = np.array(train_data)[0:5]\n",
    "l = np.array(train_labels)[0:5]\n",
    "c = np.array(newsgroups_train.target_names)[l]\n",
    "df = pd.DataFrame({'Label': l, 'Category': c, 'Data': d})\n",
    "\n",
    "# print df\n",
    "pd.set_option('max_colwidth',10000)\n",
    "df.style.set_properties(**{'text-align': 'left'})\n",
    "s = df.style\n",
    "s = s.set_properties(**{'text-align': 'left'}).set_table_styles([ \n",
    "        dict(selector='th', props=[('text-align', 'left')]) \n",
    "    ])\n",
    "s = s.hide_index()\n",
    "display(s)\n",
    "    ### STUDENT END ###\n",
    "#P1(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "onfno6uHFsxm"
   },
   "source": [
    "### Part 2:\n",
    "\n",
    "Transform the training data into a matrix of **word** unigram feature vectors.  What is the size of the vocabulary? What is the average number of non-zero features per example?  What is the fraction of the non-zero entries in the matrix?  What are the 0th and last feature strings (in alphabetical order)?<br/>\n",
    "_Use `CountVectorization` and its `.fit_transform` method.  Use `.nnz` and `.shape` attributes, and `.get_feature_names` method._\n",
    "\n",
    "Now transform the training data into a matrix of **word** unigram feature vectors using your own vocabulary with these 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"].  Confirm the size of the vocabulary. What is the average number of non-zero features per example?<br/>\n",
    "_Use `CountVectorization(vocabulary=...)` and its `.transform` method._\n",
    "\n",
    "Now transform the training data into a matrix of **character** bigram and trigram feature vectors.  What is the size of the vocabulary?<br/>\n",
    "_Use `CountVectorization(analyzer=..., ngram_range=...)` and its `.fit_transform` method._\n",
    "\n",
    "Now transform the training data into a matrix of **word** unigram feature vectors and prune words that appear in fewer than 10 documents.  What is the size of the vocabulary?<br/>\n",
    "_Use `CountVectorization(min_df=...)` and its `.fit_transform` method._\n",
    "\n",
    "Now again transform the training data into a matrix of **word** unigram feature vectors. What is the fraction of words in the development vocabulary that is missing from the training vocabulary?<br/>\n",
    "_Hint: Build vocabularies for both train and dev and look at the size of the difference._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyVwk5RvFsxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 1 (Raw Unigram, Word):\n",
      "Size of the vocabulary: 26,879\n",
      "Avg number of non-zero features per example: 97\n",
      "Fraction of the non-zero entries: 0.0036\n",
      "0th feature strings: 00\n",
      "Last feature strings: zyxel\n",
      "\n",
      "PART 2 (Custom Vocabularies):\n",
      "Vocabularies: ['atheism', 'graphics', 'space', 'religion']\n",
      "Size of the vocabulary: 4\n",
      "Avg number of non-zero features per example: 0.27\n",
      "\n",
      "PART 3 (Char Analyzer):\n",
      "Size of the vocabulary (char bigram): 3,291\n",
      "Size of the vocabulary (char trigram): 32,187\n",
      "Size of the vocabulary (char bigram/trigram): 35,478\n",
      "\n",
      "PART 4 (Filtered Word Analyzer):\n",
      "Size of the vocabulary (word unigram, frq < 10 removed): 3,064\n",
      "\n",
      "PART 5 (Missing Words in Dev):\n",
      "# of training vocabularies found in dev dataset: 12,219\n",
      "Missing vocabularies ratio in dev dataset: 0.25\n"
     ]
    }
   ],
   "source": [
    "#def P2():\n",
    "    ### STUDENT START ###    \n",
    "# vectorization\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(train_data)\n",
    "col_names = cv.get_feature_names()\n",
    "\n",
    "# find requested stats\n",
    "n, voc_size = x.shape[0], x.shape[1]\n",
    "avg_fea_per_ex = x.nnz / n\n",
    "frac_nonzero = x.nnz / (n * voc_size)\n",
    "fea_first = col_names[0]\n",
    "fea_last = col_names[len(col_names) - 1]\n",
    "\n",
    "# print results\n",
    "print(\"PART 1 (Raw Unigram, Word):\")\n",
    "print('Size of the vocabulary: {:,}'.format(voc_size))\n",
    "print('Avg number of non-zero features per example: {:.0f}'.format(avg_fea_per_ex))\n",
    "print('Fraction of the non-zero entries: {:.4f}'.format(frac_nonzero))\n",
    "print('0th feature strings: {}'.format(fea_first))\n",
    "print('Last feature strings: {}'.format(fea_last))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "MY_VOC = ['atheism', 'graphics', 'space', 'religion']\n",
    "\n",
    "# vectorization\n",
    "cv = CountVectorizer(vocabulary = MY_VOC)\n",
    "x = cv.fit_transform(train_data)\n",
    "n, voc_size = x.shape[0], x.shape[1]\n",
    "avg_fea_per_ex = x.nnz / n\n",
    "print(\"\\nPART 2 (Custom Vocabularies):\")\n",
    "print('Vocabularies: {}'.format(cv.get_feature_names()))\n",
    "print('Size of the vocabulary: {:,}'.format(voc_size))\n",
    "print('Avg number of non-zero features per example: {:.2f}'.format(avg_fea_per_ex))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "x_bigram = CountVectorizer(analyzer='char', \n",
    "                           ngram_range=[2, 2]).fit_transform(train_data)\n",
    "x_trigram = CountVectorizer(analyzer='char', \n",
    "                            ngram_range=[3, 3]).fit_transform(train_data)\n",
    "x_bi_tri = CountVectorizer(analyzer='char', \n",
    "                           ngram_range=[2, 3]).fit_transform(train_data)\n",
    "\n",
    "print(\"\\nPART 3 (Char Analyzer):\")\n",
    "print('Size of the vocabulary (char bigram): {:,}'.format(x_bigram.shape[1]))\n",
    "print('Size of the vocabulary (char trigram): {:,}'.format(x_trigram.shape[1]))\n",
    "print('Size of the vocabulary (char bigram/trigram): {:,}'.format(x_bi_tri.shape[1]))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "x = CountVectorizer(analyzer='word', min_df=10).fit_transform(train_data)\n",
    "print(\"\\nPART 4 (Filtered Word Analyzer):\")\n",
    "print('Size of the vocabulary (word unigram, frq < 10 removed): {:,}'.format(x.shape[1]))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "cv_train = CountVectorizer()\n",
    "cv_dev = CountVectorizer()\n",
    "cv_train.fit_transform(train_data)\n",
    "cv_dev.fit_transform(dev_data)\n",
    "\n",
    "fea_names_train = cv_train.get_feature_names()\n",
    "fea_names_dev = cv_dev.get_feature_names()\n",
    "num_of_train_vocs_in_dev = np.sum(np.isin(fea_names_dev, fea_names_train))\n",
    "r = 1 - num_of_train_vocs_in_dev / len(fea_names_dev)\n",
    "\n",
    "print(\"\\nPART 5 (Missing Words in Dev):\")\n",
    "print('# of training vocabularies found in dev dataset: {:,d}'.format(num_of_train_vocs_in_dev))\n",
    "print('Missing vocabularies ratio in dev dataset: {:.2f}'.format(r))\n",
    "    ### STUDENT END ###\n",
    "\n",
    "#P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydjkRh6LFsxp"
   },
   "source": [
    "### Part 3:\n",
    "\n",
    "Transform the training and development data to matrices of word unigram feature vectors.\n",
    "\n",
    "1. Produce several k-Nearest Neigbors models by varying k, including one with k set to optimize f1 score.  For each model, show the k value and f1 score.\n",
    "1. Produce several Naive Bayes models by varying smoothing (alpha), including one with alpha set approximately to optimize f1 score.  For each model, show the alpha value and f1 score.\n",
    "1. Produce several Logistic Regression models by varying L2 regularization strength (C), including one with C set approximately to optimize f1 score.  For each model, show the C value, f1 score, and sum of squared weights for each topic.\n",
    "\n",
    "* Why doesn't k-Nearest Neighbors work well for this problem?\n",
    "* Why doesn't Logistic Regression work as well as Naive Bayes does?\n",
    "* What is the relationship between logistic regression's sum of squared weights vs. C value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvhpODdWFsxp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 (KNN Model):\n",
      "K = 1, Micro F1 = 0.3831\n",
      "K = 3, Micro F1 = 0.4142\n",
      "K = 5, Micro F1 = 0.4231\n",
      "K = 7, Micro F1 = 0.4438\n",
      "K = 9, Micro F1 = 0.4305\n",
      "K = 11, Micro F1 = 0.4186\n",
      "K = 13, Micro F1 = 0.4231\n",
      "K = 15, Micro F1 = 0.4334\n",
      "K = 17, Micro F1 = 0.4482\n",
      "K = 19, Micro F1 = 0.4393\n",
      "10 of the 50 models created (K = 1, 3, 5, ..., 99) are shown.\n",
      "The approx. optimal micro f1 value (0.4763) is found at k=97\n",
      "\n",
      "Part 2 (NB Model):\n",
      "alpha = 0, Micro F1 = 0.7544\n",
      "alpha = 1e-10, Micro F1 = 0.7544\n",
      "alpha = 0.0001, Micro F1 = 0.7692\n",
      "alpha = 0.001, Micro F1 = 0.7751\n",
      "alpha = 0.01, Micro F1 = 0.7796\n",
      "alpha = 0.1, Micro F1 = 0.7929\n",
      "alpha = 0.5, Micro F1 = 0.7885\n",
      "alpha = 1, Micro F1 = 0.7811\n",
      "alpha = 2, Micro F1 = 0.7737\n",
      "alpha = 10, Micro F1 = 0.6967\n",
      "alpha = 100, Micro F1 = 0.5710\n",
      "alpha = 1000, Micro F1 = 0.4127\n",
      "alpha = 10000, Micro F1 = 0.3225\n",
      "alpha = 100000, Micro F1 = 0.2959\n",
      "14 models were created.\n",
      "The approx. optimal micro f1 value (0.7929) is found at alpha=0.1\n",
      "\n",
      "Part 3 (Logistic Regression):\n",
      "C = 0.0001, F1 = 0.5562, SSW = [0.01, 0.01, 0.01, 0.01]\n",
      "C = 0.001, F1 = 0.6376, SSW = [0.17, 0.20, 0.18, 0.19]\n",
      "C = 0.01, F1 = 0.6790, SSW = [2.54, 2.94, 2.86, 2.25]\n",
      "C = 0.1, F1 = 0.7041, SSW = [27.13, 24.66, 27.46, 23.02]\n",
      "C = 0.3, F1 = 0.7160, SSW = [69.30, 57.87, 67.91, 59.76]\n",
      "C = 0.5, F1 = 0.7145, SSW = [102.64, 83.12, 99.03, 89.03]\n",
      "C = 0.7, F1 = 0.7012, SSW = [130.86, 104.12, 124.95, 113.83]\n",
      "C = 0.9, F1 = 0.6997, SSW = [155.74, 122.54, 147.66, 135.69]\n",
      "C = 1, F1 = 0.7012, SSW = [166.92, 130.87, 157.95, 145.71]\n",
      "C = 2, F1 = 0.6967, SSW = [257.41, 197.91, 239.97, 226.60]\n",
      "C = 3, F1 = 0.6953, SSW = [323.85, 247.69, 300.04, 286.80]\n",
      "C = 4, F1 = 0.6923, SSW = [378.13, 288.29, 348.60, 335.96]\n",
      "C = 5, F1 = 0.6953, SSW = [422.83, 322.45, 389.44, 377.89]\n",
      "C = 6, F1 = 0.6953, SSW = [462.15, 352.85, 425.95, 414.61]\n",
      "C = 7, F1 = 0.6953, SSW = [497.91, 379.88, 458.35, 447.87]\n",
      "C = 8, F1 = 0.6923, SSW = [529.87, 404.40, 487.58, 477.38]\n",
      "C = 9, F1 = 0.6923, SSW = [558.94, 427.11, 514.14, 504.79]\n",
      "C = 10, F1 = 0.6893, SSW = [586.46, 448.14, 539.89, 531.25]\n",
      "C = 100, F1 = 0.6834, SSW = [1402.23, 1090.45, 1293.60, 1312.74]\n",
      "C = 1000, F1 = 0.6849, SSW = [2648.63, 1924.52, 2315.89, 2570.30]\n",
      "C = 10000, F1 = 0.6893, SSW = [2212.08, 2223.98, 2786.28, 3433.14]\n",
      "21 models were created.\n",
      "The approx. optimal micro f1 value (0.7160) is found at C=0.30 (or lambda=3.33)\n"
     ]
    }
   ],
   "source": [
    "#def P3():\n",
    "    ### STUDENT START ###\n",
    "# build training feature set\n",
    "cv_train = CountVectorizer()\n",
    "x_train = cv_train.fit_transform(train_data)\n",
    "\n",
    "# build test feature set\n",
    "cv_dev = CountVectorizer(vocabulary = cv_train.get_feature_names())\n",
    "x_dev = cv_dev.fit_transform(dev_data)\n",
    "\n",
    "# KNN\n",
    "# ----------------------------------------------------------------------\n",
    "def knn_predict(k: int, \n",
    "                train_data: np.ndarray, \n",
    "                train_labels: np.ndarray,\n",
    "                x: np.ndarray = dev_data) -> np.ndarray:\n",
    "    \"\"\"Predict using k-nn.\n",
    "    Return:\n",
    "        (np.ndarray): Predicted labels.\n",
    "    \"\"\"\n",
    "    m = KNeighborsClassifier()\n",
    "    m.set_params(n_neighbors=k)\n",
    "    m.fit(train_data, train_labels)\n",
    "    return m.predict(x)\n",
    "\n",
    "# build KNN models\n",
    "print(\"Part 1 (KNN Model):\")\n",
    "f1s = []\n",
    "for i in range(1, 100, 2):\n",
    "    dev_pds = knn_predict(i, x_train, train_labels, x_dev)\n",
    "    \n",
    "    # micro f1 is the same as accuracy in this project because every \n",
    "    # test case is guaranteed to be assigned to EXACTLY one class).\n",
    "    f1 = classification_report(dev_labels, dev_pds, output_dict=True)['accuracy']\n",
    "    \n",
    "    # only print the first 10 reports\n",
    "    if i <= 20:\n",
    "        print('K = {:.0f}, Micro F1 = {:.4f}'.format(i, f1))\n",
    "    f1s.append({'k': i, 'f1': f1})\n",
    "\n",
    "# find best f1\n",
    "optimal = max(f1s, key=lambda x: x['f1'])\n",
    "\n",
    "# print results\n",
    "print('10 of the 50 models created (K = 1, 3, 5, ..., 99) are shown.')\n",
    "print('The approx. optimal micro f1 value ({:.4f}) is found at k={}'. \\\n",
    "      format(optimal['f1'], optimal['k']))\n",
    "\n",
    "# NB\n",
    "# ----------------------------------------------------------------------\n",
    "# remove warning message due to poor predictions\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ALPHAS = (0, 1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, \n",
    "          1, 2, 10, 100, 1000, 10000, 100000)\n",
    "\n",
    "# build NB models\n",
    "print(\"\\nPart 2 (NB Model):\")\n",
    "f1s = []\n",
    "for alpha in ALPHAS:    \n",
    "    nb = MultinomialNB(alpha=alpha)\n",
    "    nb.fit(x_train, train_labels)\n",
    "    dev_pds = nb.predict(x_dev)\n",
    "    \n",
    "    f1 = classification_report(dev_labels, dev_pds, output_dict=True)['accuracy']\n",
    "    print('alpha = {}, Micro F1 = {:.4f}'.format(alpha, f1))\n",
    "    \n",
    "    f1s.append({'alpha': alpha, 'f1': f1})\n",
    "\n",
    "# find best f1\n",
    "optimal = max(f1s, key=lambda x: x['f1'])\n",
    "\n",
    "# print results\n",
    "print('{} models were created.'.format(len(ALPHAS)))\n",
    "print('The approx. optimal micro f1 value ({:.4f}) is found at alpha={}'. \\\n",
    "      format(optimal['f1'], optimal['alpha']))\n",
    "\n",
    "# Logistic Regression\n",
    "# ----------------------------------------------------------------------\n",
    "C_ARR = [0.0001, 0.001, 0.01]\n",
    "C_ARR.extend([i/10 for i in range(1, 11, 2)])\n",
    "C_ARR.extend(list(range(1, 11)))\n",
    "C_ARR.extend([100, 1000, 10000])\n",
    "\n",
    "print(\"\\nPart 3 (Logistic Regression):\")\n",
    "f1s = []\n",
    "for c in C_ARR:\n",
    "    lr = LogisticRegression(C=c, solver=\"liblinear\", multi_class=\"auto\")\n",
    "    lr_m = lr.fit(x_train, train_labels)\n",
    "    dev_pds = lr_m.predict(x_dev)\n",
    "    \n",
    "    f1 = classification_report(dev_labels, dev_pds, output_dict=True)['accuracy']\n",
    "    ssw = np.round(np.sum(lr.coef_ ** 2, axis=1), 4).tolist()\n",
    "    print('C = {}, F1 = {:.4f}, SSW = [{:.2f}, {:.2f}, {:.2f}, {:.2f}]'. \\\n",
    "          format(c, f1, ssw[0], ssw[1], ssw[2], ssw[3]))\n",
    "    \n",
    "    f1s.append({'f1': f1, 'c': c})\n",
    "\n",
    "# print optimal result\n",
    "optimal = max(f1s, key=lambda x: x['f1'])\n",
    "print('{} models were created.'.format(len(f1s)))\n",
    "print('The approx. optimal micro f1 value ({:.4f}) is found at C={:.2f} (or lambda={:.2f})'. \\\n",
    "      format(optimal['f1'], optimal['c'], 1/optimal['c']))\n",
    "    ## STUDENT END ###\n",
    "#P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWtXwAlOFsxr"
   },
   "source": [
    "ANSWER:  \n",
    "Q1  \n",
    "**Why doesn't k-Nearest Neighbors work well?**  \n",
    "KNN suffers from the curse of high dimensionality. The feature space is huge with thousands of vocabularies overlapping between the training and development datasets. All data points appear far away from each other in such high dimension.\n",
    "  \n",
    "Q2  \n",
    "**Why doesn't Logistic Regression work as well as Naive Bayes does?**  \n",
    "There are three plausible explanations:\n",
    "In general, generative model (Naive Bayes) reaches its asymptotic optimal faster than the discriminative model (logistic regression) according to the [paper](https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf) published by Professor Andrew Ng and Professor Michael I Jordan. In the case Naive Bayes vs. logistic regression, prior probabilities would help Naive Bayes to converge to its optimal faster. In sum, it is highly possible that we just don't have enough examples for the logistic regression to converge. \n",
    "  \n",
    "Q3  \n",
    "**What is the relationship between logistic regression's sum of squared weights vs. C value?**  \n",
    "The cost function of the logistic regression is:  \n",
    "  \n",
    "$J(\\theta)=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{(i)} \\log \\left(h_{\\theta}\\left(x^{(i)}\\right)\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right)\\right]+\\frac{\\lambda}{2 m} \\sum_{j=1}^{n} \\theta_{j}^{2}$  \n",
    "where $C = \\frac{1}{\\lambda}$ and $\\theta_j$ is the jth weight being estimated by the logistic regression. \n",
    "  \n",
    "That is:\n",
    "- The larger $C$ is, the smaller $\\lambda$ becomes, and the smaller the regularization term $\\frac{\\lambda}{2 m} \\sum_{j=1}^{n} w_{j}^{2}$ becomes. As a result, the effect of regularization is reduced. When $C$ goes infinite,  $\\lambda$ becomes zero and the effect of regularization is completely eliminated.\n",
    "- The smaller $C$ is, the larger $\\lambda$ becomes, and the larger the regularization term $\\frac{\\lambda}{2 m} \\sum_{j=1}^{n} w_{j}^{2}$ becomes. As a result, the effect of regularization is increased. When $C$ goes zero,  $\\lambda$ becomes infinite and the effect of regularization goes infinite (a rather meaningless setup because all weights would be driven to zero).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGEjsm_uFsxr"
   },
   "source": [
    "### Part 4:\n",
    "\n",
    "Transform the data to a matrix of word **bigram** feature vectors.  Produce a Logistic Regression model.  For each topic, find the 5 features with the largest weights (that's 20 features in total).  Show a 20 row (features) x 4 column (topics) table of the weights.\n",
    "\n",
    "Do you see any surprising features in this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WN51Nv4fFsxs"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06 th {\n",
       "          vertical-align: top;\n",
       "    }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row0_col0 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row1_col0 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row2_col0 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row3_col0 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row4_col0 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row5_col1 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row6_col1 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row7_col1 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row8_col1 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row9_col1 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row10_col2 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row11_col2 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row12_col2 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row13_col2 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row14_col2 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row15_col3 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row16_col3 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row17_col3 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row18_col3 {\n",
       "            color:  #ff8c00;\n",
       "        }    #T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row19_col3 {\n",
       "            color:  #ff8c00;\n",
       "        }</style><table id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06\" ><thead>    <tr>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >alt.atheism</th>        <th class=\"col_heading level0 col1\" >comp.graphics</th>        <th class=\"col_heading level0 col2\" >sci.space</th>        <th class=\"col_heading level0 col3\" >talk.religion.misc</th>    </tr>    <tr>        <th class=\"index_name level0\" >Category</th>        <th class=\"index_name level1\" >Rank</th>        <th class=\"index_name level2\" >Bigram</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level0_row0\" class=\"row_heading level0 row0\" rowspan=5>alt.atheism</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row0\" class=\"row_heading level1 row0\" >1</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row0\" class=\"row_heading level2 row0\" >claim that</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row0_col0\" class=\"data row0 col0\" >0.61</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row0_col1\" class=\"data row0 col1\" >-0.20</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row0_col2\" class=\"data row0 col2\" >-0.27</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row0_col3\" class=\"data row0 col3\" >-0.14</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row1\" class=\"row_heading level1 row1\" >2</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row1\" class=\"row_heading level2 row1\" >cheers kent</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row1_col0\" class=\"data row1 col0\" >0.56</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row1_col1\" class=\"data row1 col1\" >-0.70</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row1_col2\" class=\"data row1 col2\" >-0.66</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row1_col3\" class=\"data row1 col3\" >0.53</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row2\" class=\"row_heading level1 row2\" >3</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row2\" class=\"row_heading level2 row2\" >was just</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row2_col0\" class=\"data row2 col0\" >0.48</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row2_col1\" class=\"data row2 col1\" >-0.13</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row2_col2\" class=\"data row2 col2\" >-0.13</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row2_col3\" class=\"data row2 col3\" >-0.23</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row3\" class=\"row_heading level1 row3\" >4</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row3\" class=\"row_heading level2 row3\" >you are</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row3_col0\" class=\"data row3 col0\" >0.47</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row3_col1\" class=\"data row3 col1\" >-0.28</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row3_col2\" class=\"data row3 col2\" >-0.48</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row3_col3\" class=\"data row3 col3\" >0.03</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row4\" class=\"row_heading level1 row4\" >5</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row4\" class=\"row_heading level2 row4\" >are you</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row4_col0\" class=\"data row4 col0\" >0.45</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row4_col1\" class=\"data row4 col1\" >-0.25</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row4_col2\" class=\"data row4 col2\" >-0.10</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row4_col3\" class=\"data row4 col3\" >-0.31</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level0_row5\" class=\"row_heading level0 row5\" rowspan=5>comp.graphics</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row5\" class=\"row_heading level1 row5\" >1</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row5\" class=\"row_heading level2 row5\" >looking for</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row5_col0\" class=\"data row5 col0\" >-0.63</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row5_col1\" class=\"data row5 col1\" >1.11</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row5_col2\" class=\"data row5 col2\" >-0.50</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row5_col3\" class=\"data row5 col3\" >-0.57</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row6\" class=\"row_heading level1 row6\" >2</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row6\" class=\"row_heading level2 row6\" >in advance</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row6_col0\" class=\"data row6 col0\" >-0.46</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row6_col1\" class=\"data row6 col1\" >0.83</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row6_col2\" class=\"data row6 col2\" >-0.44</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row6_col3\" class=\"data row6 col3\" >-0.42</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row7\" class=\"row_heading level1 row7\" >3</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row7\" class=\"row_heading level2 row7\" >comp graphics</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row7_col0\" class=\"data row7 col0\" >-0.29</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row7_col1\" class=\"data row7 col1\" >0.80</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row7_col2\" class=\"data row7 col2\" >-0.37</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row7_col3\" class=\"data row7 col3\" >-0.29</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row8\" class=\"row_heading level1 row8\" >4</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row8\" class=\"row_heading level2 row8\" >out there</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row8_col0\" class=\"data row8 col0\" >-0.27</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row8_col1\" class=\"data row8 col1\" >0.76</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row8_col2\" class=\"data row8 col2\" >-0.48</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row8_col3\" class=\"data row8 col3\" >-0.28</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row9\" class=\"row_heading level1 row9\" >5</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row9\" class=\"row_heading level2 row9\" >is there</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row9_col0\" class=\"data row9 col0\" >-0.34</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row9_col1\" class=\"data row9 col1\" >0.75</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row9_col2\" class=\"data row9 col2\" >-0.47</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row9_col3\" class=\"data row9 col3\" >-0.26</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level0_row10\" class=\"row_heading level0 row10\" rowspan=5>sci.space</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row10\" class=\"row_heading level1 row10\" >1</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row10\" class=\"row_heading level2 row10\" >the space</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row10_col0\" class=\"data row10 col0\" >-0.27</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row10_col1\" class=\"data row10 col1\" >-0.53</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row10_col2\" class=\"data row10 col2\" >0.87</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row10_col3\" class=\"data row10 col3\" >-0.27</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row11\" class=\"row_heading level1 row11\" >2</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row11\" class=\"row_heading level2 row11\" >the moon</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row11_col0\" class=\"data row11 col0\" >-0.35</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row11_col1\" class=\"data row11 col1\" >-0.49</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row11_col2\" class=\"data row11 col2\" >0.83</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row11_col3\" class=\"data row11 col3\" >-0.21</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row12\" class=\"row_heading level1 row12\" >3</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row12\" class=\"row_heading level2 row12\" >sci space</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row12_col0\" class=\"data row12 col0\" >-0.26</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row12_col1\" class=\"data row12 col1\" >-0.33</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row12_col2\" class=\"data row12 col2\" >0.62</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row12_col3\" class=\"data row12 col3\" >-0.22</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row13\" class=\"row_heading level1 row13\" >4</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row13\" class=\"row_heading level2 row13\" >and such</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row13_col0\" class=\"data row13 col0\" >-0.20</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row13_col1\" class=\"data row13 col1\" >-0.34</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row13_col2\" class=\"data row13 col2\" >0.59</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row13_col3\" class=\"data row13 col3\" >-0.22</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row14\" class=\"row_heading level1 row14\" >5</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row14\" class=\"row_heading level2 row14\" >it was</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row14_col0\" class=\"data row14 col0\" >-0.20</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row14_col1\" class=\"data row14 col1\" >-0.31</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row14_col2\" class=\"data row14 col2\" >0.53</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row14_col3\" class=\"data row14 col3\" >-0.31</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level0_row15\" class=\"row_heading level0 row15\" rowspan=5>talk.religion.misc</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row15\" class=\"row_heading level1 row15\" >1</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row15\" class=\"row_heading level2 row15\" >the fbi</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row15_col0\" class=\"data row15 col0\" >-0.13</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row15_col1\" class=\"data row15 col1\" >-0.21</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row15_col2\" class=\"data row15 col2\" >-0.30</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row15_col3\" class=\"data row15 col3\" >0.55</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row16\" class=\"row_heading level1 row16\" >2</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row16\" class=\"row_heading level2 row16\" >cheers kent</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row16_col0\" class=\"data row16 col0\" >0.56</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row16_col1\" class=\"data row16 col1\" >-0.70</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row16_col2\" class=\"data row16 col2\" >-0.66</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row16_col3\" class=\"data row16 col3\" >0.53</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row17\" class=\"row_heading level1 row17\" >3</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row17\" class=\"row_heading level2 row17\" >ignorance is</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row17_col0\" class=\"data row17 col0\" >-0.16</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row17_col1\" class=\"data row17 col1\" >-0.17</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row17_col2\" class=\"data row17 col2\" >-0.14</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row17_col3\" class=\"data row17 col3\" >0.50</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row18\" class=\"row_heading level1 row18\" >4</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row18\" class=\"row_heading level2 row18\" >but he</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row18_col0\" class=\"data row18 col0\" >-0.19</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row18_col1\" class=\"data row18 col1\" >-0.22</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row18_col2\" class=\"data row18 col2\" >-0.14</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row18_col3\" class=\"data row18 col3\" >0.49</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level1_row19\" class=\"row_heading level1 row19\" >5</th>\n",
       "                        <th id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06level2_row19\" class=\"row_heading level2 row19\" >of jesus</th>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row19_col0\" class=\"data row19 col0\" >-0.09</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row19_col1\" class=\"data row19 col1\" >-0.17</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row19_col2\" class=\"data row19 col2\" >-0.21</td>\n",
       "                        <td id=\"T_b66c126e_bf01_11ea_9b2a_3c15c2e74c06row19_col3\" class=\"data row19 col3\" >0.42</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def P4():\n",
    "    ### STUDENT START ###\n",
    "# remove warning message due to poor predictions\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# vectorize the features\n",
    "cv = CountVectorizer(ngram_range=[2, 2])\n",
    "x_train_bigram = cv.fit_transform(train_data)\n",
    "fea_names = cv.get_feature_names()\n",
    "\n",
    "# build regression\n",
    "lr = LogisticRegression(C=0.5, solver=\"liblinear\", multi_class=\"auto\")\n",
    "lr_m = lr.fit(x_train_bigram, train_labels)\n",
    "\n",
    "# extract info\n",
    "coefs_all = lr_m.coef_\n",
    "coefs_top5 = lr_m.coef_.argsort(axis=1)[:, ::-1][:, 0:5]\n",
    "labels = newsgroups_train.target_names\n",
    "\n",
    "# create the base table\n",
    "df_coefs = pd.DataFrame(data=coefs_top5)\n",
    "df_coefs['Category'] = labels\n",
    "df_coefs = pd.melt(df_coefs, \n",
    "                   id_vars=['Category'],\n",
    "                   value_vars=[0, 1, 2, 3, 4],\n",
    "                   var_name='Rank', \n",
    "                   value_name='i')\n",
    "df_coefs = df_coefs.sort_values(by=['Category', 'Rank']).reset_index(drop=True)\n",
    "df_coefs['Bigram'] = df_coefs['i'].apply(lambda index: fea_names[index])\n",
    "\n",
    "def get_col(df: pd.DataFrame, \n",
    "            col_name: str, \n",
    "            labels: list = labels,\n",
    "            coefs: np.ndarray = coefs_all) -> pd.Series:\n",
    "    \"\"\"An ugly function that is used to specifically create new columns \n",
    "    of indices for the 4 topics as required.\n",
    "    \"\"\"\n",
    "    # create a list of indices for the topic\n",
    "    col = df.loc[:, 'i']\n",
    "    \n",
    "    # convert indices into cofficients\n",
    "    i = labels.index(col_name)\n",
    "    col = [coefs[i][coef_index] for coef_index in col]\n",
    "    \n",
    "    return col\n",
    "\n",
    "# add the 4 topic columns\n",
    "df_coefs['alt.atheism'] = get_col(df_coefs, 'alt.atheism')\n",
    "df_coefs['comp.graphics'] = get_col(df_coefs, 'comp.graphics')\n",
    "df_coefs['sci.space'] = get_col(df_coefs, 'sci.space')\n",
    "df_coefs['talk.religion.misc'] = get_col(df_coefs, 'talk.religion.misc')\n",
    "\n",
    "# refine/reorder/drop columns\n",
    "df_coefs['Rank'] += 1\n",
    "df_coefs = df_coefs.loc[:, ['Category', \n",
    "                            'Bigram', \n",
    "                            'Rank', \n",
    "                            'alt.atheism', \n",
    "                            'comp.graphics', \n",
    "                            'sci.space', \n",
    "                            'talk.religion.misc']]\n",
    "# a random agg function is applied, which should have no effect because \n",
    "# no values are aggregrated\n",
    "df_coefs = df_coefs.groupby(['Category', 'Rank', 'Bigram']).median()\n",
    "\n",
    "# highlight the top-5 coefficients in each category\n",
    "def highlight_max(x):\n",
    "    return ['color: #ff8c00' if i[0] == x.name else '' for i, _ in x.iteritems()]\n",
    "s = df_coefs.style.apply(highlight_max)\n",
    "\n",
    "# top align cell (fot the Category index column)\n",
    "s.set_table_styles([\n",
    "    {'selector': 'th',\n",
    "     'props': [\n",
    "         ('vertical-align','top')]}]\n",
    ")\n",
    "s.format({\n",
    "    'alt.atheism': '{:,.2f}'.format,\n",
    "    'comp.graphics': '{:,.2f}'.format,\n",
    "    'sci.space': '{:,.2f}'.format,\n",
    "    'talk.religion.misc': '{:,.2f}'.format\n",
    "})\n",
    "HTML(s.render())\n",
    "    ### STUDENT END ###\n",
    "#P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cY67F-tXFsxt"
   },
   "source": [
    "ANSWER:  \n",
    "The regression does a good job on optimizing the coefficients. The top-5 bigrams in each category generally have significantly larger, positive coefficients than they are in other categories. However, there are a few surprises observed:\n",
    "- Certain words such as \"cheers kent,\" appear significant in both \"alt.atheism\" and \"talk.religion.misc.\" \n",
    "- A good portion of the top word bigrams are just generic phrases such as \"you are,\" \"are you,\" \"it was,\" etc. This is not ideal; for example, the model might not generalize well if \"alt.atheism\" relies on keywords such as \"was just,\" \"you are,\" and \"are you\" for the prediction.\n",
    "- Why is the #1 word bigram in talk.religion.misc \"the fbi?\" Are people discussing cults that are targeted by FBI?\n",
    "- What does \"cheers kent\" mean? Why is the token heavily discussed in religion and atheism?\n",
    "- \"ignorance is\" is #3 on talk.religion.misc. Are people trashing each other's belief?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVWhSJHHFsxu"
   },
   "source": [
    "### Part 5:\n",
    "\n",
    "To improve generalization, it is common to try preprocessing text in various ways before splitting into words. For example, you could try transforming strings to lower case, replacing sequences of numbers with single tokens, removing various non-letter characters, and shortening long words.\n",
    "\n",
    "Produce a Logistic Regression model (with no preprocessing of text).  Evaluate and show its f1 score and size of the dictionary.\n",
    "\n",
    "Produce an improved Logistic Regression model by preprocessing the text.  Evaluate and show its f1 score and size of the vocabulary.  Try for an improvement in f1 score of at least 0.02.\n",
    "\n",
    "How much did the improved model reduce the vocabulary size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7gS3cGpFsxv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "Vocabulary Size: 26,879\n",
      "F1 = : 0.7085\n",
      "\n",
      "Model 2:\n",
      "Vocabulary Size: 16,887\n",
      "F1 = : 0.7401\n",
      "\n",
      "F1 Improvement: 0.0317\n"
     ]
    }
   ],
   "source": [
    "#def better_preprocessor(s):\n",
    "    ### STUDENT START ###\n",
    "def better_preprocessor(doc: str) -> np.ndarray:\n",
    "    \"\"\"Function to preprocess words.\n",
    "    \"\"\"\n",
    "    doc = doc.lower() # lower the case\n",
    "    # somehow removing emails don't help...\n",
    "    # doc = re.sub(r'\\S*@\\S*\\s?', 'email ', doc) # treat all emails as the same thing\n",
    "    doc = re.sub(r'[^\\w\\s]+|_+', ' ', doc) # remove non-alphanumeric characters\n",
    "    doc = re.sub(r'[0-9]{1,}', 'number', doc) # treat all numbers as the same thing\n",
    "    doc = re.sub(r'<[^>]*>', '', doc) # remove all HTML tags\n",
    "    doc = re.sub(r'-+', ' ', doc) # split words with dash\n",
    "    \n",
    "    # stem & remove stop words\n",
    "    # Note: somehow removing stop words in here in addition to \n",
    "    # CountVectorizer's stop_word='english' helps improve f1 significantly...\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = doc.split()\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    # options: PorterStemmer & LancasterStemmer\n",
    "    tokens = [nltk.stem.PorterStemmer().stem(w) for w in tokens]\n",
    "    doc = ' '.join(tokens)\n",
    "        \n",
    "    return doc\n",
    "    ### STUDENT END ###\n",
    "\n",
    "#def P5():\n",
    "    ### STUDENT START ###\n",
    "# model 1\n",
    "cv_train = CountVectorizer()\n",
    "x_train = cv_train.fit_transform(train_data)\n",
    "cv_dev = CountVectorizer(vocabulary=cv_train.get_feature_names())\n",
    "x_dev = cv_dev.fit_transform(dev_data)\n",
    "\n",
    "lr = LogisticRegression(C=0.5, solver=\"liblinear\", multi_class=\"auto\")\n",
    "lr_m = lr.fit(x_train, train_labels)\n",
    "dev_pds = lr_m.predict(x_dev)\n",
    "\n",
    "voc_size_1 = x_train.shape[1]\n",
    "f1_1 = metrics.f1_score(dev_labels, dev_pds, average=\"weighted\")\n",
    "\n",
    "# model 2\n",
    "cv_train_2 = CountVectorizer(preprocessor=better_preprocessor, \n",
    "                             stop_words='english')\n",
    "x_train_2 = cv_train_2.fit_transform(train_data)\n",
    "cv_dev_2 = CountVectorizer(vocabulary=cv_train_2.get_feature_names(),\n",
    "                           preprocessor=better_preprocessor,\n",
    "                           stop_words='english')\n",
    "x_dev_2 = cv_dev_2.fit_transform(dev_data)\n",
    "\n",
    "lr_2 = LogisticRegression(C=0.5, solver=\"liblinear\", multi_class=\"auto\")\n",
    "lr_m_2 = lr.fit(x_train_2, train_labels)\n",
    "dev_pds_2 = lr_m.predict(x_dev_2)\n",
    "\n",
    "voc_size_2 = x_train_2.shape[1]\n",
    "f1_2 = metrics.f1_score(dev_labels, dev_pds_2, average=\"weighted\")\n",
    "\n",
    "print('Model 1:')\n",
    "print('Vocabulary Size: {:,d}'.format(voc_size_1))\n",
    "print('F1 = : {:.4f}'.format(f1_1))\n",
    "print('\\nModel 2:')\n",
    "print('Vocabulary Size: {:,d}'.format(voc_size_2))\n",
    "print('F1 = : {:.4f}'.format(f1_2))\n",
    "print('\\nF1 Improvement: {:.4f}'.format(f1_2 - f1_1))\n",
    "    ### STUDENT END ###\n",
    "\n",
    "#P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:  \n",
    "F1 improves by 0.0317 and the vocabulary size drops from 26,879 to 16,887 after the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uy-WITbNFsxw"
   },
   "source": [
    "### Part 6:\n",
    "\n",
    "The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. Logistic regression seeks the set of weights that minimizes errors in the training data AND has a small total size. The default L2 regularization computes this size as the sum of the squared weights (as in Part 3 above). L1 regularization computes this size as the sum of the absolute values of the weights. Whereas L2 regularization makes all the weights relatively small, L1 regularization drives many of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "For several L1 regularization strengths ...<br/>\n",
    "* Produce a Logistic Regression model using the **L1** regularization strength.  Reduce the vocabulary to only those features that have at least one non-zero weight among the four categories.  Produce a new Logistic Regression model using the reduced vocabulary and **L2** regularization strength of 0.5.  Evaluate and show the L1 regularization strength, vocabulary size, and f1 score associated with the new model.\n",
    "\n",
    "Show a plot of f1 score vs. log vocabulary size.  Each point corresponds to a specific L1 regularization strength used to reduce the vocabulary.\n",
    "\n",
    "How does performance of the models based on reduced vocabularies compare to that of a model based on the full vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6ho31SrFsxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model 1 of 12...\n",
      "Building model 2 of 12...\n",
      "Building model 3 of 12...\n",
      "Building model 4 of 12...\n",
      "Building model 5 of 12...\n",
      "Building model 6 of 12...\n",
      "Building model 7 of 12...\n",
      "Building model 8 of 12...\n",
      "Building model 9 of 12...\n",
      "Building model 10 of 12...\n",
      "Building model 11 of 12...\n",
      "Building model 12 of 12...\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>L1 Lambda</th>\n",
       "      <th>L1 C</th>\n",
       "      <th>L2 C</th>\n",
       "      <th>F1 (Full)</th>\n",
       "      <th>F1 (Reduced)</th>\n",
       "      <th>Vocabulary Size (Full)</th>\n",
       "      <th>Vocabulary Size (Reduced)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.10</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>26879</td>\n",
       "      <td>3326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.30</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>26879</td>\n",
       "      <td>2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.90</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>26879</td>\n",
       "      <td>1621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>26879</td>\n",
       "      <td>1517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.70</td>\n",
       "      <td>26879</td>\n",
       "      <td>1394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.70</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.70</td>\n",
       "      <td>26879</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>26879</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "      <td>26879</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>26879</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "      <td>26879</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>26879</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1,000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>26879</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#def P6():\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    #np.random.seed(0)\n",
    "    \n",
    "    ### STUDENT START ###\n",
    "LAMBDAS = [i/10 for i in range(1, 11, 2)]\n",
    "LAMBDAS.extend(list(range(1, 11, 2)))\n",
    "LAMBDAS.extend([100, 1000])\n",
    "\n",
    "# baseline training features\n",
    "cv_train = CountVectorizer()\n",
    "x_train = cv_train.fit_transform(train_data)\n",
    "\n",
    "# full voc df\n",
    "df = pd.DataFrame(data=np.empty(0, dtype=[('L1 Lambda', 'float'), \n",
    "                                          ('Vocabulary Size (Full)', 'int'),\n",
    "                                          ('F1 (Full)', 'float')]))\n",
    "# reduced voc df\n",
    "df_r = pd.DataFrame(data=np.empty(0, dtype=[('L1 Lambda', 'float'), \n",
    "                                            ('Vocabulary Size (Reduced)', 'int'),\n",
    "                                            ('F1 (Reduced)', 'float')]))\n",
    "for i, lambda_ in enumerate(LAMBDAS):\n",
    "    # model 1 starts here...\n",
    "    # build raw model\n",
    "    lr = LogisticRegression(C=1/lambda_, \n",
    "                            penalty=\"l1\",\n",
    "                            tol=0.015,\n",
    "                            solver=\"liblinear\", \n",
    "                            multi_class=\"auto\")\n",
    "    lr_m = lr.fit(x_train, train_labels)\n",
    "    \n",
    "    # raw model predictions\n",
    "    cv_dev = CountVectorizer(vocabulary=cv_train.get_feature_names())\n",
    "    x_dev = cv_dev.fit_transform(dev_data)\n",
    "    dev_pds = lr_m.predict(x_dev)\n",
    "    \n",
    "    # store raw model results\n",
    "    voc_size = x_train.shape[1]\n",
    "    f1 = metrics.f1_score(dev_labels, dev_pds, average=\"weighted\")\n",
    "    df.loc[i] = [lambda_, voc_size, f1]\n",
    "    \n",
    "    # model 2 starts here...\n",
    "    # get new vocabularies\n",
    "    i_non_zero = (np.sum(lr_m.coef_, axis=0) != 0)\n",
    "    vocs_new = np.array(cv_train.get_feature_names())[i_non_zero]\n",
    "    \n",
    "    # build new model using new vocabularies\n",
    "    cv_train_2 = CountVectorizer(vocabulary=vocs_new)\n",
    "    x_train_2 = cv_train_2.fit_transform(train_data)\n",
    "    lr_2 = LogisticRegression(C=0.5, \n",
    "                              penalty=\"l2\",\n",
    "                              tol=0.015,\n",
    "                              solver=\"liblinear\", \n",
    "                              multi_class=\"auto\")\n",
    "    lr_m_2 = lr.fit(x_train_2, train_labels)\n",
    "    \n",
    "    # new model predictions\n",
    "    cv_dev = CountVectorizer(vocabulary=vocs_new)\n",
    "    x_dev = cv_dev.fit_transform(dev_data)\n",
    "    dev_pds = lr_m_2.predict(x_dev)\n",
    "    \n",
    "    # store results\n",
    "    voc_size = x_train_2.shape[1]\n",
    "    f1 = metrics.f1_score(dev_labels, dev_pds, average=\"weighted\")\n",
    "    df_r.loc[i] = [lambda_, voc_size, f1]\n",
    "    print('Building model {} of {}...'.format(i+1, len(LAMBDAS)))\n",
    "\n",
    "# merge model 1 & 2 results\n",
    "df = df.astype({'Vocabulary Size (Full)': 'int'})\n",
    "df_r = df_r.astype({'Vocabulary Size (Reduced)': 'int'})\n",
    "df_r = df_r.sort_values(by=['Vocabulary Size (Reduced)', 'L1 Lambda'], \n",
    "                        ascending=False, ignore_index=True)\n",
    "df_merge = pd.merge(df_r, df, on='L1 Lambda', how='left')\n",
    "df_merge['L1 C'] = 1 / df_merge[\"L1 Lambda\"]\n",
    "df_merge['L2 C'] = 0.5\n",
    "df_merge = df_merge.iloc[:, [0, 5, 6, 4, 2, 3, 1]]\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "display(HTML(df_merge.to_html(index=False)))\n",
    "print('')\n",
    "\n",
    "# plot performance metrics\n",
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "fig.suptitle('F1 Score vs. Log Vocabulary Size')\n",
    "ax.plot(np.log(df_merge['Vocabulary Size (Reduced)']), \n",
    "        df_merge['F1 (Reduced)'], \n",
    "        color='r', \n",
    "        linestyle='solid', \n",
    "        marker = 'o', \n",
    "        label='accuracy')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xlabel('log( Vocabulary Size )')\n",
    "\n",
    "# add annotations\n",
    "for i, lambda_ in enumerate(df_merge['L1 Lambda'].tolist()):\n",
    "    txt = r'$\\lambda$={}'.format(lambda_)\n",
    "    x = np.log(df_merge.loc[i, 'Vocabulary Size (Reduced)'])\n",
    "    y = df_r.loc[i, 'F1 (Reduced)']\n",
    "    y_adj = 0.01\n",
    "    if i % 2 == 1:\n",
    "        y_adj = -0.02\n",
    "    ax.annotate(txt, (x, y + y_adj))\n",
    "pass\n",
    "    ### STUDENT END ###\n",
    "#P6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZmIPwExFsx1"
   },
   "source": [
    "ANSWER:  \n",
    "Performance Comparison: Full-Vocabularies vs. Reduced-Vocabularies\n",
    "- The f1 score from Part 3 is 0.7145 when C is set to 0.5. This score beats all f1 scores produced by the reduced-vocabularies model in Part 6. \n",
    "    - The result is expected because L1 is generally seen as a more \"harsh\" version of L2, and, in our case, unnecessarily drives weights to zero. \n",
    "    - The undesirable harsh L1 regularization effect is especially obvious when the C goes small and eliminates most of the features for prediction. \n",
    "- On the other hand, the f1 scores are roughly equal across all regularization strengths when comparing the two models (L1 vs. L2) in Part 6. This is also expected because two models essentially remove exactly the same vocabularies from the feature set.\n",
    "\n",
    "F1 Trend Observation:\n",
    "- Increase in L1 $\\lambda$ leads to a decrease in vocabulary size. \n",
    "- F1 improves as L1 $\\lambda$ increases only till a certain threshold. F1 generally decreases afterwards.\n",
    "- The observation makes sense because an increase in L1 $\\lambda$ improves the issue of overfitting with generalization. Too much generalization, however, hurts the model's prediction power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmrHf8AMFsxz"
   },
   "source": [
    "### Part 7:\n",
    "\n",
    "How is `TfidfVectorizer` different than `CountVectorizer`?\n",
    "\n",
    "Produce a Logistic Regression model based on data represented in tf-idf form, with L2 regularization strength of 100.  Evaluate and show the f1 score.  How is `TfidfVectorizer` different than `CountVectorizer`?\n",
    "\n",
    "Show the 3 documents with highest R ratio, where ...<br/>\n",
    "$R\\,ratio = maximum\\,predicted\\,probability \\div predicted\\,probability\\,of\\,correct\\,label$\n",
    "\n",
    "Explain what the R ratio describes.  What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vwU_9t2Fsx0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (CountVectorizer):\n",
      "F1: 0.6800\n",
      "Top 3 documents by R Ratio:\n",
      "R = 2.54e+36, 2.23e+22, 5.80e+19\n",
      "\n",
      "Model 2 (TfidfVectorizer):\n",
      "F1: 0.7568\n",
      "Top 3 documents by R Ratio:\n",
      "R = 1010.92, 386.43, 358.76\n",
      "\n",
      "#1\n",
      "------------------------------------------------\n",
      "R = 1010.92\n",
      "Correct Topic: talk.religion.misc\n",
      "Predicted Topic: comp.graphics\n",
      "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
      "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
      "available through anonymous ftp (see information below). In addition to the\n",
      "change in title, the revised ETR BOM has been shortened by several pages\n",
      "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
      "have been corrected. This release includes a simplified Joseph Smith Story,\n",
      "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
      "glossary.\n",
      "\n",
      "As with the previous announcement, readers are reminded that this is a\n",
      "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
      "to make *verbatim* copies for personal use. People can recuperate the\n",
      "actual costs of printing (paper, copy center charges), but may not charge\n",
      "anything for their time in making copies, or in any way realize a profit\n",
      "from the use of this book. See the permissions notice in the book itself\n",
      "for the precise terms.\n",
      "\n",
      "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
      "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
      "\"first editions.\") I will make another announcement about the availability\n",
      "of printed copies once everything has been worked out.\n",
      "\n",
      "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
      "pub\" (you won't see anything at all until you do).\n",
      "\n",
      "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
      "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
      "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
      "print the postscript file on any postscript printer (such as an Apple\n",
      "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
      "the last release had problems on some printers; this time it should work\n",
      "better.) RTF is a standard document interchange format that can be read in\n",
      "by a number of word processors, including Microsoft Word for both the\n",
      "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
      "able to use the RTF file to print out a copy of the book.\n",
      "\n",
      "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
      "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
      "\n",
      "For more information about how this project came about, please refer to my\n",
      "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
      "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
      "\n",
      "Send all inquiries and comments to:\n",
      "\n",
      "    Lynn Matthews Anderson\n",
      "    5806 Hampton Street\n",
      "    Pittsburgh, PA 15206\n",
      "\n",
      "#2\n",
      "------------------------------------------------\n",
      "R = 386.43\n",
      "Correct Topic: alt.atheism\n",
      "Predicted Topic: talk.religion.misc\n",
      "\n",
      "The 24 children were, of course, killed by a lone gunman in a second story\n",
      "window, who fired eight bullets in the space of two seconds...\n",
      "\n",
      "#3\n",
      "------------------------------------------------\n",
      "R = 358.76\n",
      "Correct Topic: talk.religion.misc\n",
      "Predicted Topic: comp.graphics\n",
      "Can anyone provide me a ftp site where I can obtain a online version\n",
      "of the Book of Mormon. Please email the internet address if possible.\n"
     ]
    }
   ],
   "source": [
    "#def P7():\n",
    "    ### STUDENT START ###\n",
    "C = 100\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Model 1 - CountVectorizer\n",
    "# transform features\n",
    "cv_train = CountVectorizer()\n",
    "x_train_cv = cv_train.fit_transform(train_data)\n",
    "cv_dev = CountVectorizer(vocabulary=cv_train.get_feature_names())\n",
    "x_dev_cv = cv_dev.fit_transform(dev_data)\n",
    "\n",
    "# train model\n",
    "lr_cv = LogisticRegression(C=C, \n",
    "                           penalty=\"l2\",\n",
    "                           solver=\"liblinear\", \n",
    "                           multi_class=\"auto\")\n",
    "lr_m_cv = lr_cv.fit(x_train_cv, train_labels)\n",
    "\n",
    "# f1\n",
    "predictions_cv = lr_m_cv.predict(x_dev_cv)\n",
    "f1_cv = metrics.f1_score(dev_labels, predictions_cv, average=\"weighted\")\n",
    "\n",
    "# r ratio\n",
    "predictions_prob_cv = lr_m_cv.predict_proba(x_dev_cv)\n",
    "max_prob_cv = np.max(predictions_prob_cv, axis=1)\n",
    "correct_prob_cv = predictions_prob_cv[np.arange(0, dev_labels.shape[0]), dev_labels]\n",
    "r_arr_cv = max_prob_cv / correct_prob_cv\n",
    "r_top3_cv = r_arr_cv.flatten()[np.argsort(r_arr_cv, axis=None)][::-1][0:3]\n",
    "\n",
    "print(\"Model 1 (CountVectorizer):\")\n",
    "print(\"F1: {:.4f}\".format(f1_cv))\n",
    "print(\"Top 3 documents by R Ratio:\")\n",
    "print(\"R = {:.2e}, {:.2e}, {:.2e}\".format(r_top3_cv[0], r_top3_cv[1], r_top3_cv[2]))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Model 2 - TfidfVectorizer\n",
    "# transform features\n",
    "tv_train = TfidfVectorizer()\n",
    "x_train_tv = tv_train.fit_transform(train_data)\n",
    "tv_dev = TfidfVectorizer(vocabulary=tv_train.get_feature_names())\n",
    "x_dev_tv = tv_dev.fit_transform(dev_data)\n",
    "\n",
    "# train model\n",
    "lr_tv = LogisticRegression(C=C, \n",
    "                           penalty=\"l2\",\n",
    "                           solver=\"liblinear\", \n",
    "                           multi_class=\"auto\")\n",
    "lr_m_tv = lr_tv.fit(x_train_tv, train_labels)\n",
    "\n",
    "# f1\n",
    "predictions_tv = lr_m_tv.predict(x_dev_tv)\n",
    "f1_tv = metrics.f1_score(dev_labels, predictions_tv, average=\"weighted\")\n",
    "\n",
    "# r ratio\n",
    "predictions_prob_tv = lr_m_tv.predict_proba(x_dev_tv)\n",
    "max_prob_tv = np.max(predictions_prob_tv, axis=1)\n",
    "correct_prob_tv = predictions_prob_tv[np.arange(0, dev_labels.shape[0]), dev_labels]\n",
    "r_arr_tv = max_prob_tv / correct_prob_tv\n",
    "\n",
    "# indexing\n",
    "index_by_r = np.argsort(r_arr_tv, axis=None)[::-1][0:3]\n",
    "r_top3_tv = r_arr_tv[index_by_r]\n",
    "doc_top3 = np.array(dev_data)[index_by_r]\n",
    "correct_labels_top3 = np.array(dev_labels)[index_by_r]\n",
    "predicted_labels_top3 = predictions_tv[index_by_r]\n",
    "\n",
    "# print resutls\n",
    "tn = newsgroups_test.target_names\n",
    "print(\"\")\n",
    "print(\"Model 2 (TfidfVectorizer):\")\n",
    "print(\"F1: {:.4f}\".format(f1_tv))\n",
    "print(\"Top 3 documents by R Ratio:\")\n",
    "print(\"R = {:.2f}, {:.2f}, {:.2f}\".format(r_top3_tv[0], r_top3_tv[1], r_top3_tv[2]))\n",
    "\n",
    "print(\"\\n#1\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"R = {:.2f}\".format(r_top3_tv[0]))\n",
    "print(\"Correct Topic:\", tn[correct_labels_top3[0]])\n",
    "print(\"Predicted Topic:\", tn[predicted_labels_top3[0]])\n",
    "print(doc_top3[0])\n",
    "\n",
    "print(\"\\n#2\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"R = {:.2f}\".format(r_top3_tv[1]))\n",
    "print(\"Correct Topic:\", tn[correct_labels_top3[1]])\n",
    "print(\"Predicted Topic:\", tn[predicted_labels_top3[1]])\n",
    "print(doc_top3[1])\n",
    "\n",
    "print(\"#3\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"R = {:.2f}\".format(r_top3_tv[2]))\n",
    "print(\"Correct Topic:\", tn[correct_labels_top3[2]])\n",
    "print(\"Predicted Topic:\", tn[predicted_labels_top3[2]])\n",
    "print(doc_top3[2])\n",
    "    ### STUDENT END ###\n",
    "#P7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZmIPwExFsx1"
   },
   "source": [
    "ANSWER:\n",
    "- **How is TfidfVectorizer different than CountVectorizer?**  \n",
    "  CountVectorizer counts tokens while TFIDFVectorizer is CountVectorizer times the inversed toekn-frequency across all documents. This means that the more frequently the toekn appears across all the dataset, the more severely the token's score will be penalized. The effect is that TFIDFVectorizer downplays the importance of the tokens that appear frequently across multiple documents in the dataset.\n",
    "  \n",
    "  \n",
    "- **Evaluate and show the f1 score.**  \n",
    "    - TfidfVectorizer improves F1 score improves significantly (from 0.68 to 0.76). The additional penalization imposed by TfidfVectorizer seems to work well before feature transformations such as stop words removal are applied. \n",
    "  \n",
    "  \n",
    "- **Explain what the R ratio describes.**  \n",
    "    - Mathmetically:\n",
    "        - R ratio equals to 1 if the model makes a right prediction.\n",
    "        - R ratio becomes larger than 1 if the model makes a wrong predction.\n",
    "        - R ratio increases with the \"wrongfulness\" of the prediction. For example, R ratio becomes infinite if a probability of zero is assigned to the correct label.\n",
    "    - Intuitively:\n",
    "        - R measures \"how wrong\" the prediction is. The larger the R ratio is, the more wrong the prediction is.\n",
    "\n",
    "\n",
    "- **What kinds of mistakes is the model making?**\n",
    "    - Looking at the top 3 examples by R ratio, one major issue is that the model predicts \"comp.graphics\" when technocal keywords such as \"ftp,\" \"ASCII,\" and \"rtf\" are mentioned. \n",
    "    \n",
    "    \n",
    "- **Suggest a way to address one particular issue that you see.**  \n",
    "    - The goals are threefold:\n",
    "        - Ideally, we want to predict correctly. \n",
    "        - We should at least aim to lower the r ratios for wrong predictions if correct predictions are impossible due to the overwhelming number of keywords from other categories.\n",
    "        - We should also see an overall improvement of the F1 value after optimization.  \n",
    "    - Things to try:\n",
    "        - **N-gram Transformation** should allow weights to be assigned to new tokens such as \"Book of Mormon\" or \"Lynn Matthews Anderson.\"\n",
    "        - **Words Reduction** (such as stemming, stop wrods removal, etc.) combined with TfidfVectorizer should allow the model to put more emphesis on important keywards that matter. \n",
    "        - **Regularization** may or may not help. Our best educated guess is to increase the regularization strength (i.e. a smaller C with larger lambda) so that the model would pue less emphesis on a few select technical keywords shown by the top 3 examples by R ratio.\n",
    "    - Restuls (Part 8): \n",
    "        - R ratios drop for al three top 3 cases. The R ratio for Case #1 drops from 1010.92 to 473.61. Case #3 from Part 7 (with a R ratio of 358.76 and keywords such \"ftp\") has its R ratios dopped so low that it is now out of top 3.  \n",
    "        - Also note that in Part 8, two of the top three cases are now predicted wrong because of the model's confusion between \"religion\" and \"atheism.\" While still not ideal, we beleive that this is an improvement from Part 7's confusion between \"religion\" and \"comp.graphics.\"\n",
    "        - Lastly, F1 improves slightly by a scale of 0.0152."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmsDxtzoFsx1"
   },
   "source": [
    "### Part 8 EXTRA CREDIT:\n",
    "\n",
    "Produce a Logistic Regression model to implement your suggestion from Part 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZMaqe8c5Fsx2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7720\n",
      "F1 Improvement (vs. Part 7): 0.0152\n",
      "Vocubulary Size: 163,905\n",
      "\n",
      "Top 3 Documents by R Ratio:\n",
      "\n",
      "#1\n",
      "---------------------------------\n",
      "R = 473.61\n",
      "Correct Topic: talk.religion.misc\n",
      "Predicted Topic: comp.graphics\n",
      "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
      "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
      "available through anonymous ftp (see information below). In addition to the\n",
      "change in title, the revised ETR BOM has been shortened by several pages\n",
      "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
      "have been corrected. This release includes a simplified Joseph Smith Story,\n",
      "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
      "glossary.\n",
      "\n",
      "As with the previous announcement, readers are reminded that this is a\n",
      "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
      "to make *verbatim* copies for personal use. People can recuperate the\n",
      "actual costs of printing (paper, copy center charges), but may not charge\n",
      "anything for their time in making copies, or in any way realize a profit\n",
      "from the use of this book. See the permissions notice in the book itself\n",
      "for the precise terms.\n",
      "\n",
      "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
      "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
      "\"first editions.\") I will make another announcement about the availability\n",
      "of printed copies once everything has been worked out.\n",
      "\n",
      "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
      "pub\" (you won't see anything at all until you do).\n",
      "\n",
      "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
      "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
      "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
      "print the postscript file on any postscript printer (such as an Apple\n",
      "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
      "the last release had problems on some printers; this time it should work\n",
      "better.) RTF is a standard document interchange format that can be read in\n",
      "by a number of word processors, including Microsoft Word for both the\n",
      "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
      "able to use the RTF file to print out a copy of the book.\n",
      "\n",
      "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
      "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
      "\n",
      "For more information about how this project came about, please refer to my\n",
      "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
      "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
      "\n",
      "Send all inquiries and comments to:\n",
      "\n",
      "    Lynn Matthews Anderson\n",
      "    5806 Hampton Street\n",
      "    Pittsburgh, PA 15206\n",
      "\n",
      "#2\n",
      "---------------------------------\n",
      "R = 84.02\n",
      "Correct Topic: alt.atheism\n",
      "Predicted Topic: talk.religion.misc\n",
      "With the Southern Baptist Convention convening this June to consider\n",
      "the charges that Freemasonry is incompatible with christianity, I thought\n",
      "the following quotes by Mr. James Holly, the Anti-Masonic Flag Carrier,\n",
      "would amuse you all...\n",
      "\n",
      "     The following passages are exact quotes from \"The Southern \n",
      "Baptist Convention and Freemasonry\" by James L. Holly, M.D., President\n",
      "of Mission and Ministry To Men, Inc., 550 N 10th St., Beaumont, TX \n",
      "77706. \n",
      " \n",
      "     The inside cover of the book states: \"Mission & Ministry to Men, \n",
      "Inc. hereby grants permission for the reproduction of part or all of \n",
      "this booklet with two provisions: one, the material is not changed and\n",
      "two, the source is identified.\" I have followed these provisions. \n",
      "  \n",
      "     \"Freemasonry is one of the allies of the Devil\" Page iv. \n",
      " \n",
      "     \"The issue here is not moderate or conservative, the issue is God\n",
      "and the Devil\" Page vi.\" \n",
      " \n",
      "     \"It is worthwhile to remember that the formulators of public \n",
      "school education in America were Freemasons\" Page 29. \n",
      " \n",
      "     \"Jesus Christ never commanded toleration as a motive for His \n",
      "disciples, and toleration is the antithesis of the Christian message.\"\n",
      "Page 30. \n",
      " \n",
      "     \"The central dynamic of the Freemason drive for world unity \n",
      "through fraternity, liberty and equality is toleration. This is seen \n",
      "in the writings of the 'great' writers of Freemasonry\". Page 31. \n",
      " \n",
      "     \"He [Jesus Christ] established the most sectarian of all possible \n",
      "faiths.\" Page 37. \n",
      " \n",
      "     \"For narrowness and sectarianism, there is no equal to the Lord \n",
      "Jesus Christ\". Page 40. \n",
      " \n",
      "     \"What seems so right in the interest of toleration and its \n",
      "cousins-liberty, equality and fraternity-is actually one of the \n",
      "subtlest lies of the 'father of lies.'\" Page 40. \n",
      " \n",
      "     \"The Southern Baptist Convention has many churches which were \n",
      "founded in the Lodge and which have corner stones dedicated by the \n",
      "Lodge. Each of these churches should hold public ceremonies of \n",
      "repentance and of praying the blood and the Name of the Lord Jesus \n",
      "Christ over the church and renouncing the oaths taken at the \n",
      "dedication of the church and/or building.\" Page 53-54.  \n",
      " \n",
      "#3\n",
      "---------------------------------\n",
      "R = 83.05\n",
      "Correct Topic: talk.religion.misc\n",
      "Predicted Topic: alt.atheism\n",
      "Why is the NT tossed out as info on Jesus.  I realize it is normally tossed\n",
      "out because it contains miracles, but what are the other reasons?\n",
      "\n",
      "MAC\n",
      "--\n",
      "****************************************************************\n",
      "                                                    Michael A. Cobb\n",
      " \"...and I won't raise taxes on the middle     University of Illinois\n",
      "    class to pay for my programs.\"                 Champaign-Urbana\n",
      "          -Bill Clinton 3rd Debate             cobb@alexia.lis.uiuc.edu\n"
     ]
    }
   ],
   "source": [
    "C = 50\n",
    "NGRAM_RANGE = [1, 2]\n",
    "F_VALUE_PART_7 = 0.7568\n",
    "\n",
    "def better_preprocessor(doc: str) -> np.ndarray:\n",
    "    \"\"\"Function to preprocess words.\n",
    "    \"\"\"\n",
    "    doc = doc.lower() # lower the case\n",
    "    doc = re.sub(r'[^\\w\\s]+|_+', ' ', doc) # remove non-alphanumeric characters\n",
    "    doc = re.sub(r'[0-9]{1,}', 'number', doc) # treat all numbers as the same thing\n",
    "    doc = re.sub(r'<[^>]*>', '', doc) # remove all HTML tags\n",
    "    doc = re.sub(r'-+', ' ', doc) # split words with dash\n",
    "    \n",
    "    # stem & remove stop words\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = doc.split()\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [nltk.stem.PorterStemmer().stem(w) for w in tokens]\n",
    "    doc = ' '.join(tokens)\n",
    "        \n",
    "    return doc\n",
    "\n",
    "# transform features\n",
    "tv_train = TfidfVectorizer(preprocessor=better_preprocessor, \n",
    "                           stop_words='english',\n",
    "                           max_df=0.25,\n",
    "                           ngram_range=NGRAM_RANGE)\n",
    "x_train_tv = tv_train.fit_transform(train_data)\n",
    "tv_dev = TfidfVectorizer(vocabulary=tv_train.get_feature_names(),\n",
    "                         preprocessor=better_preprocessor, \n",
    "                         stop_words='english',\n",
    "                         max_df=0.25,\n",
    "                         ngram_range=NGRAM_RANGE)\n",
    "x_dev_tv = tv_dev.fit_transform(dev_data)\n",
    "\n",
    "# train model\n",
    "lr_tv = LogisticRegression(C=C, \n",
    "                           penalty=\"l2\",\n",
    "                           solver=\"liblinear\", \n",
    "                           multi_class=\"auto\")\n",
    "lr_m_tv = lr_tv.fit(x_train_tv, train_labels)\n",
    "\n",
    "# f1\n",
    "predictions_tv = lr_m_tv.predict(x_dev_tv)\n",
    "f1_tv = metrics.f1_score(dev_labels, predictions_tv, average=\"weighted\")\n",
    "\n",
    "# r ratio\n",
    "predictions_prob_tv = lr_m_tv.predict_proba(x_dev_tv)\n",
    "max_prob_tv = np.max(predictions_prob_tv, axis=1)\n",
    "correct_prob_tv = predictions_prob_tv[np.arange(0, dev_labels.shape[0]), dev_labels]\n",
    "r_arr_tv = max_prob_tv / correct_prob_tv\n",
    "\n",
    "# indexing\n",
    "index_by_r = np.argsort(r_arr_tv, axis=None)[::-1][0:3]\n",
    "r_top3 = r_arr_tv[index_by_r]\n",
    "doc_top3 = np.array(dev_data)[index_by_r]\n",
    "correct_labels_top3 = np.array(dev_labels)[index_by_r]\n",
    "predicted_labels_top3 = predictions_tv[index_by_r]\n",
    "\n",
    "\n",
    "tn = newsgroups_test.target_names\n",
    "print(\"F1: {:.4f}\".format(f1_tv))\n",
    "print(\"F1 Improvement (vs. Part 7): {:.4f}\".format(f1_tv - F_VALUE_PART_7))\n",
    "print(\"Vocubulary Size: {:,d}\".format(len(tv_train.get_feature_names())))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Top 3 Documents by R Ratio:\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"#1\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"R = {:.2f}\".format(r_top3[0]))\n",
    "print(\"Correct Topic:\", tn[correct_labels_top3[0]])\n",
    "print(\"Predicted Topic:\", tn[predicted_labels_top3[0]])\n",
    "print(doc_top3[0])\n",
    "\n",
    "print(\"\\n#2\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"R = {:.2f}\".format(r_top3[1]))\n",
    "print(\"Correct Topic:\", tn[correct_labels_top3[1]])\n",
    "print(\"Predicted Topic:\", tn[predicted_labels_top3[1]])\n",
    "print(doc_top3[1])\n",
    "\n",
    "print(\"#3\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"R = {:.2f}\".format(r_top3[2]))\n",
    "print(\"Correct Topic:\", tn[correct_labels_top3[2]])\n",
    "print(\"Predicted Topic:\", tn[predicted_labels_top3[2]])\n",
    "print(doc_top3[2])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "firstname_lastname_p2.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/MIDS-W207/Master/blob/master/Projects/firstname_lastname_p2.ipynb",
     "timestamp": 1559779272103
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
